{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.config import Config \n",
    "MODEL_DIR = \"/home/akinwilson/Code/pytorch/output/model\"\n",
    "DATA_DIR = \"/home/akinwilson/Code/pytorch/dataset/keywords\"\n",
    "LR_RANGE = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5][1]\n",
    "BATCH_SIZE_RANGE = [1,2,32, 64, 128, 256][2]\n",
    "EPOCH_RANGE = [1, 10, 30, 50, 100, 1000][1]\n",
    "ES_PATIENCE_RANGE = [1, 10, 20, 100, 200][2]\n",
    "MODELS = [\"VecM5\", \"Resnet2vec1D\",\"SpecResnet2D\", \"HSTAT\", \"DeepSpeech\", \"ResNet\"][-1]\n",
    "AUDIO_FEATURE_OPT = [\"spectrogram\", \"mfcc\", \"pcm\"][1]\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"facebook/wav2vec2-base-960h\"\n",
    "AUGS = False\n",
    "\n",
    "params = {\n",
    "    \"audio_duration\":3,\n",
    "    \"sample_rate\":16000,\n",
    "    \"model_name\": MODELS,\n",
    "    \"verbose\": False,\n",
    "    \"path\": {\n",
    "        \"model_dir\": MODEL_DIR,\n",
    "        \"data_dir\": DATA_DIR,\n",
    "        \"pretrained_name_or_path\": PRETRAINED_MODEL_NAME_OR_PATH\n",
    "        },\n",
    "    \"fit_param\": {\"init_lr\":LR_RANGE, \"weight_decay\":0.0001, \"max_epochs\":EPOCH_RANGE, \"gamma\": 0.1,\"es_patience\":ES_PATIENCE_RANGE}, \n",
    "    \"data_param\":{\"train_batch_size\": BATCH_SIZE_RANGE, \"val_batch_size\": BATCH_SIZE_RANGE,\"test_batch_size\": BATCH_SIZE_RANGE}, \n",
    "    \"audio_feature\": AUDIO_FEATURE_OPT,\n",
    "    \"audio_feature_param\": { \"mfcc\":{\"sr\":16000,\"n_mfcc\":20,\"norm\": 'ortho',\"verbose\":True,\"ref\":1.0,\"amin\":1e-10,\"top_db\":80.0,\"hop_length\":512,},\n",
    "                            \"spectrogram\":{\"sr\":16000, \"n_fft\":2048, \"win_length\":None,\"n_mels\":128,\"hop_length\":512,\"window\":'hann',\"center\":True,\"pad_mode\":'reflect',\"power\":2.0,\"htk\":False,\"fmin\":0.0,\"fmax\":None,\"norm\":1,\"trainable_mel\":False,\"trainable_STFT\":False,\"verbose\": True },\n",
    "                            \"pcm\": {}},\n",
    "    \"augmentation\":{'Gain': AUGS, 'PitchShift': AUGS, 'Shift': AUGS},\n",
    "    \"augmentation_param\":{\"Gain\": {  \"min_gain_in_db\":-18.0,\"max_gain_in_db\":  6.0,\"mode\":'per_example',\"p\":1,\"p_mode\":'per_example'},\n",
    "                        \"PitchShift\": {\"min_transpose_semitones\": -4.0, \"max_transpose_semitones\": 4.0,\"mode\":'per_example',\"p\":1,\"p_mode\":'per_example',\"sample_rate\":16000,\"target_rate\": None,\"output_type\": None,},\n",
    "                        \"Shift\":{ \"min_shift\":-0.5,\"max_shift\": 0.5,\"shift_unit\":'fraction',\"rollover\": True,\"mode\":'per_example',\"p\":1,\"p_mode\": 'per_example',\"sample_rate\": 16000,\"target_rate\":None,\"output_type\":None}},\n",
    "    }\n",
    "cfg = Config(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akinwilson/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from wwv.architecture import Architecture\n",
    "from wwv.eval import Metric\n",
    "from wwv.data import AudioDataModule\n",
    "from wwv.config import  DataPaths\n",
    "import statistics\n",
    "# data_path = DataPaths(cfg.path['data_dir'], cfg.model_name, cfg.path['model_dir'])\n",
    "cfg = Config(params)\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "root = \"/home/akinwilson/Code/pytorch/dataset/keywords\"\n",
    "# data_module = AudioDataModule(data_path.root_data_dir + \"/train.csv\",\n",
    "#                               data_path.root_data_dir + \"/val.csv\",\n",
    "#                               data_path.root_data_dir + \"/test.csv\",\n",
    "#                                cfg)\n",
    "# # model.processing_layer[3](x)\n",
    "# train_loader=  data_module.train_dataloader()\n",
    "# val_loader=  data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as  nn\n",
    "import torch.nn.functional as F\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from nnAudio import features\n",
    "import torchaudio \n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1, cfg=cfg):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.cfg= cfg \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.linear = nn.Linear(61440, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        out = F.relu(self.bn1(x))\n",
    "        # print(\"outF.relu(self.bn1(x))\", out.shape)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:93\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[item]\n\u001b[1;32m     94\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'unsqueeze'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [54], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m inputs \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 29\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43msew\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [54], line 19\u001b[0m, in \u001b[0;36mSEW.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# x = input_dict['input_values']\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     x_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(x, ampling_rate\u001b[38;5;241m=\u001b[39msampling_rate, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mx_feats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:95\u001b[0m, in \u001b[0;36mBatchFeature.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[item]\n\u001b[1;32m     94\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, SEWDForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "# dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "class SEW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"anton-l/sew-d-mid-400k-ft-keyword-spotting\")\n",
    "        self.model = SEWDForSequenceClassification.from_pretrained(\"anton-l/sew-d-mid-400k-ft-keyword-spotting\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = input_dict['input_values']\n",
    "        x_feats = self.feature_extractor(x, ampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        x_feats.unsqueeze(1)\n",
    "        logits = self.model(x)\n",
    "        return logits \n",
    "\n",
    "dataset[0][\"audio\"][\"array\"]\n",
    "sew  =SEW()\n",
    "# # audio file is decoded on the fly\n",
    "\n",
    "inputs = dataset[0][\"audio\"][\"array\"]\n",
    "with torch.no_grad():\n",
    "    logits = sew(torch.tensor(inputs))\n",
    "print(logits)\n",
    "# predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "# predicted_label = model.config.id2label[predicted_class_ids]\n",
    "# predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.cfg =cfg\n",
    "        layers = []\n",
    "        kwargs = cfg.audio_feature_param[cfg.audio_feature]\n",
    "        if cfg.audio_feature == \"spectrogram\":\n",
    "            layers.append(features.MelSpectrogram(**kwargs))\n",
    "            # layers.append(T.Resize(224)) # size expected by 2D ResNet \n",
    "        elif cfg.audio_feature == \"mfcc\":\n",
    "            layers.append(features.MFCC(**kwargs))\n",
    "            # layers.append(T.Resize(224)) # size expected by 2D ResNet\n",
    "\n",
    "        # resize inputs\n",
    "        # layers.append(transforms.RandomResizedCrop(224))\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "        # logger.info(f\"{'-'*20}> Features to be extracted: {cfg.audio_feature}\")\n",
    "        # logger.info(f\"{'-'*20}> Feature dimensions: {cfg.processing_output_shape}\")\n",
    "\n",
    "    def forward(self, x:torch.tensor) -> torch.tensor:\n",
    "        x_out = self.net(x)\n",
    "        # if self.cfg.verbose:\n",
    "        #     logger.info(f\"ProcessingLayer().foward() [in]: {x.shape}\")\n",
    "        #     logger.info(f\"ProcessingLayer().foward() [out]: {x_out.shape}\")\n",
    "        return x_out \n",
    "\n",
    "\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "\n",
    "    def __init__(self, n_feats):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "    except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel // 2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel // 2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x  # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super().__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=batch_first,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "from math import prod\n",
    "class DeepSpeech(nn.Module):\n",
    "    def __init__(self, n_cnn_layers=40, n_rnn_layers=1, rnn_dim=1096, stride=2, dropout=0.1,cfg=cfg, **kwargs):\n",
    "        super().__init__()\n",
    "        self.cfg= cfg \n",
    "        n_feats = ( 121, 20) #  self.cfg.processing_output_shape\n",
    "        # self.processing_layer = ProcessingLayer(cfg)\n",
    "        # n_feats =  (121 * 20) // 2\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3 // 2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(\n",
    "            *[ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) for _ in range(n_cnn_layers)]\n",
    "        )\n",
    "        self.fully_connected = nn.Linear(121, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(\n",
    "            *[\n",
    "                BidirectionalGRU(\n",
    "                    rnn_dim=rnn_dim if i == 0 else rnn_dim * 2,\n",
    "                    hidden_size=rnn_dim,\n",
    "                    dropout=dropout,\n",
    "                    batch_first=i == 0,\n",
    "                )\n",
    "                for i in range(n_rnn_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1402880, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Shape b cnn {x.shape}\")\n",
    "        # x = self.processing_layer(x) # (batch, mfcc, timestep)\n",
    "        # print(f\"Shape a process {x.shape}\")\n",
    "        # x = x.unsqueeze(1) # (batch, channel,  mfcc, timestep)\n",
    "        # print(f\"Shape a transpose {x.shape}\")\n",
    "        x = self.cnn(x)\n",
    "        print(f\"Shape a cnn {x.shape}\")\n",
    "        x = self.rescnn_layers(x)\n",
    "        print(f\"Shape a rescnn {x.shape}\")\n",
    "        \n",
    "        # # print(f\"after view {x.shape}\")\n",
    "        # x = x.transpose(1, 2)  # (batch, time, feature)\n",
    "\n",
    "        x = self.fully_connected(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        print(f\"Shape a fully_connected {x.shape}\")\n",
    "        x = self.birnn_layers(x)\n",
    "        print(f\"Shape a birnn_layers {x.shape}\")\n",
    "        # print(f\"after birnn_layers {x.shape}\")\n",
    "        x =  self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.data import AudioDataModule\n",
    "from wwv.config import DataPaths\n",
    "import torchaudio \n",
    "cfg = Config(params)\n",
    "data_path = DataPaths(cfg.path['data_dir'], cfg.model_name, cfg.path['model_dir'])\n",
    "data_module = AudioDataModule(data_path.root_data_dir + \"/train.csv\",\n",
    "                              data_path.root_data_dir + \"/val.csv\",\n",
    "                              data_path.root_data_dir + \"/test.csv\",\n",
    "                              cfg=cfg)\n",
    "\n",
    "# x['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | DeepSpeech | 1.6 B \n",
      "-------------------------------------\n",
      "1.6 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 B     Total params\n",
      "6,212.999 Total estimated model params size (MB)\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | DeepSpeech | 1.6 B \n",
      "-------------------------------------\n",
      "1.6 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 B     Total params\n",
      "6,212.999 Total estimated model params size (MB)\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | DeepSpeech | 1.6 B \n",
      "-------------------------------------\n",
      "1.6 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 B     Total params\n",
      "6,212.999 Total estimated model params size (MB)\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | DeepSpeech | 1.6 B \n",
      "-------------------------------------\n",
      "1.6 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 B     Total params\n",
      "6,212.999 Total estimated model params size (MB)\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | DeepSpeech | 1.6 B \n",
      "-------------------------------------\n",
      "1.6 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 B     Total params\n",
      "6,212.999 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.73 GiB (GPU 0; 10.76 GiB total capacity; 5.79 GiB already allocated; 1.17 GiB free; 5.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [40], line 144\u001b[0m\n\u001b[1;32m    137\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    138\u001b[0m                   devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    139\u001b[0m                   strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    140\u001b[0m                   logger \u001b[38;5;241m=\u001b[39m logger, \n\u001b[1;32m    141\u001b[0m                   default_root_dir\u001b[38;5;241m=\u001b[39mdata_path\u001b[38;5;241m.\u001b[39mmodel_dir,\n\u001b[1;32m    142\u001b[0m                   callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[1;32m    143\u001b[0m                 \u001b[38;5;66;03m#   num_sanity_val_steps=-1)\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRoutine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(dataloaders\u001b[38;5;241m=\u001b[39mtest_loader)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# wrapper wraps model to output prob instead of logits\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 696\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    698\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    651\u001b[0m \u001b[39m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:735\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    731\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    732\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    733\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    734\u001b[0m )\n\u001b[0;32m--> 735\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    737\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    738\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1166\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1168\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1252\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1274\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1273\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1276\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1343\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1345\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1347\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:155\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    154\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 155\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    157\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:143\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    142\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:240\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \n\u001b[1;32m    231\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 240\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1704\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1704\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1706\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/strategies/dp.py:139\u001b[0m, in \u001b[0;36mDataParallelStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    138\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 167\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplicate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(inputs)])\n\u001b[1;32m    168\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:172\u001b[0m, in \u001b[0;36mDataParallel.replicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreplicate\u001b[39m(\u001b[39mself\u001b[39m, module, device_ids):\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m replicate(module, device_ids, \u001b[39mnot\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mis_grad_enabled())\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/torch/nn/parallel/replicate.py:91\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m     89\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(network\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m     90\u001b[0m param_indices \u001b[39m=\u001b[39m {param: idx \u001b[39mfor\u001b[39;00m idx, param \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(params)}\n\u001b[0;32m---> 91\u001b[0m param_copies \u001b[39m=\u001b[39m _broadcast_coalesced_reshape(params, devices, detach)\n\u001b[1;32m     93\u001b[0m buffers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(network\u001b[39m.\u001b[39mbuffers())\n\u001b[1;32m     94\u001b[0m buffers_rg \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/torch/nn/parallel/replicate.py:67\u001b[0m, in \u001b[0;36m_broadcast_coalesced_reshape\u001b[0;34m(tensors, devices, detach)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m Broadcast\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m detach:\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m comm\u001b[39m.\u001b[39;49mbroadcast_coalesced(tensors, devices)\n\u001b[1;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[39m# Use the autograd function to broadcast if not detach\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tensors) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/torch/nn/parallel/comm.py:58\u001b[0m, in \u001b[0;36mbroadcast_coalesced\u001b[0;34m(tensors, devices, buffer_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m devices \u001b[39m=\u001b[39m [_get_device_index(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m devices]\n\u001b[1;32m     57\u001b[0m tensors \u001b[39m=\u001b[39m [_handle_complex(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tensors]\n\u001b[0;32m---> 58\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_broadcast_coalesced(tensors, devices, buffer_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.73 GiB (GPU 0; 10.76 GiB total capacity; 5.79 GiB already allocated; 1.17 GiB free; 5.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl \n",
    "import torch.nn.functional as F \n",
    "from wwv.architecture import Architecture \n",
    "from wwv.eval import Metric\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from wwv.util import OnnxExporter\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor, ModelPruning\n",
    "from wwv.data import AudioDataModule\n",
    "import torch \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "data_path = DataPaths(cfg.path['data_dir'], cfg.model_name, cfg.path['model_dir'])\n",
    "\n",
    "cfg = Config(params)\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "# model = Architecture(cfg, True)\n",
    "data_module = AudioDataModule(data_path.root_data_dir + \"/train.csv\",\n",
    "                              data_path.root_data_dir + \"/val.csv\",\n",
    "                              data_path.root_data_dir + \"/test.csv\",\n",
    "                              cfg=cfg)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "# model.processing_layer[3](x)\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    \n",
    "    def __init__(self, model, cfg):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.metric = Metric\n",
    "        self.cfg = cfg\n",
    "        self.lr = 1e-3\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self.model(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        metrics = self.metric(y_hat, y, self.cfg)()\n",
    "        return {\"loss\":loss, \"train_ttr\": metrics.ttr, \"train_ftr\": metrics.ftr, \"train_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['loss'].float().item() for x in training_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['train_ttr'].float().mean().item() for x in training_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['train_ftr'].float().mean().item() for x in training_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['train_acc'].float().mean().item() for x in training_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self.model(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y,self.cfg)()\n",
    "        return {\"val_loss\": loss, \"val_ttr\": metrics.ttr, \"val_ftr\": metrics.ftr, \"val_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].float().mean().item() for x in validation_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['val_ttr'].float().mean().item() for x in validation_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['val_ftr'].float().mean().item() for x in validation_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['val_acc'].float().mean().item() for x in validation_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self.model(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y, self.cfg)()\n",
    "        return {\"test_ttr\": metrics.ttr, \"test_ftr\": metrics.ftr, \"test_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        results = {\n",
    "            \"ttr\": torch.tensor([x['test_ttr'].float().mean().item() for x in test_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['test_ftr'].float().mean().item() for x in test_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['test_acc'].float().mean().item() for x in test_step_outputs]).mean()\n",
    "            }\n",
    "\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"test_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "early_stopping = EarlyStopping(mode=\"min\", monitor='val_loss', patience=25)\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                        dirpath=data_path.model_dir,\n",
    "                                        save_top_k=1,\n",
    "                                        mode=\"min\",\n",
    "                                        filename='{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_ttr:.2f}-{val_ftr:.2f}')\n",
    "\n",
    "model = DeepSpeech()\n",
    "# model = ResNet(block=Bottleneck, num_blocks=[8, 8, 36, 3], cfg=cfg)\n",
    "\n",
    "# parameters_to_prune = [(model.classifier, \"weight\")]\n",
    "# ModelPruning(pruning_fn=\"l1_unstructured\",parameters_to_prune=parameters_to_prune,amount=0.01,use_global_unstructured=True)\n",
    "callbacks = [checkpoint_callback, lr_monitor, early_stopping]\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\",\n",
    "                  devices=3,\n",
    "                  strategy='dp',\n",
    "                  logger = logger, \n",
    "                  default_root_dir=data_path.model_dir,\n",
    "                  callbacks=callbacks)\n",
    "                #   num_sanity_val_steps=-1)\n",
    "trainer.fit(Routine(model, cfg), train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "trainer.test(dataloaders=test_loader)\n",
    "\n",
    "# wrapper wraps model to output prob instead of logits\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits =self.model(x)\n",
    "        pred = F.sigmoid(logits)\n",
    "        return pred \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from wwv.util import OnnxExporter\n",
    "model = trainer.model.module.module.model\n",
    "predictor = Predictor(model)\n",
    "OnnxExporter( model=predictor,\n",
    "             cfg=cfg, \n",
    "             output_dir=data_path.model_dir)()\n",
    "\n",
    "\n",
    "# if isinstance(trainer.model, torch.nn.DataParallel):\n",
    "#     print(\"test\")\n",
    "#     model = trainer.model\n",
    "#####################################################################################################################\n",
    "# reload best \n",
    "#####################################################################################################################\n",
    "# automatically auto-loads the best weights from the previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "{\n",
    "  \"error\": False,\n",
    "  \"result\": {\n",
    "    \"wake_word_probability\": 0.4020392894744873,\n",
    "    \"prediction\": 0,\n",
    "    \"false_alarm_probability\": 0.5979607105255127,\n",
    "    \"decision_threshold\": 0.5,\n",
    "    \"wwvm_version\": \"test-version-name-set-inside-global-config\",\n",
    "    \"inference_time\": 0.08466243743896484\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "# Run learning rate finder\n",
    "model = ResNet(block=Bottleneck, num_blocks=[3, 8, 36, 3], cfg=cfg)\n",
    "model = Routine(model, cfg)\n",
    "\n",
    "lr_finder = trainer.tuner.lr_find(model)\n",
    "\n",
    "# Results can be found in\n",
    "lr_finder.results\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "# new_lr = lr_finder.suggestion()\n",
    "\n",
    "# # update hparams of the model\n",
    "# model.hparams.lr = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TensorRT \n",
    "rt = {\n",
    "  \"error\": False,\n",
    "  \"result\": {\n",
    "    \"wake_word_probability\": 0,\n",
    "    \"prediction\": 0,\n",
    "    \"false_alarm_probability\": 1,\n",
    "    \"decision_threshold\": 0.5,\n",
    "    \"wwvm_version\": \"docker-env-model-version\",\n",
    "    \"inference_time\": 0.01810431480407715\n",
    "  }\n",
    "}\n",
    "\n",
    "# with CPU\n",
    "cpu = {\n",
    "  \"error\": False,\n",
    "  \"result\": {\n",
    "    \"wake_word_probability\": 0,\n",
    "    \"prediction\": 0,\n",
    "    \"false_alarm_probability\": 1,\n",
    "    \"decision_threshold\": 0.5,\n",
    "    \"wwvm_version\": \"docker-env-model-version\",\n",
    "    \"inference_time\": 0.08728623390197754\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "# with cuda \n",
    "cuda = {\n",
    "  \"error\": False,\n",
    "  \"result\": {\n",
    "    \"wake_word_probability\": 0,\n",
    "    \"prediction\": 0,\n",
    "    \"false_alarm_probability\": 1,\n",
    "    \"decision_threshold\": 0.5,\n",
    "    \"wwvm_version\": \"docker-env-model-version\",\n",
    "    \"inference_time\": 0.022240400314331055\n",
    "  }\n",
    "}\n",
    "\n",
    "def get_factor(d1,d2):\n",
    "  return d1['result']['inference_time'] / d2['result']['inference_time']\n",
    "\n",
    "\n",
    "print(f\"Cuda {get_factor(cpu,cuda):.2f} faster than cpu\")\n",
    "print(f\"TensorRT {get_factor(cpu,rt):.2f} faster than cpu\")\n",
    "print(f\"TensorRT {get_factor(cuda,rt):.2f} faster than cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "# PATH = \"/home/akinwilson/Code/pytorch/output/model/epoch=27-val_loss=0.16-val_acc=0.97.ckpt\"\n",
    "# model.load_state_dict(torch.load(PATH), map_location=torch.device('cpu'))\n",
    "# trainer.test(test_loader, ckpt_path='best')\n",
    "from torch import tensor \n",
    "# ftrs = [x['train_ftr'].mean().item() for x in training_step_outputs]\n",
    "# accs = [x['train_acc'].mean().item() for x in training_step_outputs]\n",
    "# losses\n",
    "# ttrs\n",
    "# results = {\"avg_loss\": statistics.fmean([x['loss'].item() for x in training_step_outputs]),}\n",
    "            # \"avg_ttr\": torch.stack([x['train_ttr'].mean().item() for x in training_step_outputs]).mean(),\n",
    "            # \"avg_ftr\": torch.stack([x['train_ftr'].mean().item() for x in training_step_outputs]).mean(),\n",
    "            # \"avg_acc\": torch.stack([x['train_acc'].mean().item() for x in training_step_outputs]).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class dataset(Dataset):\n",
    "  def __init__(self,x,y):\n",
    "    self.x = torch.tensor(x,dtype=torch.float32)\n",
    "    self.y = torch.tensor(y,dtype=torch.float32)\n",
    "    self.length = self.x.shape[0]\n",
    " \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x[idx],self.y[idx]\n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "\n",
    "xs =torch.ones(64, 48000)\n",
    "ys = torch.ones(64)\n",
    "\n",
    "trainset = dataset(xs,ys)\n",
    "#DataLoader\n",
    "trainloader = DataLoader(trainset,batch_size=64,shuffle=False)\n",
    "for b in trainloader:\n",
    "  print(b[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-T3BHxh3q",
   "language": "python",
   "name": "pytorch-t3bhxh3q"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
