{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing onnx exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -e ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 32000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,3\"\n",
    "\n",
    "from wwv.Architecture.ResNet.model import ResNet\n",
    "from wwv.Architecture.HTSwin.model import HTSwinTransformer\n",
    "# from wwv.Architecture.DeepSpeech.model import DeepSpeech\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "# from wwv.architecture import Architecture\n",
    "from wwv.eval import Metric\n",
    "import statistics\n",
    "from wwv.data import AudioDataModule\n",
    "import wwv.config as cfg \n",
    "\n",
    "# from wwv.config import Config, FittingCfg, DataPathCfg, ResNetCfg\n",
    "from wwv.meta import params as params \n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank, DFT\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.is_available()\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_model = cfg.HTSwin() # cfg.ResNet()\n",
    "data_path = cfg.DataPath(\"/home/akinwilson/Code/HTS-Audio-Transformer\", cfg_model.model_name, cfg_model.model_dir)\n",
    "\n",
    "root = \"/home/akinwilson/Code/pytorch/dataset/keywords\"\n",
    "data_module = AudioDataModule(data_path.root_data_dir,\n",
    "                               cfg_model=cfg_model,\n",
    "                               cfg_feature=cfg_feature,\n",
    "                               cfg_fitting=cfg_fitting)\n",
    "\n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "x = next(iter(train_loader))\n",
    "x['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,3]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | HTSwinTransformer | 6.8 M \n",
      "--------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "279 K     Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.051    Total estimated model params size (MB)\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | HTSwinTransformer | 6.8 M \n",
      "--------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "279 K     Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.051    Total estimated model params size (MB)\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | HTSwinTransformer | 6.8 M \n",
      "--------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "279 K     Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.051    Total estimated model params size (MB)\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | HTSwinTransformer | 6.8 M \n",
      "--------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "279 K     Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.051    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   6%|â–Œ         | 31/562 [00:09<02:41,  3.30it/s, loss=0.646, v_num=1]"
     ]
    }
   ],
   "source": [
    "from wwv.eval import Metric\n",
    "from wwv.util import OnnxExporter\n",
    "from wwv.routine import Routine\n",
    "import bisect \n",
    "import torch \n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl \n",
    "import torch.nn.functional as F \n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor, ModelPruning\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "data_path = cfg.DataPath(\"/home/akinwilson/Code/HTS-Audio-Transformer\", cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir,\n",
    "                               cfg_model=cfg_model,\n",
    "                               cfg_feature=cfg_feature,\n",
    "                               cfg_fitting=cfg_fitting)\n",
    "\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "root = \"/home/akinwilson/Code/pytorch/dataset/keywords\"\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "# model = Architecture(cfg, True)\n",
    "                \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "\n",
    "# model.processing_layer[3](x)\n",
    "def get_callbacks():\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    early_stopping = EarlyStopping(mode=\"min\", monitor='val_loss', patience=cfg_fitting.es_patience)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                            dirpath=data_path.model_dir,\n",
    "                                            save_top_k=1,\n",
    "                                            mode=\"min\",\n",
    "                                            filename='{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_ttr:.2f}-{val_ftr:.2f}')\n",
    "    callbacks = [checkpoint_callback, lr_monitor, early_stopping]\n",
    "    return callbacks \n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = HTSwinTransformer(\n",
    "    spec_size=cfg_model.spec_size,\n",
    "    patch_size=cfg_model.patch_size,\n",
    "    in_chans=1,\n",
    "    num_classes=cfg_model.num_classes,\n",
    "    window_size=cfg_model.window_size,\n",
    "    cfg_signal= cfg_signal, \n",
    "    depths = cfg_model.depth,\n",
    "    embed_dim = cfg_model.dim,\n",
    "    patch_stride = cfg_model.stride,\n",
    "    num_heads= cfg_model.num_head\n",
    ")\n",
    "\n",
    "\n",
    "routine = Routine(model, cfg_fitting, cfg_model)\n",
    "trainer = Trainer(accelerator=\"gpu\",\n",
    "                  devices=3,\n",
    "                  strategy='dp',\n",
    "                  sync_batchnorm = True,\n",
    "                  logger = logger, \n",
    "                  default_root_dir=data_path.model_dir,\n",
    "                  callbacks=get_callbacks(),\n",
    "                  num_sanity_val_steps = 2,\n",
    "                  resume_from_checkpoint = None, \n",
    "                  gradient_clip_val=1.0,\n",
    "                 fast_dev_run=False)\n",
    "\n",
    "\n",
    "# PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "trainer.fit(routine, train_dataloaders=train_loader, val_dataloaders=val_loader) # ,ckpt_path=PATH)\n",
    "\n",
    "trainer.test(dataloaders=test_loader)\n",
    "\n",
    "\n",
    "from wwv.util import OnnxExporter\n",
    "import torch.nn as nn\n",
    "model = trainer.model.module.module.model\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits =self.model(x)\n",
    "        pred = F.sigmoid(logits)\n",
    "        return pred \n",
    "\n",
    "predictor = Predictor(model)\n",
    "OnnxExporter(model=predictor,\n",
    "             cfg=cfg,\n",
    "             input_shape=(1, 40, 75),\n",
    "             output_dir=data_path.model_dir, op_set=12)()\n",
    "\n",
    "# ####################################################################################################################\n",
    "                                           \n",
    "# ####################################################################################################################\n",
    "# if isinstance(trainer.model, torch.nn.DataParallel):\n",
    "#     print(\"test\")\n",
    "#     model = trainer.model\n",
    "# ####################################################################################################################\n",
    "# reload best \n",
    "# ####################################################################################################################\n",
    "# automatically auto-loads the best weights from the previous run \n",
    "# ####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import wwv.config as cfg \n",
    "import torch  \n",
    "import tensorflow as tf \n",
    "\n",
    "torch.cuda.is_available()\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_resnet = cfg.ResNet()\n",
    "\n",
    "model_in_path = \"/home/akinwilson/Code/pytorch/output/model/ResNet/model.onnx\"\n",
    "model_out_path = \"/home/akinwilson/Code/pytorch/notebooks/tf\"\n",
    "out_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite\"\n",
    "out_quant_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite_quant\"\n",
    "\n",
    "onnx_model = onnx.load(model_in_path)  # load onnx model\n",
    "tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "# tf_rep.export_graph(model_out_path)  # export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "model = tf.keras.models.load_model(model_out_path)\n",
    "# model.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader\n",
    "from wwv.meta import params\n",
    "from wwv.data import AudioDataModule\n",
    "Cfg = cfg.Config(params)\n",
    "\n",
    "data_path = cfg.DataPath(\"/home/akinwilson/Code/pytorch/dataset/keywords\", Cfg.model_name, Cfg.path['model_dir'])\n",
    "\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "# model = Architecture(cfg, True)\n",
    "data_module = AudioDataModule(data_path.root_data_dir,\n",
    "                              cfg=Cfg,\n",
    "                              cfg_feature=cfg_feature,\n",
    "                              cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "def get_torch_representative_dataset(test_loader):\n",
    "    representative_x = []\n",
    "    representative_y = []\n",
    "    for batch in test_loader:\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        representative_x.append(x)\n",
    "        representative_y.append(y)\n",
    "\n",
    "\n",
    "    x = torch.vstack(representative_x)\n",
    "    y = torch.stack(representative_y).view(-1,1)\n",
    "    return x,y \n",
    "# representative_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def torch_to_tf_dataset(x, y):\n",
    "    tf_feats = tf.convert_to_tensor(x.numpy())\n",
    "    tf_labels = tf.convert_to_tensor(y.numpy())\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((tf_feats, tf_labels)) \n",
    "    dataset = dataset.concatenate(dataset)\n",
    "    return dataset\n",
    "\n",
    "x, y = get_torch_representative_dataset(test_loader)\n",
    "representative_dataset = torch_to_tf_dataset(x,y)\n",
    "\n",
    "\n",
    "\n",
    "def callable_generator_convertor(_gen):\n",
    "    def gen():\n",
    "        for x,y in _gen:\n",
    "            yield x,y\n",
    "    return gen\n",
    "\n",
    "\n",
    "rep_ds = callable_generator_convertor(representative_dataset)\n",
    "\n",
    "rep_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import lite\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from pathlib import Path \n",
    "import sys\n",
    "import os\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "model_in_path = \"/home/akinwilson/Code/pytorch/output/model/ResNet/model.onnx\"\n",
    "model_out_path = \"/home/akinwilson/Code/pytorch/notebooks/tf\"\n",
    "out_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite\"\n",
    "out_quant_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite_quant\"\n",
    "\n",
    "# onnx_model = onnx.load(model_in_path)  # load onnx model\n",
    "# tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "# tf_rep.export_graph(model_out_path)  # export the model\n",
    "\n",
    "\n",
    "\n",
    "class TfliteConverter:\n",
    "    '''\n",
    "    Converts model in onnx format to TFLite. \n",
    "    '''\n",
    "    def __init__(self, in_path, out_path, out_lite_path, out_lite_quant_path, test_loader, quantise=True):\n",
    "         self.in_path =  in_path\n",
    "         self.out_path = out_path\n",
    "         self.out_lite_path = out_lite_path\n",
    "         self.out_lite_quant_path = out_lite_quant_path\n",
    "         self.test_loader = test_loader\n",
    "         self.quantise=quantise\n",
    "\n",
    "\n",
    "    def get_torch_representative_dataset(self, test_loader):\n",
    "        representative_x = []\n",
    "        representative_y = []\n",
    "        for batch in test_loader:\n",
    "            x = batch['x']\n",
    "            y = batch['y']\n",
    "            representative_x.append(x)\n",
    "            representative_y.append(y)\n",
    "\n",
    "\n",
    "        x = torch.vstack(representative_x)\n",
    "        y = torch.stack(representative_y).view(-1,1)\n",
    "        return x,y \n",
    "\n",
    "    @staticmethod\n",
    "    def callable_generator_convertor(_gen):\n",
    "        def gen():\n",
    "            for x,y in _gen:\n",
    "                yield x,y\n",
    "        return gen\n",
    "\n",
    "    def torch_to_tf_dataset(self, x, y):\n",
    "        tf_feats = tf.convert_to_tensor(x.numpy())\n",
    "        tf_labels = tf.convert_to_tensor(y.numpy())\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((tf_feats, tf_labels))\n",
    "        dataset = dataset.concatenate(dataset)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        # load from onnx and convert to tf \n",
    "        onnx_model = onnx.load(self.in_path)  # load onnx model\n",
    "        tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "        tf_rep.export_graph(self.out_path)  # export the model\n",
    "\n",
    "        # init convert \n",
    "        converter = lite.TFLiteConverter.from_saved_model(self.out_path)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        # convert model in memory \n",
    "        tflite_model = converter.convert()\n",
    "        # save converted in-memory model \n",
    "        with open(out_lite_path, \"wb\") as file_handle: \n",
    "            file_handle.write(tflite_model)\n",
    "\n",
    "        if self.quantise:\n",
    "            # convert torch test set into tf dataset for quantisation purposes \n",
    "            test_loader = self.test_loader\n",
    "            x, y = self.get_torch_representative_dataset(test_loader)\n",
    "            non_callable_tf_dataset = self.torch_to_tf_dataset(x,y)\n",
    "\n",
    "            representative_dataset = TfliteConverter.callable_generator_convertor(non_callable_tf_dataset)\n",
    "            # quantise the model \n",
    "            converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "            converter.inference_input_type = tf.float32\n",
    "            converter.inference_output_type = tf.float32\n",
    "\n",
    "            converter.representative_dataset = representative_dataset\n",
    "\n",
    "            tflite_quant_model = converter.convert()\n",
    "            with open(out_quant_lite_path, \"wb\") as file_handle:\n",
    "\n",
    "                file_handle.write(tflite_quant_model)\n",
    "\n",
    "\n",
    "quantise = True \n",
    "# converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# with open(out_lite_path, \"wb\") as file_handle: \n",
    "#     file_handle.write(tflite_model)\n",
    "\n",
    "if quantise:\n",
    "    # quantise the model \n",
    "    converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.float32\n",
    "    converter.inference_output_type = tf.float32\n",
    "\n",
    "    converter.representative_dataset = callable_generator_convertor(representative_dataset)\n",
    "\n",
    "    tflite_quant_model = converter.convert()\n",
    "    with open(out_quant_lite_path, \"wb\") as file_handle:\n",
    "\n",
    "        file_handle.write(tflite_quant_model)\n",
    "\n",
    "# open(join(model_folder, f'{model_name}_epoch_{result[\"epoch\"]}.tflite'), \"wb\")\n",
    "# Quantized TFLite Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_path = \"/home/akinwilson/Code/pytorch/notebooks/tf\"\n",
    "converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "\n",
    "converter.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, SEWDForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "# dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "class SEW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"anton-l/sew-d-mid-400k-ft-keyword-spotting\")\n",
    "        self.model = SEWDForSequenceClassification.from_pretrained(\"anton-l/sew-d-mid-400k-ft-keyword-spotting\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = input_dict['input_values']\n",
    "        x_feats = self.feature_extractor(x, ampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        x_feats.unsqueeze(1)\n",
    "        logits = self.model(x)\n",
    "        return logits \n",
    "dataset[0][\"audio\"][\"array\"]\n",
    "sew  =SEW()\n",
    "# # audio file is decoded on the fly\n",
    "inputs = dataset[0][\"audio\"][\"array\"]\n",
    "with torch.no_grad():\n",
    "    logits = sew(torch.tensor(inputs))\n",
    "print(logits)\n",
    "# predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "# predicted_label = model.config.id2label[predicted_class_ids]\n",
    "# predicted_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-T3BHxh3q",
   "language": "python",
   "name": "pytorch-t3bhxh3q"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
