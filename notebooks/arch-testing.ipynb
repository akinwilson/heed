{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.config import Config \n",
    "MODEL_DIR = \"/home/akinwilson/Code/pytorch/output/model\"\n",
    "DATA_DIR = \"/home/akinwilson/Code/pytorch/dataset/keywords\"\n",
    "LR_RANGE = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5][1]\n",
    "BATCH_SIZE_RANGE = [1,2,32, 64, 128, 256][2]\n",
    "EPOCH_RANGE = [1, 10, 30, 50, 100, 1000][1]\n",
    "ES_PATIENCE_RANGE = [1, 10, 20, 100, 200][2]\n",
    "MODELS = [\"VecM5\", \"Resnet2vec1D\",\"SpecResnet2D\", \"HSTAT\", \"DeepSpeech\", \"ResNet\"][-1]\n",
    "AUDIO_FEATURE_OPT = [\"spectrogram\", \"mfcc\", \"pcm\"][1]\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"facebook/wav2vec2-base-960h\"\n",
    "AUGS = False\n",
    "\n",
    "params = {\n",
    "    \"audio_duration\":3,\n",
    "    \"sample_rate\":16000,\n",
    "    \"model_name\": MODELS,\n",
    "    \"verbose\": False,\n",
    "    \"path\": {\n",
    "        \"model_dir\": MODEL_DIR,\n",
    "        \"data_dir\": DATA_DIR,\n",
    "        \"pretrained_name_or_path\": PRETRAINED_MODEL_NAME_OR_PATH\n",
    "        },\n",
    "    \"fit_param\": {\"init_lr\":LR_RANGE, \"weight_decay\":0.0001, \"max_epochs\":EPOCH_RANGE, \"gamma\": 0.1,\"es_patience\":ES_PATIENCE_RANGE}, \n",
    "    \"data_param\":{\"train_batch_size\": BATCH_SIZE_RANGE, \"val_batch_size\": BATCH_SIZE_RANGE,\"test_batch_size\": BATCH_SIZE_RANGE}, \n",
    "    \"audio_feature\": AUDIO_FEATURE_OPT,\n",
    "    \"audio_feature_param\": { \"mfcc\":{\"sr\":16000,\"n_mfcc\":20,\"norm\": 'ortho',\"verbose\":True,\"ref\":1.0,\"amin\":1e-10,\"top_db\":80.0,\"hop_length\":512,},\n",
    "                            \"spectrogram\":{\"sr\":16000, \"n_fft\":2048, \"win_length\":None,\"n_mels\":128,\"hop_length\":512,\"window\":'hann',\"center\":True,\"pad_mode\":'reflect',\"power\":2.0,\"htk\":False,\"fmin\":0.0,\"fmax\":None,\"norm\":1,\"trainable_mel\":False,\"trainable_STFT\":False,\"verbose\": True },\n",
    "                            \"pcm\": {}},\n",
    "    \"augmentation\":{'Gain': AUGS, 'PitchShift': AUGS, 'Shift': AUGS},\n",
    "    \"augmentation_param\":{\"Gain\": {  \"min_gain_in_db\":-18.0,\"max_gain_in_db\":  6.0,\"mode\":'per_example',\"p\":1,\"p_mode\":'per_example'},\n",
    "                        \"PitchShift\": {\"min_transpose_semitones\": -4.0, \"max_transpose_semitones\": 4.0,\"mode\":'per_example',\"p\":1,\"p_mode\":'per_example',\"sample_rate\":16000,\"target_rate\": None,\"output_type\": None,},\n",
    "                        \"Shift\":{ \"min_shift\":-0.5,\"max_shift\": 0.5,\"shift_unit\":'fraction',\"rollover\": True,\"mode\":'per_example',\"p\":1,\"p_mode\": 'per_example',\"sample_rate\": 16000,\"target_rate\":None,\"output_type\":None}},\n",
    "    }\n",
    "cfg = Config(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from wwv.architecture import Architecture\n",
    "from wwv.eval import Metric\n",
    "from wwv.data import AudioDataModule\n",
    "from wwv.config import  DataPaths\n",
    "import statistics\n",
    "# data_path = DataPaths(cfg.path['data_dir'], cfg.model_name, cfg.path['model_dir'])\n",
    "cfg = Config(params)\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "root = \"/home/akinwilson/Code/pytorch/dataset/keywords\"\n",
    "# data_module = AudioDataModule(data_path.root_data_dir + \"/train.csv\",\n",
    "#                               data_path.root_data_dir + \"/val.csv\",\n",
    "#                               data_path.root_data_dir + \"/test.csv\",\n",
    "#                                cfg)\n",
    "# # model.processing_layer[3](x)\n",
    "# train_loader=  data_module.train_dataloader()\n",
    "# val_loader=  data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as  nn\n",
    "import torch.nn.functional as F\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from nnAudio import features\n",
    "import torchaudio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.data import AudioDataModule\n",
    "from wwv.config import DataPaths\n",
    "import torchaudio \n",
    "cfg = Config(params)\n",
    "data_path = DataPaths(cfg.path['data_dir'], cfg.model_name, cfg.path['model_dir'])\n",
    "data_module = AudioDataModule(data_path.root_data_dir + \"/train.csv\",\n",
    "                              data_path.root_data_dir + \"/val.csv\",\n",
    "                              data_path.root_data_dir + \"/test.csv\",\n",
    "                              cfg=cfg)\n",
    "\n",
    "# x['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl \n",
    "import torch.nn.functional as F \n",
    "from wwv.architecture import ResNet, Predictor, Bottleneck\n",
    "from wwv.eval import Metric\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from wwv.util import OnnxExporter\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor, ModelPruning\n",
    "from wwv.data import AudioDataModule\n",
    "import torch \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "data_path = DataPaths(cfg.path['data_dir'], cfg.model_name, cfg.path['model_dir'])\n",
    "\n",
    "cfg = Config(params)\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "# model = Architecture(cfg, True)\n",
    "data_module = AudioDataModule(data_path.root_data_dir + \"/train.csv\",\n",
    "                              data_path.root_data_dir + \"/val.csv\",\n",
    "                              data_path.root_data_dir + \"/test.csv\",\n",
    "                              cfg=cfg)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "# model.processing_layer[3](x)\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    \n",
    "    def __init__(self, model, cfg):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.metric = Metric\n",
    "        self.cfg = cfg\n",
    "        self.lr = 1e-3\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self.model(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        metrics = self.metric(y_hat, y, self.cfg)()\n",
    "        return {\"loss\":loss, \"train_ttr\": metrics.ttr, \"train_ftr\": metrics.ftr, \"train_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['loss'].float().item() for x in training_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['train_ttr'].float().mean().item() for x in training_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['train_ftr'].float().mean().item() for x in training_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['train_acc'].float().mean().item() for x in training_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self.model(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y,self.cfg)()\n",
    "        return {\"val_loss\": loss, \"val_ttr\": metrics.ttr, \"val_ftr\": metrics.ftr, \"val_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].float().mean().item() for x in validation_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['val_ttr'].float().mean().item() for x in validation_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['val_ftr'].float().mean().item() for x in validation_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['val_acc'].float().mean().item() for x in validation_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self.model(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y, self.cfg)()\n",
    "        return {\"test_ttr\": metrics.ttr, \"test_ftr\": metrics.ftr, \"test_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        results = {\n",
    "            \"ttr\": torch.tensor([x['test_ttr'].float().mean().item() for x in test_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['test_ftr'].float().mean().item() for x in test_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['test_acc'].float().mean().item() for x in test_step_outputs]).mean()\n",
    "            }\n",
    "\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"test_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "early_stopping = EarlyStopping(mode=\"min\", monitor='val_loss', patience=25)\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                        dirpath=data_path.model_dir,\n",
    "                                        save_top_k=1,\n",
    "                                        mode=\"min\",\n",
    "                                        filename='{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_ttr:.2f}-{val_ftr:.2f}')\n",
    "\n",
    "model = ResNet(block=Bottleneck, num_blocks=[8, 8, 36, 3], cfg=cfg)\n",
    "callbacks = [checkpoint_callback, lr_monitor, early_stopping]\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\",\n",
    "                  devices=3,\n",
    "                  strategy='dp',\n",
    "                  logger = logger, \n",
    "                  default_root_dir=data_path.model_dir,\n",
    "                  callbacks=callbacks)\n",
    "trainer.fit(Routine(model, cfg), train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "trainer.test(dataloaders=test_loader)\n",
    "\n",
    "\n",
    "from wwv.util import OnnxExporter\n",
    "model = trainer.model.module.module.model\n",
    "predictor = Predictor(model)\n",
    "OnnxExporter( model=predictor,\n",
    "             cfg=cfg, \n",
    "             output_dir=data_path.model_dir)()\n",
    "\n",
    "#####################################################################################################################\n",
    "#                                            \n",
    "#####################################################################################################################\n",
    "# if isinstance(trainer.model, torch.nn.DataParallel):\n",
    "#     print(\"test\")\n",
    "#     model = trainer.model\n",
    "#####################################################################################################################\n",
    "# reload best \n",
    "#####################################################################################################################\n",
    "# automatically auto-loads the best weights from the previous run \n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.cfg =cfg\n",
    "        layers = []\n",
    "        kwargs = cfg.audio_feature_param[cfg.audio_feature]\n",
    "        if cfg.audio_feature == \"spectrogram\":\n",
    "            layers.append(features.MelSpectrogram(**kwargs))\n",
    "            # layers.append(T.Resize(224)) # size expected by 2D ResNet \n",
    "        elif cfg.audio_feature == \"mfcc\":\n",
    "            layers.append(features.MFCC(**kwargs))\n",
    "            # layers.append(T.Resize(224)) # size expected by 2D ResNet\n",
    "\n",
    "        # resize inputs\n",
    "        # layers.append(transforms.RandomResizedCrop(224))\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "        # logger.info(f\"{'-'*20}> Features to be extracted: {cfg.audio_feature}\")\n",
    "        # logger.info(f\"{'-'*20}> Feature dimensions: {cfg.processing_output_shape}\")\n",
    "\n",
    "    def forward(self, x:torch.tensor) -> torch.tensor:\n",
    "        x_out = self.net(x)\n",
    "        # if self.cfg.verbose:\n",
    "        #     logger.info(f\"ProcessingLayer().foward() [in]: {x.shape}\")\n",
    "        #     logger.info(f\"ProcessingLayer().foward() [out]: {x_out.shape}\")\n",
    "        return x_out \n",
    "\n",
    "\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "\n",
    "    def __init__(self, n_feats):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "    except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel // 2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel // 2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x  # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super().__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=batch_first,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "from math import prod\n",
    "class DeepSpeech(nn.Module):\n",
    "    def __init__(self, n_cnn_layers=40, n_rnn_layers=1, rnn_dim=1096, stride=2, dropout=0.1,cfg=cfg, **kwargs):\n",
    "        super().__init__()\n",
    "        self.cfg= cfg \n",
    "        n_feats = ( 121, 20) #  self.cfg.processing_output_shape\n",
    "        # self.processing_layer = ProcessingLayer(cfg)\n",
    "        # n_feats =  (121 * 20) // 2\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3 // 2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(\n",
    "            *[ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) for _ in range(n_cnn_layers)]\n",
    "        )\n",
    "        self.fully_connected = nn.Linear(121, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(\n",
    "            *[\n",
    "                BidirectionalGRU(\n",
    "                    rnn_dim=rnn_dim if i == 0 else rnn_dim * 2,\n",
    "                    hidden_size=rnn_dim,\n",
    "                    dropout=dropout,\n",
    "                    batch_first=i == 0,\n",
    "                )\n",
    "                for i in range(n_rnn_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1402880, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Shape b cnn {x.shape}\")\n",
    "        # x = self.processing_layer(x) # (batch, mfcc, timestep)\n",
    "        # print(f\"Shape a process {x.shape}\")\n",
    "        # x = x.unsqueeze(1) # (batch, channel,  mfcc, timestep)\n",
    "        # print(f\"Shape a transpose {x.shape}\")\n",
    "        x = self.cnn(x)\n",
    "        print(f\"Shape a cnn {x.shape}\")\n",
    "        x = self.rescnn_layers(x)\n",
    "        print(f\"Shape a rescnn {x.shape}\")\n",
    "        \n",
    "        # # print(f\"after view {x.shape}\")\n",
    "        # x = x.transpose(1, 2)  # (batch, time, feature)\n",
    "\n",
    "        x = self.fully_connected(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        print(f\"Shape a fully_connected {x.shape}\")\n",
    "        x = self.birnn_layers(x)\n",
    "        print(f\"Shape a birnn_layers {x.shape}\")\n",
    "        # print(f\"after birnn_layers {x.shape}\")\n",
    "        x =  self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, SEWDForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "# dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "class SEW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"anton-l/sew-d-mid-400k-ft-keyword-spotting\")\n",
    "        self.model = SEWDForSequenceClassification.from_pretrained(\"anton-l/sew-d-mid-400k-ft-keyword-spotting\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = input_dict['input_values']\n",
    "        x_feats = self.feature_extractor(x, ampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        x_feats.unsqueeze(1)\n",
    "        logits = self.model(x)\n",
    "        return logits \n",
    "\n",
    "dataset[0][\"audio\"][\"array\"]\n",
    "sew  =SEW()\n",
    "# # audio file is decoded on the fly\n",
    "\n",
    "inputs = dataset[0][\"audio\"][\"array\"]\n",
    "with torch.no_grad():\n",
    "    logits = sew(torch.tensor(inputs))\n",
    "print(logits)\n",
    "# predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "# predicted_label = model.config.id2label[predicted_class_ids]\n",
    "# predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "# Run learning rate finder\n",
    "model = ResNet(block=Bottleneck, num_blocks=[3, 8, 36, 3], cfg=cfg)\n",
    "model = Routine(model, cfg)\n",
    "\n",
    "lr_finder = trainer.tuner.lr_find(model)\n",
    "\n",
    "# Results can be found in\n",
    "lr_finder.results\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "# new_lr = lr_finder.suggestion()\n",
    "\n",
    "# # update hparams of the model\n",
    "# model.hparams.lr = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TensorRT \n",
    "rt = {\n",
    "  \"error\": False,\n",
    "  \"result\": {\n",
    "    \"wake_word_probability\": 0,\n",
    "    \"prediction\": 0,\n",
    "    \"false_alarm_probability\": 1,\n",
    "    \"decision_threshold\": 0.5,\n",
    "    \"wwvm_version\": \"docker-env-model-version\",\n",
    "    \"inference_time\": 0.01810431480407715\n",
    "  }\n",
    "}\n",
    "\n",
    "# with CPU\n",
    "cpu = {\n",
    "  \"error\": False,\n",
    "  \"result\": {\n",
    "    \"wake_word_probability\": 0,\n",
    "    \"prediction\": 0,\n",
    "    \"false_alarm_probability\": 1,\n",
    "    \"decision_threshold\": 0.5,\n",
    "    \"wwvm_version\": \"docker-env-model-version\",\n",
    "    \"inference_time\": 0.08728623390197754\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "# with cuda \n",
    "cuda = {\n",
    "  \"error\": False,\n",
    "  \"result\": {\n",
    "    \"wake_word_probability\": 0,\n",
    "    \"prediction\": 0,\n",
    "    \"false_alarm_probability\": 1,\n",
    "    \"decision_threshold\": 0.5,\n",
    "    \"wwvm_version\": \"docker-env-model-version\",\n",
    "    \"inference_time\": 0.022240400314331055\n",
    "  }\n",
    "}\n",
    "\n",
    "def get_factor(d1,d2):\n",
    "  return d1['result']['inference_time'] / d2['result']['inference_time']\n",
    "\n",
    "\n",
    "print(f\"Cuda {get_factor(cpu,cuda):.2f} faster than cpu\")\n",
    "print(f\"TensorRT {get_factor(cpu,rt):.2f} faster than cpu\")\n",
    "print(f\"TensorRT {get_factor(cuda,rt):.2f} faster than cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "# PATH = \"/home/akinwilson/Code/pytorch/output/model/epoch=27-val_loss=0.16-val_acc=0.97.ckpt\"\n",
    "# model.load_state_dict(torch.load(PATH), map_location=torch.device('cpu'))\n",
    "# trainer.test(test_loader, ckpt_path='best')\n",
    "from torch import tensor \n",
    "# ftrs = [x['train_ftr'].mean().item() for x in training_step_outputs]\n",
    "# accs = [x['train_acc'].mean().item() for x in training_step_outputs]\n",
    "# losses\n",
    "# ttrs\n",
    "# results = {\"avg_loss\": statistics.fmean([x['loss'].item() for x in training_step_outputs]),}\n",
    "            # \"avg_ttr\": torch.stack([x['train_ttr'].mean().item() for x in training_step_outputs]).mean(),\n",
    "            # \"avg_ftr\": torch.stack([x['train_ftr'].mean().item() for x in training_step_outputs]).mean(),\n",
    "            # \"avg_acc\": torch.stack([x['train_acc'].mean().item() for x in training_step_outputs]).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class dataset(Dataset):\n",
    "  def __init__(self,x,y):\n",
    "    self.x = torch.tensor(x,dtype=torch.float32)\n",
    "    self.y = torch.tensor(y,dtype=torch.float32)\n",
    "    self.length = self.x.shape[0]\n",
    " \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x[idx],self.y[idx]\n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "\n",
    "xs =torch.ones(64, 48000)\n",
    "ys = torch.ones(64)\n",
    "\n",
    "trainset = dataset(xs,ys)\n",
    "#DataLoader\n",
    "trainloader = DataLoader(trainset,batch_size=64,shuffle=False)\n",
    "for b in trainloader:\n",
    "  print(b[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-T3BHxh3q",
   "language": "python",
   "name": "pytorch-t3bhxh3q"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
