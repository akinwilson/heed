{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing onnx exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -e ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,3\"\n",
    "\n",
    "from wwv.Architecture.ResNet.model import ResNet\n",
    "from wwv.Architecture.HTSwin.model import HTSwinTransformer\n",
    "# from wwv.Architecture.DeepSpeech.model import DeepSpeech\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "# from wwv.architecture import Architecture\n",
    "from wwv.eval import Metric\n",
    "import statistics\n",
    "from wwv.data import AudioDataModule\n",
    "import wwv.config as cfg \n",
    "\n",
    "# from wwv.config import Config, FittingCfg, DataPathCfg, ResNetCfg\n",
    "from wwv.meta import params as params \n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank, DFT\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "from wwv.eval import Metric\n",
    "from wwv.util import OnnxExporter\n",
    "from wwv.routine import Routine\n",
    "import bisect \n",
    "import torch \n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl \n",
    "import torch.nn.functional as F \n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor, ModelPruning\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.cuda.is_available()\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_model1 = cfg.HTSwin() # cfg.ResNet()\n",
    "cfg_model2 = cfg.ResNet()\n",
    "\n",
    "\n",
    "class Fitter:\n",
    "\n",
    "    def __init__(self, model, cfg_model, cfg, data_path=\"/home/akinwilson/Code/HTS-Audio-Transformer\") -> None:\n",
    "        self.model = model\n",
    "        self.cfg_model = cfg_model\n",
    "        self.cfg_fitting = cfg.Fitting()\n",
    "        self.cfg_signal = cfg.Signal()\n",
    "        self.cfg_feature = cfg.Feature()\n",
    "        self.data_path = cfg.DataPath(data_path, self.cfg_model.model_name, self.cfg_model.model_dir)\n",
    "\n",
    "    def setup(self):\n",
    "        data_module = AudioDataModule(self.data_path.root_data_dir,\n",
    "                                    cfg_model=self.cfg_model,\n",
    "                                    cfg_feature=self.cfg_feature,\n",
    "                                    cfg_fitting=self.cfg_fitting)\n",
    "\n",
    "        train_loader =  data_module.train_dataloader()\n",
    "        val_loader =  data_module.val_dataloader()\n",
    "        test_loader =  data_module.test_dataloader()\n",
    "    \n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "    def get_callbacks(self):\n",
    "        lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "        early_stopping = EarlyStopping(mode=\"min\", monitor='val_loss', patience=self.cfg_fitting.es_patience)\n",
    "        checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                                dirpath=self.data_path.model_dir,\n",
    "                                                save_top_k=1,\n",
    "                                                mode=\"min\",\n",
    "                                                filename='{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_ttr:.2f}-{val_ftr:.2f}')\n",
    "        callbacks = [checkpoint_callback, lr_monitor, early_stopping]\n",
    "        return callbacks \n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        logger = TensorBoardLogger(save_dir=self.data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "        Model = self.model\n",
    "\n",
    "        if self.cfg_model.model_name == \"HSTAT\":\n",
    "            kwargs = { \"spec_size\":self.cfg_model.spec_size,\n",
    "                \"patch_size\":self.cfg_model.patch_size,\n",
    "                \"in_chans\":1,\n",
    "                \"num_classes\":self.cfg_model.num_classes,\n",
    "                \"window_size\":self.cfg_model.window_size,\n",
    "                \"cfg_signal\":self.cfg_signal, \n",
    "                \"depths\":self.cfg_model.depth,\n",
    "                \"embed_dim\":self.cfg_model.dim,\n",
    "                \"patch_stride\":self.cfg_model.stride,\n",
    "                \"num_heads\": self.cfg_model.num_head}\n",
    "        else:\n",
    "            kwargs = {\"num_blocks\":self.cfg_model.num_blocks,\"dropout\":0.2}\n",
    "        \n",
    "\n",
    "        train_loader, val_loader, test_loader = self.setup()\n",
    "        model = Model(**kwargs)\n",
    "        routine = Routine(model, self.cfg_fitting, self.cfg_model)\n",
    "        trainer = Trainer(accelerator=\"gpu\",\n",
    "                        devices=3,\n",
    "                        strategy='dp',\n",
    "                        sync_batchnorm = True,\n",
    "                        logger = logger, \n",
    "                        default_root_dir=self.data_path.model_dir,\n",
    "                        callbacks=self.get_callbacks(),\n",
    "                        num_sanity_val_steps = 2,\n",
    "                        resume_from_checkpoint = None, \n",
    "                        gradient_clip_val=1.0,\n",
    "                        fast_dev_run=False)\n",
    "\n",
    "        # PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "        trainer.fit(routine, train_dataloaders=train_loader, val_dataloaders=val_loader) # ,ckpt_path=PATH)\n",
    "        trainer.test(dataloaders=test_loader)\n",
    "\n",
    "\n",
    "\n",
    "Fitter(HTSwinTransformer, cfg_model1, cfg)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,3\"\n",
    "\n",
    "from wwv.Architecture.ResNet.model import ResNet\n",
    "from wwv.Architecture.HTSwin.model import HTSwinTransformer\n",
    "# from wwv.Architecture.DeepSpeech.model import DeepSpeech\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "# from wwv.architecture import Architecture\n",
    "from wwv.eval import Metric\n",
    "import statistics\n",
    "from wwv.data import AudioDataModule\n",
    "import wwv.config as cfg \n",
    "\n",
    "# from wwv.config import Config, FittingCfg, DataPathCfg, ResNetCfg\n",
    "from wwv.meta import params as params \n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank, DFT\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "from wwv.eval import Metric\n",
    "from wwv.util import OnnxExporter\n",
    "from wwv.routine import Routine\n",
    "import bisect \n",
    "import torch \n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl \n",
    "import torch.nn.functional as F \n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor, ModelPruning\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.cuda.is_available()\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_model = cfg.HTSwin() # cfg.ResNet()\n",
    "cfg_model2 = cfg.ResNet()\n",
    "\n",
    "\n",
    "data_path = cfg.DataPath(\"/home/akinwilson/Code/HTS-Audio-Transformer\", cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir,\n",
    "                               cfg_model=cfg_model,\n",
    "                               cfg_feature=cfg_feature,\n",
    "                               cfg_fitting=cfg_fitting)\n",
    "\n",
    "\n",
    "import copy \n",
    "\n",
    "test_loader = data_module.test_dataloader()\n",
    "x = next(iter(test_loader))\n",
    "input_shape = tuple(x['x'].shape[1:])\n",
    "input_shape = copy.deepcopy(input_shape)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akinwilson/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-05 05:10:19.332431: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-05 05:10:19.475326: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-05 05:10:20.033753: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-05 05:10:20.033803: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-05 05:10:20.033807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Created a temporary directory at /tmp/tmp0oa2jea8\n",
      "Created a temporary directory at /tmp/tmp0oa2jea8\n",
      "Writing /tmp/tmp0oa2jea8/_remote_module_non_scriptable.py\n",
      "Writing /tmp/tmp0oa2jea8/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m EarlyStopping,ModelCheckpoint,LearningRateMonitor, ModelPruning\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlr_scheduler\u001b[39;00m \u001b[39mimport\u001b[39;00m ReduceLROnPlateau\n\u001b[0;32m---> 15\u001b[0m data_path \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mDataPath(\u001b[39m\"\u001b[39m\u001b[39m/home/akinwilson/Code/HTS-Audio-Transformer\u001b[39m\u001b[39m\"\u001b[39m, cfg_model\u001b[39m.\u001b[39mmodel_name, cfg_model\u001b[39m.\u001b[39mmodel_dir)\n\u001b[1;32m     16\u001b[0m data_module \u001b[39m=\u001b[39m AudioDataModule(data_path\u001b[39m.\u001b[39mroot_data_dir,\n\u001b[1;32m     17\u001b[0m                                cfg_model\u001b[39m=\u001b[39mcfg_model,\n\u001b[1;32m     18\u001b[0m                                cfg_feature\u001b[39m=\u001b[39mcfg_feature,\n\u001b[1;32m     19\u001b[0m                                cfg_fitting\u001b[39m=\u001b[39mcfg_fitting)\n\u001b[1;32m     21\u001b[0m \u001b[39m# model = Architecture(cfg, training=True)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "from wwv.eval import Metric\n",
    "from wwv.util import OnnxExporter\n",
    "from wwv.routine import Routine\n",
    "import bisect \n",
    "import torch \n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl \n",
    "import torch.nn.functional as F \n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor, ModelPruning\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "data_path = cfg.DataPath(\"/home/akinwilson/Code/HTS-Audio-Transformer\", cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir,\n",
    "                               cfg_model=cfg_model,\n",
    "                               cfg_feature=cfg_feature,\n",
    "                               cfg_fitting=cfg_fitting)\n",
    "\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "root = \"/home/akinwilson/Code/pytorch/dataset/keywords\"\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "# model = Architecture(cfg, True)\n",
    "                \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "\n",
    "# model.processing_layer[3](x)\n",
    "def get_callbacks():\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    early_stopping = EarlyStopping(mode=\"min\", monitor='val_loss', patience=cfg_fitting.es_patience)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                            dirpath=data_path.model_dir,\n",
    "                                            save_top_k=1,\n",
    "                                            mode=\"min\",\n",
    "                                            filename='{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_ttr:.2f}-{val_ftr:.2f}')\n",
    "    callbacks = [checkpoint_callback, lr_monitor, early_stopping]\n",
    "    return callbacks \n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = HTSwinTransformer(\n",
    "#     spec_size=cfg_model.spec_size,\n",
    "#     patch_size=cfg_model.patch_size,\n",
    "#     in_chans=1,\n",
    "#     num_classes=cfg_model.num_classes,\n",
    "#     window_size=cfg_model.window_size,\n",
    "#     cfg_signal= cfg_signal, \n",
    "#     depths = cfg_model.depth,\n",
    "#     embed_dim = cfg_model.dim,\n",
    "#     patch_stride = cfg_model.stride,\n",
    "#     num_heads= cfg_model.num_head\n",
    "# )\n",
    "\n",
    "\n",
    "# routine = Routine(model, cfg_fitting, cfg_model)\n",
    "# trainer = Trainer(accelerator=\"gpu\",\n",
    "#                   devices=3,\n",
    "#                   strategy='dp',\n",
    "#                   sync_batchnorm = True,\n",
    "#                   logger = logger, \n",
    "#                   default_root_dir=data_path.model_dir,\n",
    "#                   callbacks=get_callbacks(),\n",
    "#                   num_sanity_val_steps = 2,\n",
    "#                   resume_from_checkpoint = None, \n",
    "#                   gradient_clip_val=1.0,\n",
    "#                  fast_dev_run=False)\n",
    "\n",
    "\n",
    "# # PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "# trainer.fit(routine, train_dataloaders=train_loader, val_dataloaders=val_loader) # ,ckpt_path=PATH)\n",
    "\n",
    "# trainer.test(dataloaders=test_loader)\n",
    "\n",
    "\n",
    "# from wwv.util import OnnxExporter\n",
    "# import torch.nn as nn\n",
    "# model = trainer.model.module.module.model\n",
    "\n",
    "# class Predictor(nn.Module):\n",
    "#     def __init__(self, model):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         logits =self.model(x)\n",
    "#         pred = F.sigmoid(logits)\n",
    "#         return pred \n",
    "\n",
    "# predictor = Predictor(model)\n",
    "# OnnxExporter(model=predictor,\n",
    "#              cfg=cfg,\n",
    "#              input_shape=(1, 40, 75),\n",
    "#              output_dir=data_path.model_dir, op_set=12)()\n",
    "\n",
    "# # ####################################################################################################################\n",
    "                                           \n",
    "# ####################################################################################################################\n",
    "# if isinstance(trainer.model, torch.nn.DataParallel):\n",
    "#     print(\"test\")\n",
    "#     model = trainer.model\n",
    "# ####################################################################################################################\n",
    "# reload best \n",
    "# ####################################################################################################################\n",
    "# automatically auto-loads the best weights from the previous run \n",
    "# ####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import wwv.config as cfg \n",
    "import torch  \n",
    "import tensorflow as tf \n",
    "\n",
    "torch.cuda.is_available()\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_resnet = cfg.ResNet()\n",
    "\n",
    "model_in_path = \"/home/akinwilson/Code/pytorch/output/model/ResNet/model.onnx\"\n",
    "model_out_path = \"/home/akinwilson/Code/pytorch/notebooks/tf\"\n",
    "out_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite\"\n",
    "out_quant_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite_quant\"\n",
    "\n",
    "onnx_model = onnx.load(model_in_path)  # load onnx model\n",
    "tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "# tf_rep.export_graph(model_out_path)  # export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "model = tf.keras.models.load_model(model_out_path)\n",
    "# model.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader\n",
    "from wwv.meta import params\n",
    "from wwv.data import AudioDataModule\n",
    "Cfg = cfg.Config(params)\n",
    "\n",
    "data_path = cfg.DataPath(\"/home/akinwilson/Code/pytorch/dataset/keywords\", Cfg.model_name, Cfg.path['model_dir'])\n",
    "\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "# model = Architecture(cfg, True)\n",
    "data_module = AudioDataModule(data_path.root_data_dir,\n",
    "                              cfg=Cfg,\n",
    "                              cfg_feature=cfg_feature,\n",
    "                              cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "def get_torch_representative_dataset(test_loader):\n",
    "    representative_x = []\n",
    "    representative_y = []\n",
    "    for batch in test_loader:\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        representative_x.append(x)\n",
    "        representative_y.append(y)\n",
    "\n",
    "\n",
    "    x = torch.vstack(representative_x)\n",
    "    y = torch.stack(representative_y).view(-1,1)\n",
    "    return x,y \n",
    "# representative_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def torch_to_tf_dataset(x, y):\n",
    "    tf_feats = tf.convert_to_tensor(x.numpy())\n",
    "    tf_labels = tf.convert_to_tensor(y.numpy())\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((tf_feats, tf_labels)) \n",
    "    dataset = dataset.concatenate(dataset)\n",
    "    return dataset\n",
    "\n",
    "x, y = get_torch_representative_dataset(test_loader)\n",
    "representative_dataset = torch_to_tf_dataset(x,y)\n",
    "\n",
    "\n",
    "\n",
    "def callable_generator_convertor(_gen):\n",
    "    def gen():\n",
    "        for x,y in _gen:\n",
    "            yield x,y\n",
    "    return gen\n",
    "\n",
    "\n",
    "rep_ds = callable_generator_convertor(representative_dataset)\n",
    "\n",
    "rep_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import lite\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from pathlib import Path \n",
    "import sys\n",
    "import os\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "model_in_path = \"/home/akinwilson/Code/pytorch/output/model/ResNet/model.onnx\"\n",
    "model_out_path = \"/home/akinwilson/Code/pytorch/notebooks/tf\"\n",
    "out_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite\"\n",
    "out_quant_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite_quant\"\n",
    "\n",
    "# onnx_model = onnx.load(model_in_path)  # load onnx model\n",
    "# tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "# tf_rep.export_graph(model_out_path)  # export the model\n",
    "\n",
    "\n",
    "\n",
    "class TfliteConverter:\n",
    "    '''\n",
    "    Converts model in onnx format to TFLite. \n",
    "    '''\n",
    "    def __init__(self, in_path, out_path, out_lite_path, out_lite_quant_path, test_loader, quantise=True):\n",
    "         self.in_path =  in_path\n",
    "         self.out_path = out_path\n",
    "         self.out_lite_path = out_lite_path\n",
    "         self.out_lite_quant_path = out_lite_quant_path\n",
    "         self.test_loader = test_loader\n",
    "         self.quantise=quantise\n",
    "\n",
    "\n",
    "    def get_torch_representative_dataset(self, test_loader):\n",
    "        representative_x = []\n",
    "        representative_y = []\n",
    "        for batch in test_loader:\n",
    "            x = batch['x']\n",
    "            y = batch['y']\n",
    "            representative_x.append(x)\n",
    "            representative_y.append(y)\n",
    "\n",
    "\n",
    "        x = torch.vstack(representative_x)\n",
    "        y = torch.stack(representative_y).view(-1,1)\n",
    "        return x,y \n",
    "\n",
    "    @staticmethod\n",
    "    def callable_generator_convertor(_gen):\n",
    "        def gen():\n",
    "            for x,y in _gen:\n",
    "                yield x,y\n",
    "        return gen\n",
    "\n",
    "    def torch_to_tf_dataset(self, x, y):\n",
    "        tf_feats = tf.convert_to_tensor(x.numpy())\n",
    "        tf_labels = tf.convert_to_tensor(y.numpy())\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((tf_feats, tf_labels))\n",
    "        dataset = dataset.concatenate(dataset)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        # load from onnx and convert to tf \n",
    "        onnx_model = onnx.load(self.in_path)  # load onnx model\n",
    "        tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "        tf_rep.export_graph(self.out_path)  # export the model\n",
    "\n",
    "        # init convert \n",
    "        converter = lite.TFLiteConverter.from_saved_model(self.out_path)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        # convert model in memory \n",
    "        tflite_model = converter.convert()\n",
    "        # save converted in-memory model \n",
    "        with open(out_lite_path, \"wb\") as file_handle: \n",
    "            file_handle.write(tflite_model)\n",
    "\n",
    "        if self.quantise:\n",
    "            # convert torch test set into tf dataset for quantisation purposes \n",
    "            test_loader = self.test_loader\n",
    "            x, y = self.get_torch_representative_dataset(test_loader)\n",
    "            non_callable_tf_dataset = self.torch_to_tf_dataset(x,y)\n",
    "\n",
    "            representative_dataset = TfliteConverter.callable_generator_convertor(non_callable_tf_dataset)\n",
    "            # quantise the model \n",
    "            converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "            converter.inference_input_type = tf.float32\n",
    "            converter.inference_output_type = tf.float32\n",
    "\n",
    "            converter.representative_dataset = representative_dataset\n",
    "\n",
    "            tflite_quant_model = converter.convert()\n",
    "            with open(out_quant_lite_path, \"wb\") as file_handle:\n",
    "\n",
    "                file_handle.write(tflite_quant_model)\n",
    "\n",
    "\n",
    "quantise = True \n",
    "# converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# with open(out_lite_path, \"wb\") as file_handle: \n",
    "#     file_handle.write(tflite_model)\n",
    "\n",
    "if quantise:\n",
    "    # quantise the model \n",
    "    converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.float32\n",
    "    converter.inference_output_type = tf.float32\n",
    "\n",
    "    converter.representative_dataset = callable_generator_convertor(representative_dataset)\n",
    "\n",
    "    tflite_quant_model = converter.convert()\n",
    "    with open(out_quant_lite_path, \"wb\") as file_handle:\n",
    "\n",
    "        file_handle.write(tflite_quant_model)\n",
    "\n",
    "# open(join(model_folder, f'{model_name}_epoch_{result[\"epoch\"]}.tflite'), \"wb\")\n",
    "# Quantized TFLite Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_path = \"/home/akinwilson/Code/pytorch/notebooks/tf\"\n",
    "converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "\n",
    "converter.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, SEWDForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "# dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "class SEW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"anton-l/sew-d-mid-400k-ft-keyword-spotting\")\n",
    "        self.model = SEWDForSequenceClassification.from_pretrained(\"anton-l/sew-d-mid-400k-ft-keyword-spotting\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = input_dict['input_values']\n",
    "        x_feats = self.feature_extractor(x, ampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        x_feats.unsqueeze(1)\n",
    "        logits = self.model(x)\n",
    "        return logits \n",
    "dataset[0][\"audio\"][\"array\"]\n",
    "sew  =SEW()\n",
    "# # audio file is decoded on the fly\n",
    "inputs = dataset[0][\"audio\"][\"array\"]\n",
    "with torch.no_grad():\n",
    "    logits = sew(torch.tensor(inputs))\n",
    "print(logits)\n",
    "# predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "# predicted_label = model.config.id2label[predicted_class_ids]\n",
    "# predicted_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-T3BHxh3q",
   "language": "python",
   "name": "pytorch-t3bhxh3q"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
