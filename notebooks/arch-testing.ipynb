{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing onnx exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.Architecture.HTSwin.model import HTSwinTransformer\n",
    "import torch \n",
    "import onnx\n",
    "import numpy as np \n",
    "import wwv.config  as cfg \n",
    "torch.cuda.is_available()\n",
    "cfg_htswin = cfg.HTSwin()\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_signal = cfg.Signal()\n",
    "model = HTSwinTransformer(\n",
    "    spec_size=cfg_htswin.spec_size,\n",
    "    patch_size=cfg_htswin.patch_size,\n",
    "    in_chans=1,\n",
    "    num_classes=cfg_htswin.num_classes,\n",
    "    window_size=cfg_htswin.window_size,\n",
    "    cfg_signal= cfg_signal, \n",
    "    depths = cfg_htswin.depth,\n",
    "    embed_dim = cfg_htswin.dim,\n",
    "    patch_stride = cfg_htswin.stride,\n",
    "    num_heads= cfg_htswin.num_head\n",
    ")\n",
    "batch_size = 1 \n",
    "x = torch.randn(batch_size, 48000)\n",
    "model.eval()\n",
    "# Input to the model\n",
    "torch_out = model(x, None, True)\n",
    "filename=  \"test-hst.onnx\"\n",
    "# Export the model\n",
    "torch.onnx.export(model,                     # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  filename,                  # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=17,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})\n",
    "onnx_model = onnx.load(filename)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(filename, providers=['CUDAExecutionProvider'])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")\n",
    "import time \n",
    "s = time.time()\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "f = time.time()\n",
    "print(f-s)\n",
    "# ort_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -e ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,3\"\n",
    "\n",
    "from wwv.Architecture.ResNet.model import ResNet\n",
    "# from wwv.Architecture.HTSwin.model import HTSwinTransformer\n",
    "# from wwv.Architecture.DeepSpeech.model import DeepSpeech\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "# from wwv.architecture import Architecture\n",
    "from wwv.eval import Metric\n",
    "import statistics\n",
    "from wwv.data import AudioDataModule\n",
    "import wwv.config as cfg \n",
    "\n",
    "# from wwv.config import Config, FittingCfg, DataPathCfg, ResNetCfg\n",
    "from wwv.meta import params as params \n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank, DFT\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.is_available()\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_resnet = cfg.ResNet()\n",
    "\n",
    "\n",
    "\n",
    "params['model_name'] = \"ResNet\"\n",
    "params['audio_feature_param'] = \"mfcc\"\n",
    "params[\"audio_duration\"] = 1.5\n",
    "Cfg = cfg.Config(params)\n",
    "\n",
    "\n",
    "data_path = cfg.DataPath(Cfg.path['data_dir'], Cfg.model_name, Cfg.path['model_dir'])\n",
    "\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "root = \"/home/akinwilson/Code/pytorch/dataset/keywords\"\n",
    "data_module = AudioDataModule(data_path.root_data_dir + \"/train.csv\",\n",
    "                              data_path.root_data_dir + \"/val.csv\",\n",
    "                              data_path.root_data_dir + \"/test.csv\",\n",
    "                               Cfg,\n",
    "                               cfg_feature=cfg_feature,\n",
    "                               cfg_fitting=cfg_fitting)\n",
    "\n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "x = next(iter(train_loader))\n",
    "x['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wwv.config as cfg \n",
    "cfg_features = cfg.Feature()\n",
    "from pprint import pprint \n",
    "pprint(cfg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio as ta\n",
    "import torch \n",
    "############################################################################################################\n",
    "# ondevice feature generation \n",
    "############################################################################################################\n",
    "\n",
    "# (time_step x mfcc)\n",
    "# 75x16 for 1.5 seconds \n",
    "\n",
    "\n",
    "\n",
    "audio_duration = 1.5\n",
    "sample_rate = 16000\n",
    "window_len = int( 0.040 * sample_rate )\n",
    "window_step = int( 0.020 *sample_rate ) \n",
    "mel_coefficients = 16\n",
    "mel_filters = 40\n",
    "num_fft = 1024\n",
    "low_freq = 20.\n",
    "high_freq = 8000.\n",
    "# preemph = 0.0,\n",
    "# lifter = 0\n",
    "melspec_kwargs = {\"n_fft\": num_fft,\"win_length\": window_len, \"hop_length\": window_step, \"n_mels\": mel_filters, \"onesided\":True, \"center\": True,\"pad_mode\": \"reflect\",  \"f_min\" : high_freq, \"f_max\" : high_freq}\n",
    "transform = ta.transforms.MFCC(sample_rate=sample_rate, n_mfcc=mel_coefficients, melkwargs=melspec_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "transform(torch.randn(1, int(sample_rate * audio_duration)))[:,:,:int ( (cfg_features.sample_rate * cfg_features.audio_duration) / cfg_features.window_step ) ].shape\n",
    "\n",
    "\n",
    "# Cfg = cfg.Config(params)\n",
    "# data_path = cfg.DataPath(\"/home/akinwilson/Code/HTS-Audio-Transformer\", Cfg.model_name, Cfg.path['model_dir'])\n",
    "\n",
    "# # model = Architecture(cfg, training=True)\n",
    "# # model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "# # model = Architecture(cfg, True)\n",
    "# data_module = AudioDataModule(data_path.root_data_dir + \"/train.csv\",\n",
    "#                               data_path.root_data_dir + \"/val.csv\",\n",
    "#                               data_path.root_data_dir + \"/test.csv\",\n",
    "#                               cfg=Cfg,\n",
    "#                               cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "# train_loader =  data_module.train_dataloader()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.meta import params \n",
    "\n",
    "\n",
    "Cfg = cfg.Config(params)\n",
    "data_path = cfg.DataPath(\"/home/akinwilson/Code/pytorch/dataset/keywords\", Cfg.model_name, Cfg.path['model_dir'])\n",
    "\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "# model = Architecture(cfg, True)\n",
    "data_module = AudioDataModule(data_path.root_data_dir,\n",
    "                              cfg=Cfg,\n",
    "                              cfg_feature=cfg_feature,\n",
    "                              cfg_fitting=cfg_fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3,) + (2,3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.eval import Metric\n",
    "from wwv.util import OnnxExporter\n",
    "from wwv.routine import Routine, HTSwinRoutine\n",
    "import bisect \n",
    "import torch \n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl \n",
    "import torch.nn.functional as F \n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor, ModelPruning\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "params\n",
    "Cfg = cfg.Config(params)\n",
    "data_path = cfg.DataPath(\"/home/akinwilson/Code/pytorch/dataset/keywords\", Cfg.model_name, Cfg.path['model_dir'])\n",
    "\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "# model = Architecture(cfg, True)\n",
    "data_module = AudioDataModule(data_path.root_data_dir,\n",
    "                              cfg=Cfg,\n",
    "                              cfg_feature=cfg_feature,\n",
    "                              cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "# model.processing_layer[3](x)\n",
    "cfg = Cfg \n",
    "def get_callbacks():\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    early_stopping = EarlyStopping(mode=\"min\", monitor='val_loss', patience=cfg_fitting.es_patience)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                            dirpath=data_path.model_dir,\n",
    "                                            save_top_k=1,\n",
    "                                            mode=\"min\",\n",
    "                                            filename='{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_ttr:.2f}-{val_ftr:.2f}')\n",
    "    callbacks = [checkpoint_callback, lr_monitor, early_stopping]\n",
    "    return callbacks \n",
    "\n",
    "\n",
    "model = ResNet(num_blocks=cfg_resnet.num_blocks, cfg=cfg)\n",
    "\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\",\n",
    "                  devices=3,\n",
    "                  strategy='dp',\n",
    "                  logger = logger, \n",
    "                  default_root_dir=data_path.model_dir,\n",
    "                  callbacks=get_callbacks())\n",
    "\n",
    "\n",
    "PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "trainer.fit(Routine(model, cfg), train_dataloaders=train_loader, val_dataloaders=val_loader) # ,ckpt_path=PATH)\n",
    "\n",
    "trainer.test(dataloaders=test_loader)\n",
    "\n",
    "\n",
    "from wwv.util import OnnxExporter\n",
    "import torch.nn as nn\n",
    "model = trainer.model.module.module.model\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits =self.model(x)\n",
    "        pred = F.sigmoid(logits)\n",
    "        return pred \n",
    "\n",
    "predictor = Predictor(model)\n",
    "OnnxExporter(model=predictor,\n",
    "             cfg=cfg,\n",
    "             input_shape=(1, 40, 75),\n",
    "             output_dir=data_path.model_dir, op_set=12)()\n",
    "\n",
    "# ####################################################################################################################\n",
    "                                           \n",
    "# ####################################################################################################################\n",
    "# if isinstance(trainer.model, torch.nn.DataParallel):\n",
    "#     print(\"test\")\n",
    "#     model = trainer.model\n",
    "# ####################################################################################################################\n",
    "# reload best \n",
    "# ####################################################################################################################\n",
    "# automatically auto-loads the best weights from the previous run \n",
    "# ####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 09:24:42.417951: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-04 09:24:42.533309: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-04 09:24:43.072525: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-04 09:24:43.072570: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-04 09:24:43.072575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/akinwilson/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-04 09:24:44.937688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.938154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.938533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.938908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.964024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.964447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.964773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.965107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.965473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.965807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.966137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.966475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:44.967169: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-04 09:24:45.317819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:45.318212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:45.318574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:45.318930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:45.319283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:45.319625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:45.320025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:45.320416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:45.320984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:45.321309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:45.321632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:45.321968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.471004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.471489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.472140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.472505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.472875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.473224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.473565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.473908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.474253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.474606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8888 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-11-04 09:24:46.474959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.475287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9633 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:21:00.0, compute capability: 7.5\n",
      "2022-11-04 09:24:46.475549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.475934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9633 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:4c:00.0, compute capability: 7.5\n",
      "2022-11-04 09:24:46.476217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-04 09:24:46.476616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9569 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:4d:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import wwv.config as cfg \n",
    "import torch  \n",
    "import tensorflow as tf \n",
    "\n",
    "torch.cuda.is_available()\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_resnet = cfg.ResNet()\n",
    "\n",
    "model_in_path = \"/home/akinwilson/Code/pytorch/output/model/ResNet/model.onnx\"\n",
    "model_out_path = \"/home/akinwilson/Code/pytorch/notebooks/tf\"\n",
    "out_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite\"\n",
    "out_quant_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite_quant\"\n",
    "\n",
    "onnx_model = onnx.load(model_in_path)  # load onnx model\n",
    "tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "# tf_rep.export_graph(model_out_path)  # export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "model = tf.keras.models.load_model(model_out_path)\n",
    "# model.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader\n",
    "from wwv.meta import params\n",
    "from wwv.data import AudioDataModule\n",
    "Cfg = cfg.Config(params)\n",
    "\n",
    "data_path = cfg.DataPath(\"/home/akinwilson/Code/pytorch/dataset/keywords\", Cfg.model_name, Cfg.path['model_dir'])\n",
    "\n",
    "# model = Architecture(cfg, training=True)\n",
    "# model.extractor(torch.randn((1,48000))) # (torch.randn((1,48000)))\n",
    "# model = Architecture(cfg, True)\n",
    "data_module = AudioDataModule(data_path.root_data_dir,\n",
    "                              cfg=Cfg,\n",
    "                              cfg_feature=cfg_feature,\n",
    "                              cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "def get_torch_representative_dataset(test_loader):\n",
    "    representative_x = []\n",
    "    representative_y = []\n",
    "    for batch in test_loader:\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        representative_x.append(x)\n",
    "        representative_y.append(y)\n",
    "\n",
    "\n",
    "    x = torch.vstack(representative_x)\n",
    "    y = torch.stack(representative_y).view(-1,1)\n",
    "    return x,y \n",
    "# representative_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def torch_to_tf_dataset(x, y):\n",
    "    tf_feats = tf.convert_to_tensor(x.numpy())\n",
    "    tf_labels = tf.convert_to_tensor(y.numpy())\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((tf_feats, tf_labels)) \n",
    "    dataset = dataset.concatenate(dataset)\n",
    "    return dataset\n",
    "\n",
    "x, y = get_torch_representative_dataset(test_loader)\n",
    "representative_dataset = torch_to_tf_dataset(x,y)\n",
    "\n",
    "\n",
    "\n",
    "def callable_generator_convertor(_gen):\n",
    "    def gen():\n",
    "        for x,y in _gen:\n",
    "            yield x,y\n",
    "    return gen\n",
    "\n",
    "\n",
    "rep_ds = callable_generator_convertor(representative_dataset)\n",
    "\n",
    "rep_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import lite\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from pathlib import Path \n",
    "import sys\n",
    "import os\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "model_in_path = \"/home/akinwilson/Code/pytorch/output/model/ResNet/model.onnx\"\n",
    "model_out_path = \"/home/akinwilson/Code/pytorch/notebooks/tf\"\n",
    "out_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite\"\n",
    "out_quant_lite_path = \"/home/akinwilson/Code/pytorch/notebooks/tflite_quant\"\n",
    "\n",
    "# onnx_model = onnx.load(model_in_path)  # load onnx model\n",
    "# tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "# tf_rep.export_graph(model_out_path)  # export the model\n",
    "\n",
    "\n",
    "\n",
    "class TfliteConverter:\n",
    "    '''\n",
    "    Converts model in onnx format to TFLite. \n",
    "    '''\n",
    "    def __init__(self, in_path, out_path, out_lite_path, out_lite_quant_path, test_loader, quantise=True):\n",
    "         self.in_path =  in_path\n",
    "         self.out_path = out_path\n",
    "         self.out_lite_path = out_lite_path\n",
    "         self.out_lite_quant_path = out_lite_quant_path\n",
    "         self.test_loader = test_loader\n",
    "         self.quantise=quantise\n",
    "\n",
    "\n",
    "    def get_torch_representative_dataset(self, test_loader):\n",
    "        representative_x = []\n",
    "        representative_y = []\n",
    "        for batch in test_loader:\n",
    "            x = batch['x']\n",
    "            y = batch['y']\n",
    "            representative_x.append(x)\n",
    "            representative_y.append(y)\n",
    "\n",
    "\n",
    "        x = torch.vstack(representative_x)\n",
    "        y = torch.stack(representative_y).view(-1,1)\n",
    "        return x,y \n",
    "\n",
    "    @staticmethod\n",
    "    def callable_generator_convertor(_gen):\n",
    "        def gen():\n",
    "            for x,y in _gen:\n",
    "                yield x,y\n",
    "        return gen\n",
    "\n",
    "    def torch_to_tf_dataset(self, x, y):\n",
    "        tf_feats = tf.convert_to_tensor(x.numpy())\n",
    "        tf_labels = tf.convert_to_tensor(y.numpy())\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_tensor_slices((tf_feats, tf_labels))\n",
    "        dataset = dataset.concatenate(dataset)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        # load from onnx and convert to tf \n",
    "        onnx_model = onnx.load(self.in_path)  # load onnx model\n",
    "        tf_rep = prepare(onnx_model)  # prepare tf representation\n",
    "        tf_rep.export_graph(self.out_path)  # export the model\n",
    "\n",
    "        # init convert \n",
    "        converter = lite.TFLiteConverter.from_saved_model(self.out_path)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        # convert model in memory \n",
    "        tflite_model = converter.convert()\n",
    "        # save converted in-memory model \n",
    "        with open(out_lite_path, \"wb\") as file_handle: \n",
    "            file_handle.write(tflite_model)\n",
    "\n",
    "        if self.quantise:\n",
    "            # convert torch test set into tf dataset for quantisation purposes \n",
    "            test_loader = self.test_loader\n",
    "            x, y = self.get_torch_representative_dataset(test_loader)\n",
    "            non_callable_tf_dataset = self.torch_to_tf_dataset(x,y)\n",
    "\n",
    "            representative_dataset = TfliteConverter.callable_generator_convertor(non_callable_tf_dataset)\n",
    "            # quantise the model \n",
    "            converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "            converter.inference_input_type = tf.float32\n",
    "            converter.inference_output_type = tf.float32\n",
    "\n",
    "            converter.representative_dataset = representative_dataset\n",
    "\n",
    "            tflite_quant_model = converter.convert()\n",
    "            with open(out_quant_lite_path, \"wb\") as file_handle:\n",
    "\n",
    "                file_handle.write(tflite_quant_model)\n",
    "\n",
    "\n",
    "quantise = True \n",
    "# converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# with open(out_lite_path, \"wb\") as file_handle: \n",
    "#     file_handle.write(tflite_model)\n",
    "\n",
    "if quantise:\n",
    "    # quantise the model \n",
    "    converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.float32\n",
    "    converter.inference_output_type = tf.float32\n",
    "\n",
    "    converter.representative_dataset = callable_generator_convertor(representative_dataset)\n",
    "\n",
    "    tflite_quant_model = converter.convert()\n",
    "    with open(out_quant_lite_path, \"wb\") as file_handle:\n",
    "\n",
    "        file_handle.write(tflite_quant_model)\n",
    "\n",
    "# open(join(model_folder, f'{model_name}_epoch_{result[\"epoch\"]}.tflite'), \"wb\")\n",
    "# Quantized TFLite Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_path = \"/home/akinwilson/Code/pytorch/notebooks/tf\"\n",
    "converter = lite.TFLiteConverter.from_saved_model(model_out_path)\n",
    "\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "\n",
    "converter.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, SEWDForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "# dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "class SEW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"anton-l/sew-d-mid-400k-ft-keyword-spotting\")\n",
    "        self.model = SEWDForSequenceClassification.from_pretrained(\"anton-l/sew-d-mid-400k-ft-keyword-spotting\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = input_dict['input_values']\n",
    "        x_feats = self.feature_extractor(x, ampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        x_feats.unsqueeze(1)\n",
    "        logits = self.model(x)\n",
    "        return logits \n",
    "dataset[0][\"audio\"][\"array\"]\n",
    "sew  =SEW()\n",
    "# # audio file is decoded on the fly\n",
    "inputs = dataset[0][\"audio\"][\"array\"]\n",
    "with torch.no_grad():\n",
    "    logits = sew(torch.tensor(inputs))\n",
    "print(logits)\n",
    "# predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "# predicted_label = model.config.id2label[predicted_class_ids]\n",
    "# predicted_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-T3BHxh3q",
   "language": "python",
   "name": "pytorch-t3bhxh3q"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
