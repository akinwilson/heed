{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efed102f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# notebook config imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6112835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # set vis gpus \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import pytorch_lightning as pl\n",
    "import wwv.config  as cfg \n",
    "from wwv.util import CallbackCollection \n",
    "from wwv.data import AudioDataModule\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg_fitting = cfg.Fitting(batch_size=64, train_bs=64, val_bs=64)\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_model = cfg.CNNAE()\n",
    "\n",
    "data_path = cfg.DataPath(os.environ['DATA_ROOT'], cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469a162",
   "metadata": {},
   "source": [
    "## Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffdb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, latent_dim=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        \n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(latent_dim, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "    \n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "        return decoded_x\n",
    "\n",
    "x = torch.randn((1,1,32000), device=device)\n",
    "\n",
    "model = CNNAE()\n",
    "model.to(device=device)\n",
    "x_reconstructed = model(x)\n",
    "assert x_reconstructed.shape == x.shape,  f\"The reconstructed input is of different dimensions to the original input. Original: {x_reconstructed.shape}. Reconstructed: {x.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec930ba",
   "metadata": {},
   "source": [
    "## Fitting autoencoder and visualizing embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "import pytorch_lighone_hot_labels\n",
    "class Routine(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Routine for fitting a autoencoder: encoder decoder structure\n",
    "    \"\"\"\n",
    "    def __init__ (self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-4\n",
    "\n",
    "    def encode(self,x):\n",
    "        idx1, idx2, idx3, x_encoded = self.model.encode(x)\n",
    "        return x_encoded,  idx1, idx2, idx3\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        x_reconstructed = self.model.decode( idx1, idx2, idx3, x)\n",
    "        return x_reconstructed \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_encoded,  idx1, idx2, idx3 = self.encode(x)\n",
    "        x_reconstructed  = self.decode( idx1, idx2, idx3, x_encoded)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "\n",
    "    def training_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"loss\": loss }\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "\n",
    "    def validation_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"val_loss\": loss }\n",
    "\n",
    "    def validation_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['val_loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer } # , \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "routine = Routine(model)\n",
    "trainer = Trainer(accelerator=\"gpu\",sync_batchnorm = True, max_epochs = 5 ,num_sanity_val_steps = 2, gradient_clip_val=1.0)\n",
    "# # PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "# # Trainer executes fitting; training and validating proceducres \n",
    "trainer.fit(routine, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# used downstream, need to define object here, check the forward method of the AE_classifier \n",
    "encoder = trainer.model\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TFVisualiser:\n",
    "    '''\n",
    "    Writes latent code generated by model to tensorboard for adding visualisations\n",
    "\n",
    "    NOTE THE BELOW MODEL EXTRACTION EXPECTS THE ROUTINE BE HAVE BEEN FITTED WITH A SINGLE GPU\n",
    "    ----> you'll have to extract the model slightly differently in a distributed environment \n",
    "    '''\n",
    "\n",
    "    def __init__(self, model=trainer.model.model, test_loader=test_loader, sample_size=2000):\n",
    "\n",
    "        self.model = model \n",
    "        self.sample_size = sample_size\n",
    "        # get test data to embedd \n",
    "        xs,ys = [],[]\n",
    "        for batch in test_loader:\n",
    "            xs.append(batch['x'])\n",
    "            ys.append(batch['y'])\n",
    "        X = torch.vstack(xs)\n",
    "        Y = torch.concat(ys)\n",
    "        X_sampled, Y_sampled = self.sample(X,Y) \n",
    "\n",
    "        self.X = X_sampled\n",
    "        self.Y = Y_sampled\n",
    "        self.latent_code_output_dir = 'runs/cnnae'\n",
    "\n",
    "\n",
    "    def sample(self, X, Y):\n",
    "\n",
    "        smaple_permuation_idxs = torch.randperm(X.size(0))\n",
    "        idxs = smaple_permuation_idxs[:self.sample_size]\n",
    "        X_sampled = X[idxs]\n",
    "        Y_sampled = Y[idxs]\n",
    "        return X_sampled, Y_sampled\n",
    "\n",
    "    def save_latent_code(self):\n",
    "        # encode with model \n",
    "        _,_,_, latent_code = self.model.encode(self.X)\n",
    "        label_list = [{1.:\"Wake word\", 0.:\"Not wake word\"}[y] for y in  self.Y.numpy().tolist()]\n",
    "        # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "        writer = SummaryWriter(self.latent_code_output_dir)\n",
    "        # log for visualisation \n",
    "        writer.add_embedding(latent_code, metadata=label_list, tag=\"AudioEmbedding\")\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        from pathlib import Path \n",
    "\n",
    "        print(f\"Saving subsample of {self.sample_size} of test set's latent encodings to location: {Path().cwd() / self.latent_code_output_dir}\")\n",
    "        self.save_latent_code()\n",
    "\n",
    "TFVisualiser()()\n",
    "\n",
    "# Extracting encoder for down stream task.\n",
    "encoder = trainer.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fb668",
   "metadata": {},
   "source": [
    "## Downstream autoencoder application\n",
    "### Vanilla MLP classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class AE_classifier(nn.Module):\n",
    "    '''\n",
    "    Classifier takes latent code and performs predictions using the latent code\n",
    "\n",
    "    Applications:\n",
    "        Upstream feature extraction for memory-constraint classifier \n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,latent_dim, dropout=0.2, compression_factor=3):\n",
    "        '''\n",
    "        Compression factor detemines the intermitten dimension reduction factor of the dense network.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "        self.do_rate = dropout\n",
    "\n",
    "        dense_layer_1_output = int(latent_dim / compression_factor)\n",
    "        dense_layer_2_output = int(dense_layer_1_output / compression_factor)\n",
    "\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            OrderedDict([\n",
    "            (\"DenseLayer1\", nn.Linear(latent_dim, dense_layer_1_output) ) , \n",
    "            (\"relu1\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout1\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer2\", nn.Linear(dense_layer_1_output, dense_layer_2_output) ) , \n",
    "            (\"relu2\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout2\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer3\", nn.Linear(dense_layer_2_output, 1) ),\n",
    "            ])\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder.to(\"cuda\")\n",
    "        x_encoded, _ , _ , _ = encoder.encode(x)\n",
    "        logits = self.layers(x_encoded)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb8e7f",
   "metadata": {},
   "source": [
    "## Autoencoder was fit using training and validation datasets. \n",
    "### Will evaluate performance of representation  / manifold learnt by freezing autoencoder layers an training a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f912f0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set contains: 12015\n",
      "Validation set contains: 5340\n",
      "Testing set contains: 4005\n",
      "Saving to directory: /media/akinwilson/Samsung_T5/data/audio/keyword-spotting/ae_data\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from pathlib import Path \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def create_temporary_fitting_set():\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"ae_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    fitting_set_path = data_out_path.parent / \"test.csv\"\n",
    "    train_test_set, val_set = train_test_split(pd.read_csv(fitting_set_path))\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "\n",
    "create_temporary_fitting_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.data import AudioDataModule\n",
    "from wwv.util import CallbackCollection\n",
    "import wwv.config as cfg\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from wwv.eval import Metric\n",
    "from pathlib import Path \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "cfg_model = cfg.AEClassifier()\n",
    "cfg_fitting = cfg.Fitting(max_epoch= 50, es_patience=10)\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "data_out_path  = str(Path(os.environ['DATA_ROOT']) / \"ae_data\")\n",
    "\n",
    "data_path = cfg.DataPath(data_out_path, cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "# get input shape for onnx exporting\n",
    "input_shape = data_module.input_shape\n",
    "# init model\n",
    "model = AE_classifier(latent_dim=1024)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, cfg_fitting, cfg_model, localization=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.metric = Metric\n",
    "        self.cfg_fitting = cfg_fitting\n",
    "        self.cfg_model = cfg_model\n",
    "        self.localization = localization\n",
    "        self.lr = 1e-3\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x) \n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"loss\":loss, \"train_ttr\": metrics.ttr, \"train_ftr\": metrics.ftr, \"train_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['loss'].item() for x in training_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['train_ttr'] for x in training_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['train_ftr'] for x in training_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['train_acc'] for x in training_step_outputs]).mean()\n",
    "            }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for (k,v) in results.items():\n",
    "        \n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"val_loss\": loss, \"val_ttr\": metrics.ttr, \"val_ftr\": metrics.ftr, \"val_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # from pprint import pprint \n",
    "        # pprint(validation_step_outputs)\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].item() for x in validation_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['val_ttr'] for x in validation_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['val_ftr'] for x in validation_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['val_acc'] for x in validation_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        # (batch_probabilities,)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        # (batch_labels,)\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"test_ttr\": metrics.ttr, \"test_ftr\": metrics.ftr, \"test_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        results = {\n",
    "            \"ttr\": torch.tensor([x['test_ttr'] for x in test_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['test_ftr'] for x in test_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['test_acc'] for x in test_step_outputs]).mean()\n",
    "            }\n",
    "\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"test_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "    \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "routine = Routine(model, cfg_fitting, cfg_model)\n",
    "# Init a trainer to execute routine\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    sync_batchnorm=True,\n",
    "    logger=logger,\n",
    "    max_epochs=cfg_fitting.max_epoch,\n",
    "    callbacks=CallbackCollection(cfg_fitting, data_path)(),\n",
    "    gradient_clip_val=1.0,\n",
    "    fast_dev_run=cfg_fitting.fast_dev_run,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "trainer.test(dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157354d",
   "metadata": {},
   "source": [
    "### Research: Semi-supervised loss : combining the reconstruction and binary cross entropy loss\n",
    "\n",
    "- Combine all available data - such that we have the form:\n",
    "\n",
    "    D = (x, y)\n",
    "    \n",
    "    Where the y $\\in$ $\\{0,1,2\\}$:\n",
    "- Let the loss function be a **piece-wise function** on the domain of the target \n",
    "\n",
    "    \n",
    "$$ Loss(x, x_{recon} y,\\hat{y})=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      MSE(x, x_{recon}), \\text{    }  y \\in \\{2 \\} \\\\\n",
    "      MSE(x, x_{recon}) + BinaryCrossEntropy(y,\\hat{y}), \\text{    }  y \\in \\{0,1\\} \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "We consider then the target label of 2 to be the unknown target label. \n",
    "\n",
    "I need to make sure that both the encoder, decoder and classifier head are updated for the  case of $y \\in \\{0,1\\}$; I am not too sure if this is happening at the moment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path \n",
    "from collections import OrderedDict\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from wwv.data import AudioDataModule\n",
    "from wwv.util import CallbackCollection\n",
    "import wwv.config as cfg\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import bisect\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from wwv.eval import Metric\n",
    "from pathlib import Path \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def semi_supervised_conversion(df, unknown_label_fraction=0.9,  unknown_target_label=2.):\n",
    "\n",
    "    sample_size = df.shape[0]\n",
    "    unknown_label_sample_size = int( sample_size * unknown_label_fraction)  \n",
    "\n",
    "    print(f\"Corrupting {unknown_label_sample_size} samples out of {sample_size}\")\n",
    "    indices = np.random.choice(np.arange(0, sample_size,1), size=unknown_label_sample_size, replace=False)\n",
    "    df.loc[indices,'label'] = 2.\n",
    "    return df \n",
    "\n",
    "def create_temporary_semi_superivsed_fitting_set(unknown_label_fraction=0.9,  unknown_target_label=2.):\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"ss_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    FILES = ['train.csv', 'val.csv', 'test.csv']\n",
    "    fitting_set_paths = [str(data_out_path.parent / fname) for fname in FILES]\n",
    "    df = pd.concat([pd.read_csv(file_path) for file_path in fitting_set_paths])\n",
    "    df.reset_index(drop=True, inplace=True )\n",
    "    df = semi_supervised_conversion(df, unknown_label_fraction,  unknown_target_label)\n",
    "    print(\"New target distribution\")\n",
    "    print(df.label.value_counts())\n",
    "    # return df \n",
    "    train_test_set, val_set = train_test_split(df)\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "\n",
    "create_temporary_semi_superivsed_fitting_set()\n",
    "\n",
    "data_out_path  = Path(os.environ['DATA_ROOT']) / \"ss_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968eee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenseClassifier(nn.Module):\n",
    "    def __init__(self,latent_dim, dropout=0.2, compression_factor=3):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "        self.do_rate = dropout\n",
    "\n",
    "        dense_layer_1_output = int(latent_dim / compression_factor)\n",
    "        dense_layer_2_output = int(dense_layer_1_output / compression_factor)\n",
    "\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            OrderedDict([\n",
    "            (\"DenseLayer1\", nn.Linear(latent_dim, dense_layer_1_output) ) , \n",
    "            (\"relu1\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout1\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer2\", nn.Linear(dense_layer_1_output, dense_layer_2_output) ) , \n",
    "            (\"relu2\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout2\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer3\", nn.Linear(dense_layer_2_output, 1) ),\n",
    "            ])\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class SS_CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, latent_dim=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(latent_dim, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "\n",
    "        self.classifier = DenseClassifier(latent_dim)\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "    def classify(self, x):\n",
    "        _, _, _, x_encoded = self.encode(x)\n",
    "        logits = self.classifier(x_encoded)\n",
    "        return logits \n",
    "\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Autoencoding forward method'''\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        logits = self.classify(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "\n",
    "        return decoded_x, logits # mse and binary cross entropy inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def semi_supervised_loss(x=None, x_recon=None,y=None, y_hat=None):\n",
    "    bs = x.shape[0]\n",
    "    losses = []\n",
    "    for idx in range(bs):\n",
    "        if y[idx] == 2.:\n",
    "            loss = F.mse_loss(x[idx],x_recon[idx])\n",
    "        else: \n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat[idx], y[idx]) + F.mse_loss(x[idx],x_recon[idx])\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, cfg_fitting, cfg_model, localization=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.metric = Metric\n",
    "        self.cfg_fitting = cfg_fitting\n",
    "        self.cfg_model = cfg_model\n",
    "        self.localization = localization\n",
    "        self.lr = 1e-3\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_recon, logits = self.model(x)\n",
    "        return x_recon, logits\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        x_recon, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "\n",
    "        loss = semi_supervised_loss(x, x_recon, y, y_hat)\n",
    "\n",
    "        y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"loss\":loss, \"train_ttr\": metrics.ttr, \"train_ftr\": metrics.ftr, \"train_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['loss'].mean().item() for x in training_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['train_ttr'] for x in training_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['train_ftr'] for x in training_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['train_acc'] for x in training_step_outputs]).mean()\n",
    "            }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for (k,v) in results.items():\n",
    "        \n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        x_recon, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "        \n",
    "        loss = semi_supervised_loss(x, x_recon, y, y_hat)\n",
    "        print(\"y\", y)\n",
    "        print(\"y_hat\", y_hat)\n",
    "        print(\"x_recon\", x_recon.shape)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"val_loss\": loss, \"val_ttr\": metrics.ttr, \"val_ftr\": metrics.ftr, \"val_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # from pprint import pprint \n",
    "        # pprint(validation_step_outputs)\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].mean().item() for x in validation_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['val_ttr'] for x in validation_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['val_ftr'] for x in validation_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['val_acc'] for x in validation_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        _, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "        # (batch,)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        # (batch_probabilities,)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        # (batch_labels,)\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"test_ttr\": metrics.ttr, \"test_ftr\": metrics.ftr, \"test_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        results = {\n",
    "            \"ttr\": torch.tensor([x['test_ttr'] for x in test_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['test_ftr'] for x in test_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['test_acc'] for x in test_step_outputs]).mean()\n",
    "            }\n",
    "\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"test_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # for normal models CNNs etc. \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "        \n",
    "\n",
    "\n",
    "model = SS_CNNAE()\n",
    "cfg_model = cfg.SSCNNAE()\n",
    "\n",
    "\n",
    "cfg_fitting = cfg.Fitting(max_epoch= 50, es_patience=10)\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "data_out_path  = str(Path(os.environ['DATA_ROOT']) / \"ss_data\")\n",
    "\n",
    "data_path = cfg.DataPath(data_out_path, cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "routine = Routine(model, cfg_fitting, cfg_model)\n",
    "# Init a trainer to execute routine\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    sync_batchnorm=True,\n",
    "    logger=logger,\n",
    "    max_epochs=cfg_fitting.max_epoch,\n",
    "    callbacks=CallbackCollection(cfg_fitting, data_path)(),\n",
    "    gradient_clip_val=1.0,\n",
    "    fast_dev_run=cfg_fitting.fast_dev_run,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "trainer.test(dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3786cd07",
   "metadata": {},
   "source": [
    "### Generative DL: conditional variational autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a8564d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    33064\n",
      "Name: label, dtype: int64\n",
      "Age distribution\n",
      "Gender distribution\n",
      "Training set contains: 18598\n",
      "Validation set contains: 8266\n",
      "Testing set contains: 6200\n",
      "Saving to directory: /media/akinwilson/Samsung_T5/data/audio/keyword-spotting/synthesis_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_synthesis_fitting_set():\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"synthesis_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    FILES = ['train.csv', 'val.csv', 'test.csv']\n",
    "    fitting_set_paths = [str(data_out_path.parent / fname) for fname in FILES]\n",
    "    df = pd.concat([pd.read_csv(file_path) for file_path in fitting_set_paths])\n",
    "    df.reset_index(drop=True, inplace=True )\n",
    "\n",
    "    df = df[df.label == 1.0]\n",
    "    print(df.label.value_counts())\n",
    "\n",
    "\n",
    "    df_fit_data = df[['label', 'wav_path']]\n",
    "\n",
    "    conditonal_generation_cols = ['annotated_age', 'annotated_voice_type']\n",
    "\n",
    "    df = df[conditonal_generation_cols]\n",
    "    print(\"Age distribution\")\n",
    "    df.annotated_age.value_counts() # head()\n",
    "    print(\"Gender distribution\")\n",
    "    df.annotated_voice_type.value_counts() # head()\n",
    "\n",
    "    df_1h = pd.get_dummies(df, columns = conditonal_generation_cols)\n",
    "    df_fitting = pd.concat([df_fit_data , df_1h], axis=1, join='inner')\n",
    "\n",
    "    train_test_set, val_set = train_test_split(df_fitting)\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "\n",
    "create_synthesis_fitting_set()\n",
    "\n",
    "\n",
    "# cols= ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "# df = pd.read_csv(path) # [cols]# .columns\n",
    "\n",
    "\n",
    "# df = df[categorical_cols]\n",
    "# #import pandas as pd\n",
    "# df = pd.get_dummies(df, columns = categorical_cols)\n",
    "\n",
    "# df.head().to_numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b501d",
   "metadata": {},
   "source": [
    "### Will conditional generate samples based on age and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dc3dda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>wav_path</th>\n",
       "      <th>annotated_age_ADULT</th>\n",
       "      <th>annotated_age_KID</th>\n",
       "      <th>annotated_age_UNSURE</th>\n",
       "      <th>annotated_voice_type_FEMALE</th>\n",
       "      <th>annotated_voice_type_MALE</th>\n",
       "      <th>annotated_voice_type_UNKNOWN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/akinwilson/Samsung_T5/data/audio/keywor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/akinwilson/Samsung_T5/data/audio/keywor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/akinwilson/Samsung_T5/data/audio/keywor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/akinwilson/Samsung_T5/data/audio/keywor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/akinwilson/Samsung_T5/data/audio/keywor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                           wav_path  \\\n",
       "11    1.0  /media/akinwilson/Samsung_T5/data/audio/keywor...   \n",
       "14    1.0  /media/akinwilson/Samsung_T5/data/audio/keywor...   \n",
       "18    1.0  /media/akinwilson/Samsung_T5/data/audio/keywor...   \n",
       "20    1.0  /media/akinwilson/Samsung_T5/data/audio/keywor...   \n",
       "21    1.0  /media/akinwilson/Samsung_T5/data/audio/keywor...   \n",
       "\n",
       "    annotated_age_ADULT  annotated_age_KID  annotated_age_UNSURE  \\\n",
       "11                    1                  0                     0   \n",
       "14                    1                  0                     0   \n",
       "18                    1                  0                     0   \n",
       "20                    1                  0                     0   \n",
       "21                    1                  0                     0   \n",
       "\n",
       "    annotated_voice_type_FEMALE  annotated_voice_type_MALE  \\\n",
       "11                            0                          1   \n",
       "14                            1                          0   \n",
       "18                            0                          1   \n",
       "20                            1                          0   \n",
       "21                            1                          0   \n",
       "\n",
       "    annotated_voice_type_UNKNOWN  \n",
       "11                             0  \n",
       "14                             0  \n",
       "18                             0  \n",
       "20                             0  \n",
       "21                             0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fitting = pd.concat([df_fit_data , df_1h], axis=1, join='inner')\n",
    "df_fitting.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7456f8",
   "metadata": {},
   "source": [
    "## Generative variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.routine import Routine\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchaudio as ta \n",
    "import torch\n",
    "import torchaudio \n",
    "import logging\n",
    "\n",
    "class Scaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"int16_max\", torch.tensor([32767]).float())\n",
    "        # self.cfg = cfg \n",
    "\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "\n",
    "        x_scaled = x / self.int16_max\n",
    "        return x_scaled \n",
    "\n",
    "\n",
    "\n",
    "class DataCollator:\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        x = [ x for (x,_) in batch ]\n",
    "        y = [ y for (_,y) in batch ]\n",
    "\n",
    "        x_batched = torch.stack(x).float()\n",
    "        y_batched = torch.stack(y).float()\n",
    "        return {\n",
    "        \"x\": x_batched,\n",
    "        \"y\": y_batched\n",
    "        }\n",
    "\n",
    "\n",
    "class Padder:\n",
    "    def __call__(self, x:torch.tensor) -> torch.tensor:\n",
    "        padding = torch.tensor([0.0]).repeat([1,self.cfg.max_sample_len - x.size()[-1]])\n",
    "        x_new = torch.hstack([x, padding])\n",
    "        x_new = x_new.to(device) \n",
    "        return x_new # (1 ,1 , pad_to_len)\n",
    "\n",
    "class Scaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"int16_max\", torch.tensor([32767]).float())\n",
    "        # self.cfg = cfg \n",
    "\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "\n",
    "        x_scaled = x / self.int16_max\n",
    "        return x_scaled \n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                df_path,\n",
    "                cfg_model,\n",
    "                cfg_feature):\n",
    "        self.df = pd.read_csv(df_path)\n",
    "\n",
    "        self.x_pad = Padder\n",
    "        self.x_scale = Scaler()\n",
    "        kwargs = {\"window_fn\": torch.hann_window,\"wkwargs\":{\"device\": device}}\n",
    "        melkwargs = {**kwargs, **cfg_feature.melspec_kwargs}\n",
    "\n",
    "        self.x_mfcc = torchaudio.transforms.MFCC(melkwargs=melkwargs)\n",
    "        self.x_melspec = torchaudio.transforms.MelSpectrogram(**melkwargs)\n",
    "\n",
    "        self.cfg_model = cfg_model\n",
    "        self.cfg_feature = cfg_feature\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.df.loc[idx]['label']\n",
    "        x_path = self.df.loc[idx]['wav_path']\n",
    "        y =  torch.tensor(int(y), device=device)\n",
    "        x,_ = ta.load(x_path)\n",
    "        ##########################################################\n",
    "        #  CAN MAKE PART OF ARCHITECTURE WITH \n",
    "        #  https://github.com/qiuqiangkong/torchlibrosa\n",
    "        ##########################################################\n",
    "        x = self.x_scale(x)\n",
    "        x = self.x_pad(x)\n",
    "        if self.cfg_model.audio_feature == \"mfcc\":\n",
    "            x = self.x_mfcc(x)\n",
    "            n_mfcc = int( (self.cfg_feature.sample_rate * self.cfg_feature.audio_duration) / self.cfg_feature.window_step )\n",
    "            x = x[:,:,:n_mfcc]\n",
    "            \n",
    "        elif self.cfg_model.audio_feature == \"spectrogram\":\n",
    "             x = self.x_melspec(x).transpose(1,2)\n",
    "        else:\n",
    "            x = x\n",
    "        ##########################################################\n",
    "        return x,y\n",
    "\n",
    "\n",
    "class AudioDataModule():  # pl.LightningDataModule):\n",
    "    def __init__(self,df_path, cfg_model, cfg_fitting, cfg_feature):\n",
    "        super().__init__()\n",
    "\n",
    "        # the DataPath data class makes sure the files below are present on init in the root directory. \n",
    "        self.train_df_path = df_path  + \"/train.csv\"\n",
    "        self.val_df_path =  df_path  + \"/val.csv\"\n",
    "        self.test_df_path =  df_path  + \"/test.csv\"\n",
    "\n",
    "        self.cfg_model = cfg_model\n",
    "\n",
    "        self.cfg_fitting = cfg_fitting\n",
    "        self.cfg_feature = cfg_feature\n",
    "        self.pin_memory =  False # True if torch.cuda.is_available() else False \n",
    "        \n",
    "\n",
    "        # get input shape to network \n",
    "        # dummpy_ds = self.test_dataloader()\n",
    "        \n",
    "        # x = next(iter(dummpy_ds))\n",
    "        # input_shape = tuple(x['x'].shape[1:])\n",
    "        # input_shape = copy.deepcopy(input_shape)\n",
    "        # self.input_shape = input_shape\n",
    "        # del dummpy_ds \n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ds_train = AudioDataset(df_path=self.train_df_path,cfg_model= self.cfg_model,  cfg_feature=self.cfg_feature) # apply_augmentation)\n",
    "        return DataLoader(ds_train,\n",
    "                          batch_size=self.cfg_fitting.train_bs,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= DataCollator())\n",
    "\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        ds_val = AudioDataset(df_path=self.val_df_path,  cfg_model= self.cfg_model, cfg_feature=self.cfg_feature)\n",
    "        return  DataLoader(ds_val,\n",
    "                          batch_size=self.cfg_fitting.val_bs,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= DataCollator())\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        ds_test = AudioDataset(df_path=self.test_df_path,cfg_model= self.cfg_model, cfg_feature=self.cfg_feature)\n",
    "        return  DataLoader(ds_test,\n",
    "                          batch_size=self.cfg_fitting.test_bs,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= DataCollator())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8703ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVCNNAE(nn.Module):\n",
    "    '''\n",
    "    Conditional variational convoultional neural network auto encoder\n",
    "    '''\n",
    "\n",
    "    def __init__(self,input_size=32000, n_input=1, latent_dim=1024, stride=16, n_channel=32 ,labels_length=6):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.input_size_w_labels =  input_size + labels_length\n",
    "        self.latent_dim = latent_dim\n",
    "        self.latent_dim_w_labels = latent_dim + labels_length\n",
    "\n",
    "\n",
    "\n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "\n",
    "        self.e_fc4_mean = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        self.e_fc4_var = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(self.latent_dim_w_labels, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "    \n",
    "\n",
    "\n",
    "    def encode(self, x, labels):\n",
    "        x_inputs =  torch.cat([x, labels], axis=1) \n",
    "        x = self.e_conv1(x_inputs)\n",
    "\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x_mean = self.e_fc4_mean(x)\n",
    "        x_var = self.e_fc4_var(x)\n",
    "        return idx1, idx2, idx3, x_mean, x_var \n",
    "\n",
    "    def reparameterization_trick(self, mean, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps*std\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, z, labels):\n",
    "        x_inputs =  torch.cat([z, labels], axis=1) \n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x_inputs)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        idx1, idx2, idx3, encoded_x_mean, encoded_x_var = self.encode(x, labels)\n",
    "\n",
    "        z  = self.reparameterization_trick(encoded_x_mean, encoded_x_var)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3, z, labels)\n",
    "        return decoded_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237239c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        labels = one_hot(labels, 10)\n",
    "        recon_batch, mu, logvar = model(data, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(test_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            labels = one_hot(labels, 10)\n",
    "            recon_batch, mu, logvar = model(data, labels)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).detach().cpu().numpy()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 5)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(-1, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        with torch.no_grad():\n",
    "            c = torch.eye(10, 10).cuda()\n",
    "            sample = torch.randn(10, 20).to(device)\n",
    "            sample = model.decode(sample, c).cpu()\n",
    "            save_image(sample.view(10, 1, 28, 28),\n",
    "                       'sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f7aac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DenseCVAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, labels_length=6,drop_out_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.do_rate = drop_out_rate\n",
    "        # adding 1 hot encoded observables to input dimension \n",
    "        input_size_with_label = input_size + labels_length\n",
    "\n",
    "        # adding 1 hot encoded observables to input dimension of decoder, i.e. latent dimension \n",
    "        hidden_size += labels_length\n",
    "        \n",
    "        dense_layer_1_output = 512\n",
    "        dense_layer_2_output = 256\n",
    "        dense_layer_3_output = 128\n",
    "        # Encoder layers, notice that we parameterise both the variance and the mean vector of the distribution \n",
    "        self.EncoderDenseLayer1  = nn.Linear(input_size_with_label, dense_layer_1_output)\n",
    "        self.EncoderRelu1 =  nn.ReLU(inplace=True)\n",
    "        self.EncoderDropout1 = nn.Dropout(self.do_rate)\n",
    "        self.EncoderDenseLayer2 = nn.Linear(dense_layer_1_output, dense_layer_2_output) \n",
    "        self.EncoderRelu2 = nn.ReLU(inplace=True)\n",
    "        self.EncoderDropout2 =  nn.Dropout(self.do_rate)\n",
    "        \n",
    "        self.EncoderDenseLayer3_var =  nn.Linear(dense_layer_2_output, hidden_size)\n",
    "        self.EncoderDenseLayer3_mean =  nn.Linear(dense_layer_2_output, hidden_size)\n",
    "\n",
    "        # decoder layers \n",
    "        self.DecoderDenseLayer1 = nn.Linear(hidden_size, dense_layer_2_output) \n",
    "        self.DecoderRelu1 = nn.ReLU(inplace=True)\n",
    "        self.DecoderDropout1 =  nn.Dropout(self.do_rate)\n",
    "        self.DecoderDenseLayer2 = nn.Linear(dense_layer_2_output, dense_layer_1_output) \n",
    "        self.DecoderRelu2 = nn.ReLU(inplace=True)\n",
    "        self.DecoderDropout2 =  nn.Dropout(self.do_rate)\n",
    "        self.DecoderDenseLayer3 = nn.Linear(dense_layer_1_output, input_size)\n",
    "        \n",
    "\n",
    "    \n",
    "    def encode(self, x, one_hot_labels):\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs, -1)\n",
    "        x_input = torch.cat([x,one_hot_labels], axis=1)\n",
    "        x = self.EncoderDenseLayer1(x_input)\n",
    "        x = self.EncoderRelu1(x)\n",
    "        x = self.EncoderDropout1(x)\n",
    "        x = self.EncoderDenseLayer2(x)\n",
    "        x = self.EncoderRelu2(x)\n",
    "        x = self.EncoderDropout2(x)\n",
    "        z_var = self.EncoderDenseLayer3_var(x)\n",
    "        z_mean = self.EncoderDenseLayer3_mean(x)\n",
    "        return z_mean, z_var \n",
    "        \n",
    "        \n",
    "        \n",
    "    def decode(self, z, one_hot_labels):\n",
    "\n",
    "        x_decoder_input = torch.cat([z,one_hot_labels], axis=1)\n",
    "        x = self.DecoderDenseLayer1(x_decoder_input)\n",
    "        x = self.DecoderRelu1(x)\n",
    "        x = self.DecoderDropout1(x)\n",
    "        x = self.DecoderDenseLayer2(x)\n",
    "        x = self.DecoderRelu2(x)\n",
    "        x = self.DecoderDropout2(x)\n",
    "        logits = self.DecoderDenseLayer3(x)\n",
    "    \n",
    "        return logits\n",
    "        \n",
    "    def reparameterization_trick(self, mean, logvar):\n",
    "        std = torch.exp(0.5 *logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mean)\n",
    "\n",
    "\n",
    "    def forward(self,x, labels):\n",
    "        mean, logvar = self.encode(x, labels)\n",
    "        z = self.reparameterization_trick(mean, logvar)\n",
    "        x = self.decode(z, labels)\n",
    "        return x, mean, logvar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=20):\n",
    "#     validation_losses = []\n",
    "#     optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "#     log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "#     with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "#         for i in range(epochs):\n",
    "#             for batch, labels in dataloader:\n",
    "#                 batch = batch.to(DEVICE)\n",
    "#                 labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "#                 if flatten:\n",
    "#                     batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "#                 optim.zero_grad()\n",
    "#                 x,mu,logvar = net(batch, labels)\n",
    "#                 loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "#                 loss.backward()\n",
    "#                 optim.step()\n",
    "#             evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "#             pbar_outer.update(1)\n",
    "#             tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "#     plt.show()\n",
    "#     return validation_losses\n",
    "# cvae = CVAE(28*28).to(DEVICE)\n",
    "# def vae_loss_fn(x, recon_x, mu, logvar):\n",
    "#     BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "#     KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "#     return BCE + KLD\n",
    "\n",
    "# def evaluate(losses, autoencoder, dataloader, flatten=True):\n",
    "#     model = lambda x, y: autoencoder(x, y)[0]    \n",
    "#     loss_sum = []\n",
    "#     inp, out = [],[]\n",
    "#     loss_fn = nn.MSELoss()\n",
    "#     for inputs, labels in dataloader:\n",
    "#         inputs = inputs.to(DEVICE)\n",
    "#         labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "#         if flatten:\n",
    "#             inputs = inputs.view(inputs.size(0), 28*28)\n",
    "\n",
    "#         outputs = model(inputs, labels)\n",
    "#         loss = loss_fn(inputs, outputs)            \n",
    "#         loss_sum.append(loss)\n",
    "#         inp = inputs\n",
    "#         out = outputs\n",
    "\n",
    "#     with torch.set_grad_enabled(False):\n",
    "#         plot_gallery([inp[0].detach().cpu(),out[0].detach().cpu()],28,28,1,2)    \n",
    "\n",
    "#     losses.append((sum(loss_sum)/len(loss_sum)).item())\n",
    "# def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=50):\n",
    "#     validation_losses = []\n",
    "#     optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "#     log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "#     with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "#         for i in range(epochs):\n",
    "#             for batch, labels in dataloader:\n",
    "#                 batch = batch.to(DEVICE)\n",
    "#                 labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "#                 if flatten:\n",
    "#                     batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "#                 optim.zero_grad()\n",
    "#                 x,mu,logvar = net(batch, labels)\n",
    "#                 loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "#                 loss.backward()\n",
    "#                 optim.step()\n",
    "#             evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "#             pbar_outer.update(1)\n",
    "#             tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "#     plt.show()\n",
    "#     return validation_losses\n",
    "\n",
    "\n",
    "\n",
    "# history = train_cvae(cvae, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6056e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end-to-end-l-ED2XZd",
   "language": "python",
   "name": "end-to-end-l-ed2xzd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
