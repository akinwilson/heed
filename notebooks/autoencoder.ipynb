{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook config imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6112835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # set vis gpus \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import pytorch_lightning as pl\n",
    "import wwv.config  as cfg \n",
    "from wwv.util import CallbackCollection \n",
    "from wwv.data import AudioDataModule\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg_fitting = cfg.Fitting(batch_size=64, train_bs=64, val_bs=64)\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_model = cfg.CNNAE()\n",
    "\n",
    "data_path = cfg.DataPath(os.environ['DATA_ROOT'], cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469a162",
   "metadata": {},
   "source": [
    "## Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ffdb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, latent_dim=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        \n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(latent_dim, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "    \n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "        return decoded_x\n",
    "\n",
    "x = torch.randn((1,1,32000), device=device)\n",
    "\n",
    "model = CNNAE()\n",
    "model.to(device=device)\n",
    "x_reconstructed = model(x)\n",
    "assert x_reconstructed.shape == x.shape,  f\"The reconstructed input is of different dimensions to the original input. Original: {x_reconstructed.shape}. Reconstructed: {x.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec930ba",
   "metadata": {},
   "source": [
    "## Fitting autoencoder and visualizing embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Routine for fitting a autoencoder: encoder decoder structure\n",
    "    \"\"\"\n",
    "    def __init__ (self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-4\n",
    "\n",
    "    def encode(self,x):\n",
    "        idx1, idx2, idx3, x_encoded = self.model.encode(x)\n",
    "        return x_encoded,  idx1, idx2, idx3\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        x_reconstructed = self.model.decode( idx1, idx2, idx3, x)\n",
    "        return x_reconstructed \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_encoded,  idx1, idx2, idx3 = self.encode(x)\n",
    "        x_reconstructed  = self.decode( idx1, idx2, idx3, x_encoded)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "\n",
    "    def training_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"loss\": loss }\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "\n",
    "    def validation_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"val_loss\": loss }\n",
    "\n",
    "    def validation_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['val_loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer } # , \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "routine = Routine(model)\n",
    "trainer = Trainer(accelerator=\"gpu\",sync_batchnorm = True, max_epochs = 5 ,num_sanity_val_steps = 2, gradient_clip_val=1.0)\n",
    "# # PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "# # Trainer executes fitting; training and validating proceducres \n",
    "trainer.fit(routine, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# used downstream, need to define object here, check the forward method of the AE_classifier \n",
    "encoder = trainer.model\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TFVisualiser:\n",
    "    '''\n",
    "    Writes latent code generated by model to tensorboard for adding visualisations\n",
    "\n",
    "    NOTE THE BELOW MODEL EXTRACTION EXPECTS THE ROUTINE BE HAVE BEEN FITTED WITH A SINGLE GPU\n",
    "    ----> you'll have to extract the model slightly differently in a distributed environment \n",
    "    '''\n",
    "\n",
    "    def __init__(self, model=trainer.model.model, test_loader=test_loader, sample_size=2000):\n",
    "\n",
    "        self.model = model \n",
    "        self.sample_size = sample_size\n",
    "        # get test data to embedd \n",
    "        xs,ys = [],[]\n",
    "        for batch in test_loader:\n",
    "            xs.append(batch['x'])\n",
    "            ys.append(batch['y'])\n",
    "        X = torch.vstack(xs)\n",
    "        Y = torch.concat(ys)\n",
    "        X_sampled, Y_sampled = self.sample(X,Y) \n",
    "\n",
    "        self.X = X_sampled\n",
    "        self.Y = Y_sampled\n",
    "        self.latent_code_output_dir = 'runs/cnnae'\n",
    "\n",
    "\n",
    "    def sample(self, X, Y):\n",
    "\n",
    "        smaple_permuation_idxs = torch.randperm(X.size(0))\n",
    "        idxs = smaple_permuation_idxs[:self.sample_size]\n",
    "        X_sampled = X[idxs]\n",
    "        Y_sampled = Y[idxs]\n",
    "        return X_sampled, Y_sampled\n",
    "\n",
    "    def save_latent_code(self):\n",
    "        # encode with model \n",
    "        _,_,_, latent_code = self.model.encode(self.X)\n",
    "        label_list = [{1.:\"Wake word\", 0.:\"Not wake word\"}[y] for y in  self.Y.numpy().tolist()]\n",
    "        # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "        writer = SummaryWriter(self.latent_code_output_dir)\n",
    "        # log for visualisation \n",
    "        writer.add_embedding(latent_code, metadata=label_list, tag=\"AudioEmbedding\")\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        from pathlib import Path \n",
    "\n",
    "        print(f\"Saving subsample of {self.sample_size} of test set's latent encodings to location: {Path().cwd() / self.latent_code_output_dir}\")\n",
    "        self.save_latent_code()\n",
    "\n",
    "TFVisualiser()()\n",
    "\n",
    "# Extracting encoder for down stream task.\n",
    "encoder = trainer.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fb668",
   "metadata": {},
   "source": [
    "## Downstream autoencoder application\n",
    "### Vanilla MLP classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b34ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class AE_classifier(nn.Module):\n",
    "    '''\n",
    "    Classifier takes latent code and performs predictions using the latent code\n",
    "\n",
    "    Applications:\n",
    "        Upstream feature extraction for memory-constraint classifier \n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,latent_dim, dropout=0.2, compression_factor=3):\n",
    "        '''\n",
    "        Compression factor detemines the intermitten dimension reduction factor of the dense network.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "        self.do_rate = dropout\n",
    "\n",
    "        dense_layer_1_output = int(latent_dim / compression_factor)\n",
    "        dense_layer_2_output = int(dense_layer_1_output / compression_factor)\n",
    "\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            OrderedDict([\n",
    "            (\"DenseLayer1\", nn.Linear(latent_dim, dense_layer_1_output) ) , \n",
    "            (\"relu1\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout1\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer2\", nn.Linear(dense_layer_1_output, dense_layer_2_output) ) , \n",
    "            (\"relu2\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout2\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer3\", nn.Linear(dense_layer_2_output, 1) ),\n",
    "            ])\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder.to(\"cuda\")\n",
    "        x_encoded, _ , _ , _ = encoder.encode(x)\n",
    "        logits = self.layers(x_encoded)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb8e7f",
   "metadata": {},
   "source": [
    "## Autoencoder was fit using training and validation datasets. \n",
    "### Will evaluate performance of representation  / manifold learnt by freezing autoencoder layers an training a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from pathlib import Path \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def create_temporary_fitting_set():\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"ae_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    fitting_set_path = data_out_path.parent / \"test.csv\"\n",
    "    train_test_set, val_set = train_test_split(pd.read_csv(fitting_set_path))\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "\n",
    "create_temporary_fitting_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcd3b7b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/akinwilson/Samsung_T5/data/audio/keyword-spotting/ae_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m cfg_feature \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mFeature()\n\u001b[1;32m     20\u001b[0m data_out_path  \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(Path(os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mDATA_ROOT\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mae_data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m data_path \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39;49mDataPath(data_out_path, cfg_model\u001b[39m.\u001b[39;49mmodel_name, cfg_model\u001b[39m.\u001b[39;49mmodel_dir)\n\u001b[1;32m     23\u001b[0m data_module \u001b[39m=\u001b[39m AudioDataModule(data_path\u001b[39m.\u001b[39mroot_data_dir, cfg_model\u001b[39m=\u001b[39mcfg_model, cfg_feature\u001b[39m=\u001b[39mcfg_feature, cfg_fitting\u001b[39m=\u001b[39mcfg_fitting)\n\u001b[1;32m     26\u001b[0m logger \u001b[39m=\u001b[39m TensorBoardLogger(save_dir\u001b[39m=\u001b[39mdata_path\u001b[39m.\u001b[39mmodel_dir, version\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlightning_logs\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m<string>:6\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, root_data_dir, model_name, root_model_dir)\u001b[0m\n",
      "File \u001b[0;32m~/Code/end-to-end/app/src/wwv/config.py:210\u001b[0m, in \u001b[0;36mDataPath.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__post_init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    209\u001b[0m     expected_files \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mtrain.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mval.csv\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39;49m(x \u001b[39min\u001b[39;49;00m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot_data_dir) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m expected_files):\n\u001b[1;32m    211\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected: \u001b[39m\u001b[39m{\u001b[39;00mexpected_files\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTo exist in dir: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_data_dir\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOnly found: \u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_data_dir)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m     model_dir \u001b[39m=\u001b[39m (Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_model_dir) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name)\n",
      "File \u001b[0;32m~/Code/end-to-end/app/src/wwv/config.py:210\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__post_init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    209\u001b[0m     expected_files \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mtrain.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mval.csv\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(x \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot_data_dir) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m expected_files):\n\u001b[1;32m    211\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected: \u001b[39m\u001b[39m{\u001b[39;00mexpected_files\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTo exist in dir: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_data_dir\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOnly found: \u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_data_dir)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m     model_dir \u001b[39m=\u001b[39m (Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_model_dir) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/akinwilson/Samsung_T5/data/audio/keyword-spotting/ae_data'"
     ]
    }
   ],
   "source": [
    "from wwv.data import AudioDataModule\n",
    "from wwv.util import CallbackCollection\n",
    "import wwv.config as cfg\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from wwv.eval import Metric\n",
    "from pathlib import Path \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "cfg_model = cfg.AEClassifier()\n",
    "cfg_fitting = cfg.Fitting(max_epoch= 50, es_patience=10)\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "data_out_path  = str(Path(os.environ['DATA_ROOT']) / \"ae_data\")\n",
    "\n",
    "data_path = cfg.DataPath(data_out_path, cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "# get input shape for onnx exporting\n",
    "input_shape = data_module.input_shape\n",
    "# init model\n",
    "model = AE_classifier(latent_dim=1024)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, cfg_fitting, cfg_model, localization=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.metric = Metric\n",
    "        self.cfg_fitting = cfg_fitting\n",
    "        self.cfg_model = cfg_model\n",
    "        self.localization = localization\n",
    "        self.lr = 1e-3\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x) \n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"loss\":loss, \"train_ttr\": metrics.ttr, \"train_ftr\": metrics.ftr, \"train_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['loss'].item() for x in training_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['train_ttr'] for x in training_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['train_ftr'] for x in training_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['train_acc'] for x in training_step_outputs]).mean()\n",
    "            }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for (k,v) in results.items():\n",
    "        \n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"val_loss\": loss, \"val_ttr\": metrics.ttr, \"val_ftr\": metrics.ftr, \"val_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # from pprint import pprint \n",
    "        # pprint(validation_step_outputs)\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].item() for x in validation_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['val_ttr'] for x in validation_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['val_ftr'] for x in validation_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['val_acc'] for x in validation_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        # (batch_probabilities,)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        # (batch_labels,)\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"test_ttr\": metrics.ttr, \"test_ftr\": metrics.ftr, \"test_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        results = {\n",
    "            \"ttr\": torch.tensor([x['test_ttr'] for x in test_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['test_ftr'] for x in test_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['test_acc'] for x in test_step_outputs]).mean()\n",
    "            }\n",
    "\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"test_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "    \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "routine = Routine(model, cfg_fitting, cfg_model)\n",
    "# Init a trainer to execute routine\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    sync_batchnorm=True,\n",
    "    logger=logger,\n",
    "    max_epochs=cfg_fitting.max_epoch,\n",
    "    callbacks=CallbackCollection(cfg_fitting, data_path)(),\n",
    "    gradient_clip_val=1.0,\n",
    "    fast_dev_run=cfg_fitting.fast_dev_run,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "trainer.test(dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157354d",
   "metadata": {},
   "source": [
    "### Research: Semi-supervised loss : combining the reconstruction and binary cross entropy loss\n",
    "\n",
    "- Combine all available data - such that we have the form:\n",
    "\n",
    "    D = (x, y)\n",
    "    \n",
    "    Where the y $\\in$ $\\{0,1,2\\}$:\n",
    "- Let the loss function be a **piece-wise function** on the domain of the target \n",
    "\n",
    "    \n",
    "$$ Loss(x, x_{recon} y,\\hat{y})=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      MSE(x, x_{recon}), \\text{    }  y \\in \\{2 \\} \\\\\n",
    "      MSE(x, x_{recon}) + BinaryCrossEntropy(y,\\hat{y}), \\text{    }  y \\in \\{0,1\\} \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "We consider then the target label of 2 to be the unknown target label. \n",
    "\n",
    "I need to make sure that both the encoder, decoder and classifier head are updated for the  case of $y \\in \\{0,1\\}$; I am not too sure if this is happening at the moment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c4e7794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupting 85368 samples out of 94854\n",
      "New target distribution\n",
      "2.0    85368\n",
      "0.0     6129\n",
      "1.0     3357\n",
      "Name: label, dtype: int64\n",
      "Training set contains: 53355\n",
      "Validation set contains: 23714\n",
      "Testing set contains: 17785\n",
      "Saving to directory: /media/akinwilson/Samsung_T5/data/audio/keyword-spotting/ss_data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path \n",
    "from collections import OrderedDict\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from wwv.data import AudioDataModule\n",
    "from wwv.util import CallbackCollection\n",
    "import wwv.config as cfg\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import bisect\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from wwv.eval import Metric\n",
    "from pathlib import Path \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def semi_supervised_conversion(df, unknown_label_fraction=0.9,  unknown_target_label=2.):\n",
    "\n",
    "    sample_size = df.shape[0]\n",
    "    unknown_label_sample_size = int( sample_size * unknown_label_fraction)  \n",
    "\n",
    "    print(f\"Corrupting {unknown_label_sample_size} samples out of {sample_size}\")\n",
    "    indices = np.random.choice(np.arange(0, sample_size,1), size=unknown_label_sample_size, replace=False)\n",
    "    df.loc[indices,'label'] = 2.\n",
    "    return df \n",
    "\n",
    "def create_temporary_semi_superivsed_fitting_set(unknown_label_fraction=0.9,  unknown_target_label=2.):\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"ss_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    FILES = ['train.csv', 'val.csv', 'test.csv']\n",
    "    fitting_set_paths = [str(data_out_path.parent / fname) for fname in FILES]\n",
    "    df = pd.concat([pd.read_csv(file_path) for file_path in fitting_set_paths])\n",
    "    df.reset_index(drop=True, inplace=True )\n",
    "    df = semi_supervised_conversion(df, unknown_label_fraction,  unknown_target_label)\n",
    "    print(\"New target distribution\")\n",
    "    print(df.label.value_counts())\n",
    "    # return df \n",
    "    train_test_set, val_set = train_test_split(df)\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "\n",
    "create_temporary_semi_superivsed_fitting_set()\n",
    "\n",
    "data_out_path  = Path(os.environ['DATA_ROOT']) / \"ss_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "968eee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | SS_CNNAE | 4.1 M \n",
      "-----------------------------------\n",
      "4.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 M     Total params\n",
      "16.441    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 284\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m# Init a trainer to execute routine\u001b[39;00m\n\u001b[1;32m    273\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m    274\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    275\u001b[0m     devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     fast_dev_run\u001b[39m=\u001b[39mcfg_fitting\u001b[39m.\u001b[39mfast_dev_run,\n\u001b[1;32m    282\u001b[0m )\n\u001b[0;32m--> 284\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    285\u001b[0m     routine, train_dataloaders\u001b[39m=\u001b[39;49mtrain_loader, val_dataloaders\u001b[39m=\u001b[39;49mval_loader\n\u001b[1;32m    286\u001b[0m )\n\u001b[1;32m    287\u001b[0m trainer\u001b[39m.\u001b[39mtest(dataloaders\u001b[39m=\u001b[39mtest_loader)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:582\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 582\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    583\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    584\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:624\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    617\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    619\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    620\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    622\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    623\u001b[0m )\n\u001b[0;32m--> 624\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    626\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1061\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1059\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1061\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1063\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1140\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1153\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1152\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1155\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1225\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1225\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1229\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    154\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    136\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    138\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 234\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1443\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1443\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1445\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    389\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn [24], line 186\u001b[0m, in \u001b[0;36mRoutine.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m    185\u001b[0m     x \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 186\u001b[0m     y \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m    187\u001b[0m     x_recon, logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, y) \n\u001b[1;32m    188\u001b[0m     y_hat \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39msqueeze()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'y'"
     ]
    }
   ],
   "source": [
    "\n",
    "class DenseClassifier(nn.Module):\n",
    "    def __init__(self,latent_dim, dropout=0.2, compression_factor=3):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "        self.do_rate = dropout\n",
    "\n",
    "        dense_layer_1_output = int(latent_dim / compression_factor)\n",
    "        dense_layer_2_output = int(dense_layer_1_output / compression_factor)\n",
    "\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            OrderedDict([\n",
    "            (\"DenseLayer1\", nn.Linear(latent_dim, dense_layer_1_output) ) , \n",
    "            (\"relu1\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout1\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer2\", nn.Linear(dense_layer_1_output, dense_layer_2_output) ) , \n",
    "            (\"relu2\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout2\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer3\", nn.Linear(dense_layer_2_output, 1) ),\n",
    "            ])\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class SS_CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, latent_dim=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(latent_dim, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "\n",
    "        self.classifier = DenseClassifier(latent_dim)\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "    def classify(self, x):\n",
    "        _, _, _, x_encoded = self.encode(x)\n",
    "        logits = self.classifier(x_encoded)\n",
    "        return logits \n",
    "\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Autoencoding forward method'''\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        logits = self.classify(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "\n",
    "        return decoded_x, logits # mse and binary cross entropy inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def semi_supervised_loss(x=None, x_recon=None,y=None, y_hat=None):\n",
    "    bs = x.shape[0]\n",
    "    losses = []\n",
    "    for idx in range(bs):\n",
    "        if y[idx] == 2.:\n",
    "            loss = F.mse_loss(x[idx],x_recon[idx])\n",
    "        else: \n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat[idx], y[idx]) + F.mse_loss(x[idx],x_recon[idx])\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, cfg_fitting, cfg_model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.metric = Metric\n",
    "        self.cfg_fitting = cfg_fitting\n",
    "        self.cfg_model = cfg_model\n",
    "        self.lr = 1e-3\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_recon, logits = self.model(x)\n",
    "        return x_recon, logits\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        x_recon, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "\n",
    "        loss = semi_supervised_loss(x, x_recon, y, y_hat)\n",
    "\n",
    "        y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"loss\":loss, \"train_ttr\": metrics.ttr, \"train_ftr\": metrics.ftr, \"train_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['loss'].mean().item() for x in training_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['train_ttr'] for x in training_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['train_ftr'] for x in training_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['train_acc'] for x in training_step_outputs]).mean()\n",
    "            }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for (k,v) in results.items():\n",
    "        \n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        x_recon, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "        \n",
    "        loss = semi_supervised_loss(x, x_recon, y, y_hat)\n",
    "        print(\"y\", y)\n",
    "        print(\"y_hat\", y_hat)\n",
    "        print(\"x_recon\", x_recon.shape)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"val_loss\": loss, \"val_ttr\": metrics.ttr, \"val_ftr\": metrics.ftr, \"val_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # from pprint import pprint \n",
    "        # pprint(validation_step_outputs)\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].mean().item() for x in validation_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['val_ttr'] for x in validation_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['val_ftr'] for x in validation_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['val_acc'] for x in validation_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        _, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "        # (batch,)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        # (batch_probabilities,)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        # (batch_labels,)\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"test_ttr\": metrics.ttr, \"test_ftr\": metrics.ftr, \"test_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        results = {\n",
    "            \"ttr\": torch.tensor([x['test_ttr'] for x in test_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['test_ftr'] for x in test_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['test_acc'] for x in test_step_outputs]).mean()\n",
    "            }\n",
    "\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"test_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # for normal models CNNs etc. \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "        \n",
    "\n",
    "\n",
    "model = SS_CNNAE()\n",
    "cfg_model = cfg.SSCNNAE()\n",
    "\n",
    "\n",
    "cfg_fitting = cfg.Fitting(max_epoch= 50, es_patience=10)\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "data_out_path  = str(Path(os.environ['DATA_ROOT']) / \"ss_data\")\n",
    "\n",
    "data_path = cfg.DataPath(data_out_path, cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "routine = Routine(model, cfg_fitting, cfg_model)\n",
    "# Init a trainer to execute routine\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    sync_batchnorm=True,\n",
    "    logger=logger,\n",
    "    max_epochs=cfg_fitting.max_epoch,\n",
    "    callbacks=CallbackCollection(cfg_fitting, data_path)(),\n",
    "    gradient_clip_val=1.0,\n",
    "    fast_dev_run=cfg_fitting.fast_dev_run,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "trainer.test(dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3786cd07",
   "metadata": {},
   "source": [
    "### Generative DL: conditional variational autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a8564d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    33064\n",
      "Name: label, dtype: int64\n",
      "Age distribution\n",
      "Gender distribution\n",
      "Training set contains: 18598\n",
      "Validation set contains: 8266\n",
      "Testing set contains: 6200\n",
      "Saving to directory: /media/akinwilson/Samsung_T5/data/audio/keyword-spotting/synthesis_data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>wav_path</th>\n",
       "      <th>annotated_age_ADULT</th>\n",
       "      <th>annotated_age_KID</th>\n",
       "      <th>annotated_age_UNSURE</th>\n",
       "      <th>annotated_voice_type_FEMALE</th>\n",
       "      <th>annotated_voice_type_MALE</th>\n",
       "      <th>annotated_voice_type_UNKNOWN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/akinwilson/Samsung_T5/data/audio/keywor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/akinwilson/Samsung_T5/data/audio/keywor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/akinwilson/Samsung_T5/data/audio/keywor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/akinwilson/Samsung_T5/data/audio/keywor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/akinwilson/Samsung_T5/data/audio/keywor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           wav_path  \\\n",
       "0    1.0  /media/akinwilson/Samsung_T5/data/audio/keywor...   \n",
       "1    1.0  /media/akinwilson/Samsung_T5/data/audio/keywor...   \n",
       "2    1.0  /media/akinwilson/Samsung_T5/data/audio/keywor...   \n",
       "3    1.0  /media/akinwilson/Samsung_T5/data/audio/keywor...   \n",
       "4    1.0  /media/akinwilson/Samsung_T5/data/audio/keywor...   \n",
       "\n",
       "   annotated_age_ADULT  annotated_age_KID  annotated_age_UNSURE  \\\n",
       "0                    1                  0                     0   \n",
       "1                    1                  0                     0   \n",
       "2                    1                  0                     0   \n",
       "3                    0                  0                     1   \n",
       "4                    1                  0                     0   \n",
       "\n",
       "   annotated_voice_type_FEMALE  annotated_voice_type_MALE  \\\n",
       "0                            0                          1   \n",
       "1                            1                          0   \n",
       "2                            0                          1   \n",
       "3                            0                          0   \n",
       "4                            0                          1   \n",
       "\n",
       "   annotated_voice_type_UNKNOWN  \n",
       "0                             0  \n",
       "1                             0  \n",
       "2                             0  \n",
       "3                             1  \n",
       "4                             0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_synthesis_fitting_set():\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"synthesis_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    FILES = ['train.csv', 'val.csv', 'test.csv']\n",
    "    fitting_set_paths = [str(data_out_path.parent / fname) for fname in FILES]\n",
    "    df = pd.concat([pd.read_csv(file_path) for file_path in fitting_set_paths])\n",
    "    df.reset_index(drop=True, inplace=True )\n",
    "\n",
    "    df = df[df.label == 1.0]\n",
    "    print(df.label.value_counts())\n",
    "    df_fit_data = df[['label', 'wav_path']]\n",
    "    conditonal_generation_cols = ['annotated_age', 'annotated_voice_type']\n",
    "    df = df[conditonal_generation_cols]\n",
    "    print(\"Age distribution\")\n",
    "    df.annotated_age.value_counts() # head()\n",
    "    print(\"Gender distribution\")\n",
    "    df.annotated_voice_type.value_counts() # head()\n",
    "\n",
    "    df_1h = pd.get_dummies(df, columns = conditonal_generation_cols)\n",
    "    df_fitting = pd.concat([df_fit_data , df_1h], axis=1, join='inner')\n",
    "\n",
    "    train_test_set, val_set = train_test_split(df_fitting)\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "    return test_set\n",
    "\n",
    "\n",
    "df= create_synthesis_fitting_set()\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()\n",
    "# cols= ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "# df = pd.read_csv(path) # [cols]# .columns\n",
    "# df = df[categorical_cols]\n",
    "# #import pandas as pd\n",
    "# df = pd.get_dummies(df, columns = categorical_cols)\n",
    "# df.head().to_numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b501d",
   "metadata": {},
   "source": [
    "### Will conditional generate samples based on age and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "one_hot_cateogrical_colum_names = [\n",
    "    \"annotated_age_ADULT\",\n",
    "    \"annotated_age_KID\",\n",
    "    \"annotated_age_UNSURE\",\n",
    "    \"annotated_voice_type_FEMALE\",\n",
    "    \"annotated_voice_type_MALE\",\n",
    "    \"annotated_voice_type_UNKNOWN\"\n",
    "    ]\n",
    "\n",
    "df.loc[1][one_hot_cateogrical_colum_names].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7456f8",
   "metadata": {},
   "source": [
    "### Conditional generative variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ac172",
   "metadata": {},
   "source": [
    "#### architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f5234da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVCNNAE(nn.Module):\n",
    "    '''\n",
    "    Conditional variational convoultional neural network auto encoder\n",
    "    '''\n",
    "\n",
    "    def __init__(self,input_size=32000, n_input=1, latent_dim=1024, stride=16, n_channel=32 ,labels_length=6):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.input_size_w_labels =  input_size + labels_length\n",
    "        self.latent_dim = latent_dim\n",
    "        self.latent_dim_w_labels = latent_dim + labels_length\n",
    "\n",
    "\n",
    "\n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "\n",
    "        self.e_fc4_mean = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        self.e_fc4_var = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(self.latent_dim_w_labels, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "    \n",
    "\n",
    "\n",
    "    def encode(self, x, labels):\n",
    "        labels.unsqueeze_(dim=1)\n",
    "        x_inputs =  torch.cat([x, labels], axis=2) \n",
    "\n",
    "    \n",
    "        x = self.e_conv1(x_inputs)\n",
    "\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x_mean = self.e_fc4_mean(x)\n",
    "        x_var = self.e_fc4_var(x)\n",
    "        return idx1, idx2, idx3, x_mean, x_var \n",
    "\n",
    "    def reparameterization_trick(self, mean, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps*std\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, z, labels):\n",
    "        \n",
    "        # print(\"z.shape\", z.shape)\n",
    "        # print(\"labels.shape\", labels.shape)\n",
    "\n",
    "        labels.squeeze_(dim=1)\n",
    "        x_inputs =  torch.cat([z, labels], axis=1) \n",
    "        bs = x_inputs.shape[0]\n",
    "        x = self.d_fc4(x_inputs)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        idx1, idx2, idx3,  z_log_mean, z_log_var = self.encode(x, labels)\n",
    "\n",
    "        z  = self.reparameterization_trick( z_log_mean, z_log_var)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3, z, labels)\n",
    "        return x, decoded_x, z_log_mean, z_log_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ed402",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5f70b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.routine import Routine\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchaudio as ta \n",
    "import torch\n",
    "import wwv.config as cfg \n",
    "from pathlib import Path\n",
    "# import torchaudio \n",
    "# import logging\n",
    "\n",
    "one_hot_cateogrical_colum_names = [\n",
    "    \"annotated_age_ADULT\",\n",
    "    \"annotated_age_KID\",\n",
    "    \"annotated_age_UNSURE\",\n",
    "    \"annotated_voice_type_FEMALE\",\n",
    "    \"annotated_voice_type_MALE\",\n",
    "    \"annotated_voice_type_UNKNOWN\"\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Scaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"int16_max\", torch.tensor([32767]).float())\n",
    "        # self.cfg = cfg \n",
    "\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "\n",
    "        x_scaled = x / self.int16_max\n",
    "        return x_scaled \n",
    "\n",
    "\n",
    "\n",
    "class DataCollator:\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        x = [ x for (x,_) in batch ]\n",
    "        cats = [ cats for (_,cats) in batch ]\n",
    "\n",
    "        x_batched = torch.stack(x).float()\n",
    "        y_batched = torch.stack(cats).float()\n",
    "        return {\n",
    "        \"x\": x_batched,\n",
    "        \"cats\": y_batched\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "class Padder:\n",
    "    def __call__(self, x:torch.tensor) -> torch.tensor:\n",
    "        padding = torch.tensor([0.0]).repeat(1,32000 - x.size()[-1])\n",
    "        x_new = torch.hstack([x, padding])\n",
    "        x_new = x_new.to(device) \n",
    "        return x_new # (1 ,1 , pad_to_len)\n",
    "\n",
    "\n",
    "\n",
    "class Scaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"int16_max\", torch.tensor([32767]).float())\n",
    "        # self.cfg = cfg \n",
    "\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "\n",
    "        x_scaled = x / self.int16_max\n",
    "        return x_scaled \n",
    "\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                df_path,\n",
    "                cfg_model,\n",
    "                cfg_feature):\n",
    "        self.df = pd.read_csv(df_path)\n",
    "\n",
    "        self.x_pad = Padder()\n",
    "        self.x_scale = Scaler()\n",
    "        self.cfg_model = cfg_model\n",
    "        self.cfg_feature = cfg_feature\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cats_1h = self.df.loc[idx][one_hot_cateogrical_colum_names].to_numpy()\n",
    "        cats_1h = cats_1h.astype(\"int64\")\n",
    "        x_path = self.df.loc[idx]['wav_path']\n",
    "        cats_1h_tensor =  torch.tensor(cats_1h, device=device)\n",
    "        x,_ = ta.load(x_path)\n",
    "        x = self.x_scale(x)\n",
    "        x = self.x_pad(x)\n",
    "        return x,cats_1h_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AudioDataModule():\n",
    "    def __init__(self,df_path, cfg_model, cfg_fitting, cfg_feature):\n",
    "        super().__init__()\n",
    "\n",
    "        # the DataPath data class makes sure the files below are present on init in the root directory. \n",
    "        self.train_df_path = df_path  + \"/train.csv\"\n",
    "        self.val_df_path =  df_path  + \"/val.csv\"\n",
    "        self.test_df_path =  df_path  + \"/test.csv\"\n",
    "\n",
    "        self.cfg_model = cfg_model\n",
    "\n",
    "        self.cfg_fitting = cfg_fitting\n",
    "        self.cfg_feature = cfg_feature\n",
    "        self.pin_memory =  False # True if torch.cuda.is_available() else False \n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ds_train = AudioDataset(df_path=self.train_df_path,cfg_model= self.cfg_model,  cfg_feature=self.cfg_feature) # apply_augmentation)\n",
    "        return DataLoader(ds_train,\n",
    "                          batch_size=self.cfg_fitting.train_bs,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= DataCollator())\n",
    "\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        ds_val = AudioDataset(df_path=self.val_df_path,  cfg_model= self.cfg_model, cfg_feature=self.cfg_feature)\n",
    "        return  DataLoader(ds_val,\n",
    "                          batch_size=self.cfg_fitting.val_bs,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= DataCollator())\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        ds_test = AudioDataset(df_path=self.test_df_path,cfg_model= self.cfg_model, cfg_feature=self.cfg_feature)\n",
    "        return  DataLoader(ds_test,\n",
    "                          batch_size=self.cfg_fitting.test_bs,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= DataCollator())\n",
    "\n",
    "\n",
    "\n",
    "model = CVCNNAE()\n",
    "cfg_model = cfg.CVCNNAE()\n",
    "\n",
    "cfg_fitting = cfg.Fitting(max_epoch= 50, es_patience=10)\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "data_out_path  = str(Path(os.environ['DATA_ROOT']) / \"synthesis_data\")\n",
    "data_path = cfg.DataPath(data_out_path, cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "test_loader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe97c1",
   "metadata": {},
   "source": [
    "#### Fitting routine definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80f17943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-3\n",
    "\n",
    "\n",
    "    def forward(self, x, cats):\n",
    "        x, decoded_x, z_log_mean, z_log_var = self.model(x, labels=cats)\n",
    "        return x, decoded_x, z_log_mean, z_log_var\n",
    "\n",
    "\n",
    "\n",
    "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
    "    def loss_function(self, recon_x, x, logmean, logvar):\n",
    "        MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - logmean.pow(2) - logvar.exp())\n",
    "        return MSE + KLD\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        cats = batch['cats']\n",
    "        \n",
    "        \n",
    "        x, decoded_x, z_log_mean, z_log_var = self(x, cats) \n",
    "        ########################################################################################################################\n",
    "        loss =  self.loss_function(x, decoded_x, z_log_mean, z_log_var)\n",
    "        ########################################################################################################################\n",
    "        return {\"loss\":loss}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        results = {\n",
    "            \"loss\": torch.teepsnsor([x['loss'].mean().item() for x in training_step_outputs]).mean(),\n",
    "            }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        cats = batch['cats']\n",
    "        x, decoded_x, z_log_mean, z_log_var  = self(x, cats) \n",
    "        ########################################################################################################################\n",
    "        loss =  self.loss_function(x, decoded_x, z_log_mean, z_log_var)\n",
    "        ########################################################################################################################        \n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # from pprint import pprint \n",
    "        # pprint(validation_step_outputs)\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].mean().item() for x in validation_step_outputs]).mean(),\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # for normal models CNNs etc. \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8703ded",
   "metadata": {},
   "source": [
    "#### Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "237239c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | CVCNNAE | 5.6 M \n",
      "----------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.275    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  15%|█▌        | 128/839 [00:48<04:27,  2.65it/s, loss=43, v_num=1]     "
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from wwv.util import CallbackCollection\n",
    "\n",
    "routine = Routine(model)\n",
    "# Init a trainer to execute routine\n",
    "model = CVCNNAE()\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    sync_batchnorm=True,\n",
    "    logger=logger,\n",
    "    max_epochs=cfg_fitting.max_epoch,\n",
    "    callbacks=CallbackCollection(cfg_fitting, data_path)(),\n",
    "    gradient_clip_val=1.0,\n",
    "    fast_dev_run=cfg_fitting.fast_dev_run,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "\n",
    "trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6056e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end-to-end-l-ED2XZd",
   "language": "python",
   "name": "end-to-end-l-ed2xzd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
