{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6112835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import librosa\n",
    "import warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\" # set vis gpus \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import wwv.config  as cfg \n",
    "from wwv.util import CallbackCollection \n",
    "from wwv.data import AudioDataModule\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg_fitting = cfg.Fitting(batch_size=256, train_bs=256, val_bs=256)\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_model = cfg.CNNAE()\n",
    "\n",
    "data_path = cfg.DataPath(os.environ['DATA_ROOT'], cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469a162",
   "metadata": {},
   "source": [
    "## Discriminative Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ffdb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, latent_dim=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(latent_dim, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "    \n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "\n",
    "\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "\n",
    "        x = self.d_conv3(x)\n",
    "\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        \n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        \n",
    "\n",
    "        x = self.d_pool2(x, idx2)\n",
    "\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        \n",
    "\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "        return decoded_x\n",
    "\n",
    "x = torch.randn((1,1,32000), device=device)\n",
    "\n",
    "model = CNNAE()\n",
    "model.to(device=device)\n",
    "x_reconstructed = model(x)\n",
    "assert x_reconstructed.shape == x.shape,  f\"The reconstructed input is of different dimensions to the original input. Original: {x_reconstructed.shape}. Reconstructed: {x.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b88c55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): LightningParallelModule(\n",
       "    (_forward_module): Routine(\n",
       "      (model): CNNAE(\n",
       "        (e_conv1): Conv1d(1, 32, kernel_size=(80,), stride=(16,))\n",
       "        (e_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (e_pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "        (e_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
       "        (e_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (e_pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "        (e_conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
       "        (e_bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (e_pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "        (e_conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
       "        (e_bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (e_pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (e_fc4): Linear(in_features=1792, out_features=1024, bias=True)\n",
       "        (d_fc4): Linear(in_features=1024, out_features=1792, bias=True)\n",
       "        (d_pool4): MaxUnpool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "        (d_bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (d_conv4): ConvTranspose1d(64, 64, kernel_size=(3,), stride=(1,))\n",
       "        (d_pool3): MaxUnpool1d(kernel_size=(4,), stride=(4,), padding=(0,))\n",
       "        (d_bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (d_conv3): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "        (d_pool2): MaxUnpool1d(kernel_size=(4,), stride=(4,), padding=(0,))\n",
       "        (d_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (d_conv2): ConvTranspose1d(32, 32, kernel_size=(3,), stride=(1,))\n",
       "        (d_pool1): MaxUnpool1d(kernel_size=(4,), stride=(4,), padding=(0,))\n",
       "        (d_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (d_conv1): ConvTranspose1d(32, 1, kernel_size=(80,), stride=(16,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__ (self, model): # , cfg_model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-4\n",
    "        # self.cfg_model = cfg_model \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model.encode(x)\n",
    "\n",
    "\n",
    "    def fit_forward(self,x):\n",
    "        x_reconstructed = self.model.encode(x)\n",
    "        return x_reconstructed \n",
    "\n",
    "\n",
    "    def training_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.fit_forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"loss\": loss }\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "\n",
    "    def validation_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.fit_forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"val_loss\": loss }\n",
    "\n",
    "    def validation_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['val_loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer } # , \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "# routine = Routine(model)\n",
    "# trainer = Trainer(accelerator=\"gpu\",devices=3,strategy='dp',sync_batchnorm = True, max_epochs = 5 ,num_sanity_val_steps = 2, gradient_clip_val=1.0)\n",
    "# # # PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "# # # Trainer executes fitting; training and validating proceducres \n",
    "# trainer.fit(routine, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "m = trainer.model\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "066a3d0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module '__main__' has no attribute 'CNNAE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m Y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat(ys)\n\u001b[1;32m     16\u001b[0m PATH \u001b[39m=\u001b[39m cfg_model\u001b[39m.\u001b[39mmodel_dir \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mcfg_model\u001b[39m.\u001b[39mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m/model.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(PATH)\n\u001b[1;32m     20\u001b[0m _,_,_, latent_code \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mencode(X)\n\u001b[1;32m     21\u001b[0m label_list \u001b[39m=\u001b[39m [{\u001b[39m1.\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mWake word\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0.\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mNot wake word\u001b[39m\u001b[39m\"\u001b[39m}[y] \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m  Y\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   1214\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[1;32m   1215\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:1529\u001b[0m, in \u001b[0;36m_Unpickler.load_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1527\u001b[0m module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1528\u001b[0m name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1529\u001b[0m klass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_class(module, name)\n\u001b[1;32m   1530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend(klass)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/torch/serialization.py:1124\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m mod_name \u001b[39m=\u001b[39m load_module_mapping\u001b[39m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-T3BHxh3q/lib/python3.10/site-packages/pytorch_lightning/_graveyard/legacy_import_unpickler.py:24\u001b[0m, in \u001b[0;36mRedirectingUnpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m module \u001b[39m!=\u001b[39m new_module:\n\u001b[1;32m     23\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRedirecting import of \u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mnew_module\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(new_module, name)\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:1584\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1582\u001b[0m     \u001b[39mreturn\u001b[39;00m _getattribute(sys\u001b[39m.\u001b[39mmodules[module], name)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1583\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1584\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(sys\u001b[39m.\u001b[39;49mmodules[module], name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module '__main__' has no attribute 'CNNAE'"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# get test data to embedd \n",
    "xs,ys = [],[]\n",
    "\n",
    "for batch in test_loader:\n",
    "    x = batch['x']\n",
    "    y = batch['y']\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "\n",
    "X = torch.vstack(xs)\n",
    "Y = torch.concat(ys)\n",
    "\n",
    "\n",
    "# encode with model \n",
    "_,_,_, latent_code = m.encode(X)\n",
    "label_list = [{1.:\"Wake word\", 0.:\"Not wake word\"}[y] for y in  Y.numpy().tolist()]\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/cnnae')\n",
    "# log for visualisation \n",
    "writer.add_embedding(latent_code,\n",
    "                    metadata=label_list,\n",
    "                    tag=\"AudioEmbedding\")\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34ba0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8564d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/akinwilson/Code/HTS/val.csv\"\n",
    "import pandas as pd \n",
    "cols= ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "df = pd.read_csv(path) # [cols]# .columns\n",
    "categorical_cols = ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "\n",
    "df = df[categorical_cols]\n",
    "#import pandas as pd\n",
    "df = pd.get_dummies(df, columns = categorical_cols)\n",
    "\n",
    "df.head().to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 1\n",
    "n_channel = 32\n",
    "stride= 2\n",
    "e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=160, stride=stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7456f8",
   "metadata": {},
   "source": [
    "## Generative variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, c_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim + c_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim + c_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "    \n",
    "    def encoder(self, x, c):\n",
    "        concat_input = torch.cat([x, c], 1)\n",
    "        h = F.relu(self.fc1(concat_input))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add(mu) # return z sample\n",
    "    \n",
    "    def decoder(self, z, c):\n",
    "        concat_input = torch.cat([z, c], 1)\n",
    "        h = F.relu(self.fc4(concat_input))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h))\n",
    "    \n",
    "    def forward(self, x, c):\n",
    "\n",
    "        \n",
    "        mu, log_var = self.encoder(x.view(-1, 784), c)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z, c), mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.routine import Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7aac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=20):\n",
    "        super(CVAE, self).__init__()\n",
    "        input_size_with_label = input_size + labels_length\n",
    "        hidden_size += labels_length\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size_with_label,512)\n",
    "        self.fc21 = nn.Linear(512, hidden_size)\n",
    "        self.fc22 = nn.Linear(512, hidden_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size, 512)\n",
    "        self.fc4 = nn.Linear(512, input_size)\n",
    "    \n",
    "    def encode(self, x, labels):\n",
    "        x = x.view(-1, 1*28*28)\n",
    "        x = torch.cat((x, labels), 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc21(x), self.fc22(x)\n",
    "        \n",
    "    def decode(self, z, labels):\n",
    "        torch.cat((z, labels), 1)\n",
    "        z = self.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(z))\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 *logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "        \n",
    "    def forward(self,x, labels):\n",
    "        #targets = one_hot(targets,labels_length-1).float().to(DEVICE)\n",
    "        mu, logvar = self.encode(x, labels)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x = self.decode(z, labels)\n",
    "        return x, mu, logvar\n",
    "\n",
    "def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=20):\n",
    "    validation_losses = []\n",
    "    optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "        for i in range(epochs):\n",
    "            for batch, labels in dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "                if flatten:\n",
    "                    batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                x,mu,logvar = net(batch, labels)\n",
    "                loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "    plt.show()\n",
    "    return validation_losses\n",
    "cvae = CVAE(28*28).to(DEVICE)\n",
    "def vae_loss_fn(x, recon_x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def evaluate(losses, autoencoder, dataloader, flatten=True):\n",
    "    model = lambda x, y: autoencoder(x, y)[0]    \n",
    "    loss_sum = []\n",
    "    inp, out = [],[]\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "        if flatten:\n",
    "            inputs = inputs.view(inputs.size(0), 28*28)\n",
    "\n",
    "        outputs = model(inputs, labels)\n",
    "        loss = loss_fn(inputs, outputs)            \n",
    "        loss_sum.append(loss)\n",
    "        inp = inputs\n",
    "        out = outputs\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        plot_gallery([inp[0].detach().cpu(),out[0].detach().cpu()],28,28,1,2)    \n",
    "\n",
    "    losses.append((sum(loss_sum)/len(loss_sum)).item())\n",
    "def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=50):\n",
    "    validation_losses = []\n",
    "    optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "        for i in range(epochs):\n",
    "            for batch, labels in dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "                if flatten:\n",
    "                    batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                x,mu,logvar = net(batch, labels)\n",
    "                loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "    plt.show()\n",
    "    return validation_losses\n",
    "\n",
    "\n",
    "\n",
    "history = train_cvae(cvae, train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
