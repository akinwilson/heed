{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook config imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6112835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # set vis gpus \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import pytorch_lightning as pl\n",
    "import wwv.config  as cfg \n",
    "from wwv.util import CallbackCollection \n",
    "from wwv.data import AudioDataModule\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg_fitting = cfg.Fitting(batch_size=64, train_bs=64, val_bs=64)\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_model = cfg.CNNAE()\n",
    "\n",
    "data_path = cfg.DataPath(os.environ['DATA_ROOT'], cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469a162",
   "metadata": {},
   "source": [
    "## Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffdb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, latent_dim=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        \n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(latent_dim, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "    \n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "        return decoded_x\n",
    "\n",
    "x = torch.randn((1,1,32000), device=device)\n",
    "\n",
    "model = CNNAE()\n",
    "model.to(device=device)\n",
    "x_reconstructed = model(x)\n",
    "assert x_reconstructed.shape == x.shape,  f\"The reconstructed input is of different dimensions to the original input. Original: {x_reconstructed.shape}. Reconstructed: {x.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec930ba",
   "metadata": {},
   "source": [
    "## Fitting autoencoder and visualizing embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Routine for fitting a autoencoder: encoder decoder structure\n",
    "    \"\"\"\n",
    "    def __init__ (self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-4\n",
    "\n",
    "    def encode(self,x):\n",
    "        idx1, idx2, idx3, x_encoded = self.model.encode(x)\n",
    "        return x_encoded,  idx1, idx2, idx3\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        x_reconstructed = self.model.decode( idx1, idx2, idx3, x)\n",
    "        return x_reconstructed \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_encoded,  idx1, idx2, idx3 = self.encode(x)\n",
    "        x_reconstructed  = self.decode( idx1, idx2, idx3, x_encoded)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "\n",
    "    def training_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"loss\": loss }\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "\n",
    "    def validation_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"val_loss\": loss }\n",
    "\n",
    "    def validation_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['val_loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer } # , \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "routine = Routine(model)\n",
    "trainer = Trainer(accelerator=\"gpu\",sync_batchnorm = True, max_epochs = 5 ,num_sanity_val_steps = 2, gradient_clip_val=1.0)\n",
    "# # PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "# # Trainer executes fitting; training and validating proceducres \n",
    "trainer.fit(routine, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# used downstream, need to define object here, check the forward method of the AE_classifier \n",
    "encoder = trainer.model\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TFVisualiser:\n",
    "    '''\n",
    "    Writes latent code generated by model to tensorboard for adding visualisations\n",
    "\n",
    "    NOTE THE BELOW MODEL EXTRACTION EXPECTS THE ROUTINE BE HAVE BEEN FITTED WITH A SINGLE GPU\n",
    "    ----> you'll have to extract the model slightly differently in a distributed environment \n",
    "    '''\n",
    "\n",
    "    def __init__(self, model=trainer.model.model, test_loader=test_loader, sample_size=2000):\n",
    "\n",
    "        self.model = model \n",
    "        self.sample_size = sample_size\n",
    "        # get test data to embedd \n",
    "        xs,ys = [],[]\n",
    "        for batch in test_loader:\n",
    "            xs.append(batch['x'])\n",
    "            ys.append(batch['y'])\n",
    "        X = torch.vstack(xs)\n",
    "        Y = torch.concat(ys)\n",
    "        X_sampled, Y_sampled = self.sample(X,Y) \n",
    "\n",
    "        self.X = X_sampled\n",
    "        self.Y = Y_sampled\n",
    "        self.latent_code_output_dir = 'runs/cnnae'\n",
    "\n",
    "\n",
    "    def sample(self, X, Y):\n",
    "\n",
    "        smaple_permuation_idxs = torch.randperm(X.size(0))\n",
    "        idxs = smaple_permuation_idxs[:self.sample_size]\n",
    "        X_sampled = X[idxs]\n",
    "        Y_sampled = Y[idxs]\n",
    "        return X_sampled, Y_sampled\n",
    "\n",
    "    def save_latent_code(self):\n",
    "        # encode with model \n",
    "        _,_,_, latent_code = self.model.encode(self.X)\n",
    "        label_list = [{1.:\"Wake word\", 0.:\"Not wake word\"}[y] for y in  self.Y.numpy().tolist()]\n",
    "        # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "        writer = SummaryWriter(self.latent_code_output_dir)\n",
    "        # log for visualisation \n",
    "        writer.add_embedding(latent_code, metadata=label_list, tag=\"AudioEmbedding\")\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        from pathlib import Path \n",
    "\n",
    "        print(f\"Saving subsample of {self.sample_size} of test set's latent encodings to location: {Path().cwd() / self.latent_code_output_dir}\")\n",
    "        self.save_latent_code()\n",
    "\n",
    "TFVisualiser()()\n",
    "\n",
    "# Extracting encoder for down stream task.\n",
    "encoder = trainer.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fb668",
   "metadata": {},
   "source": [
    "## Downstream autoencoder application\n",
    "### Vanilla MLP classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class AE_classifier(nn.Module):\n",
    "    '''\n",
    "    Classifier takes latent code and performs predictions using the latent code\n",
    "\n",
    "    Applications:\n",
    "        Upstream feature extraction for memory-constraint classifier \n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,latent_dim, dropout=0.2, compression_factor=3):\n",
    "        '''\n",
    "        Compression factor detemines the intermitten dimension reduction factor of the dense network.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "        self.do_rate = dropout\n",
    "\n",
    "        dense_layer_1_output = int(latent_dim / compression_factor)\n",
    "        dense_layer_2_output = int(dense_layer_1_output / compression_factor)\n",
    "\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            OrderedDict([\n",
    "            (\"DenseLayer1\", nn.Linear(latent_dim, dense_layer_1_output) ) , \n",
    "            (\"relu1\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout1\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer2\", nn.Linear(dense_layer_1_output, dense_layer_2_output) ) , \n",
    "            (\"relu2\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout2\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer3\", nn.Linear(dense_layer_2_output, 1) ),\n",
    "            ])\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder.to(\"cuda\")\n",
    "        x_encoded, _ , _ , _ = encoder.encode(x)\n",
    "        logits = self.layers(x_encoded)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb8e7f",
   "metadata": {},
   "source": [
    "## Autoencoder was fit using training and validation datasets. \n",
    "### Will evaluate performance of representation  / manifold learnt by freezing autoencoder layers an training a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from pathlib import Path \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def create_temporary_fitting_set():\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"ae_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    fitting_set_path = data_out_path.parent / \"test.csv\"\n",
    "    train_test_set, val_set = train_test_split(pd.read_csv(fitting_set_path))\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "\n",
    "create_temporary_fitting_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.data import AudioDataModule\n",
    "from wwv.util import CallbackCollection\n",
    "import wwv.config as cfg\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from wwv.eval import Metric\n",
    "from pathlib import Path \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "cfg_model = cfg.AEClassifier()\n",
    "cfg_fitting = cfg.Fitting(max_epoch= 50, es_patience=10)\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "data_out_path  = str(Path(os.environ['DATA_ROOT']) / \"ae_data\")\n",
    "\n",
    "data_path = cfg.DataPath(data_out_path, cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "# get input shape for onnx exporting\n",
    "input_shape = data_module.input_shape\n",
    "# init model\n",
    "model = AE_classifier(latent_dim=1024)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, cfg_fitting, cfg_model, localization=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.metric = Metric\n",
    "        self.cfg_fitting = cfg_fitting\n",
    "        self.cfg_model = cfg_model\n",
    "        self.localization = localization\n",
    "        self.lr = 1e-3\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x) \n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"loss\":loss, \"train_ttr\": metrics.ttr, \"train_ftr\": metrics.ftr, \"train_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['loss'].item() for x in training_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['train_ttr'] for x in training_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['train_ftr'] for x in training_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['train_acc'] for x in training_step_outputs]).mean()\n",
    "            }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for (k,v) in results.items():\n",
    "        \n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"val_loss\": loss, \"val_ttr\": metrics.ttr, \"val_ftr\": metrics.ftr, \"val_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # from pprint import pprint \n",
    "        # pprint(validation_step_outputs)\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].item() for x in validation_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['val_ttr'] for x in validation_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['val_ftr'] for x in validation_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['val_acc'] for x in validation_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        # (batch_probabilities,)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        # (batch_labels,)\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"test_ttr\": metrics.ttr, \"test_ftr\": metrics.ftr, \"test_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        results = {\n",
    "            \"ttr\": torch.tensor([x['test_ttr'] for x in test_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['test_ftr'] for x in test_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['test_acc'] for x in test_step_outputs]).mean()\n",
    "            }\n",
    "\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"test_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "    \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "routine = Routine(model, cfg_fitting, cfg_model)\n",
    "# Init a trainer to execute routine\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    sync_batchnorm=True,\n",
    "    logger=logger,\n",
    "    max_epochs=cfg_fitting.max_epoch,\n",
    "    callbacks=CallbackCollection(cfg_fitting, data_path)(),\n",
    "    gradient_clip_val=1.0,\n",
    "    fast_dev_run=cfg_fitting.fast_dev_run,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "trainer.test(dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157354d",
   "metadata": {},
   "source": [
    "### Research: Semi-supervised loss : combining the reconstruction and binary cross entropy loss\n",
    "\n",
    "- Combine all available data - such that we have the form:\n",
    "\n",
    "    D = (x, y)\n",
    "    \n",
    "    Where the y $\\in$ $\\{0,1,2\\}$:\n",
    "- Let the loss function be a **piece-wise function** on the domain of the target \n",
    "\n",
    "    \n",
    "$$ Loss(x, x_{recon} y,\\hat{y})=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      MSE(x, x_{recon}), \\text{    }  y \\in \\{2 \\} \\\\\n",
    "      MSE(x, x_{recon}) + BinaryCrossEntropy(y,\\hat{y}), \\text{    }  y \\in \\{0,1\\} \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "We consider then the target label of 2 to be the unknown target label. \n",
    "\n",
    "I need to make sure that both the encoder, decoder and classifier head are updated for the  case of $y \\in \\{0,1\\}$; I am not too sure if this is happening at the moment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path \n",
    "from collections import OrderedDict\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from wwv.data import AudioDataModule\n",
    "from wwv.util import CallbackCollection\n",
    "import wwv.config as cfg\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import bisect\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from wwv.eval import Metric\n",
    "from pathlib import Path \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def semi_supervised_conversion(df, unknown_label_fraction=0.9,  unknown_target_label=2.):\n",
    "\n",
    "    sample_size = df.shape[0]\n",
    "    unknown_label_sample_size = int( sample_size * unknown_label_fraction)  \n",
    "\n",
    "    print(f\"Corrupting {unknown_label_sample_size} samples out of {sample_size}\")\n",
    "    indices = np.random.choice(np.arange(0, sample_size,1), size=unknown_label_sample_size, replace=False)\n",
    "    df.loc[indices,'label'] = 2.\n",
    "    return df \n",
    "\n",
    "def create_temporary_semi_superivsed_fitting_set(unknown_label_fraction=0.9,  unknown_target_label=2.):\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"ss_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    FILES = ['train.csv', 'val.csv', 'test.csv']\n",
    "    fitting_set_paths = [str(data_out_path.parent / fname) for fname in FILES]\n",
    "    df = pd.concat([pd.read_csv(file_path) for file_path in fitting_set_paths])\n",
    "    df.reset_index(drop=True, inplace=True )\n",
    "    df = semi_supervised_conversion(df, unknown_label_fraction,  unknown_target_label)\n",
    "    print(\"New target distribution\")\n",
    "    print(df.label.value_counts())\n",
    "    # return df \n",
    "    train_test_set, val_set = train_test_split(df)\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "\n",
    "create_temporary_semi_superivsed_fitting_set()\n",
    "\n",
    "data_out_path  = Path(os.environ['DATA_ROOT']) / \"ss_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968eee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenseClassifier(nn.Module):\n",
    "    def __init__(self,latent_dim, dropout=0.2, compression_factor=3):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "        self.do_rate = dropout\n",
    "\n",
    "        dense_layer_1_output = int(latent_dim / compression_factor)\n",
    "        dense_layer_2_output = int(dense_layer_1_output / compression_factor)\n",
    "\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            OrderedDict([\n",
    "            (\"DenseLayer1\", nn.Linear(latent_dim, dense_layer_1_output) ) , \n",
    "            (\"relu1\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout1\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer2\", nn.Linear(dense_layer_1_output, dense_layer_2_output) ) , \n",
    "            (\"relu2\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout2\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer3\", nn.Linear(dense_layer_2_output, 1) ),\n",
    "            ])\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class SS_CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, latent_dim=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(latent_dim, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "\n",
    "        self.classifier = DenseClassifier(latent_dim)\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "    def classify(self, x):\n",
    "        _, _, _, x_encoded = self.encode(x)\n",
    "        logits = self.classifier(x_encoded)\n",
    "        return logits \n",
    "\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Autoencoding forward method'''\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        logits = self.classify(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "\n",
    "        return decoded_x, logits # mse and binary cross entropy inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def semi_supervised_loss(x=None, x_recon=None,y=None, y_hat=None):\n",
    "    bs = x.shape[0]\n",
    "    losses = []\n",
    "    for idx in range(bs):\n",
    "        if y[idx] == 2.:\n",
    "            loss = F.mse_loss(x[idx],x_recon[idx])\n",
    "        else: \n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat[idx], y[idx]) + F.mse_loss(x[idx],x_recon[idx])\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, cfg_fitting, cfg_model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.metric = Metric\n",
    "        self.cfg_fitting = cfg_fitting\n",
    "        self.cfg_model = cfg_model\n",
    "        self.lr = 1e-3\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_recon, logits = self.model(x)\n",
    "        return x_recon, logits\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        x_recon, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "\n",
    "        loss = semi_supervised_loss(x, x_recon, y, y_hat)\n",
    "\n",
    "        y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"loss\":loss, \"train_ttr\": metrics.ttr, \"train_ftr\": metrics.ftr, \"train_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['loss'].mean().item() for x in training_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['train_ttr'] for x in training_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['train_ftr'] for x in training_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['train_acc'] for x in training_step_outputs]).mean()\n",
    "            }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for (k,v) in results.items():\n",
    "        \n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        x_recon, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "        \n",
    "        loss = semi_supervised_loss(x, x_recon, y, y_hat)\n",
    "        print(\"y\", y)\n",
    "        print(\"y_hat\", y_hat)\n",
    "        print(\"x_recon\", x_recon.shape)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"val_loss\": loss, \"val_ttr\": metrics.ttr, \"val_ftr\": metrics.ftr, \"val_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # from pprint import pprint \n",
    "        # pprint(validation_step_outputs)\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].mean().item() for x in validation_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['val_ttr'] for x in validation_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['val_ftr'] for x in validation_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['val_acc'] for x in validation_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        _, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "        # (batch,)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        # (batch_probabilities,)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        # (batch_labels,)\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"test_ttr\": metrics.ttr, \"test_ftr\": metrics.ftr, \"test_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        results = {\n",
    "            \"ttr\": torch.tensor([x['test_ttr'] for x in test_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['test_ftr'] for x in test_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['test_acc'] for x in test_step_outputs]).mean()\n",
    "            }\n",
    "\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"test_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # for normal models CNNs etc. \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "        \n",
    "\n",
    "\n",
    "model = SS_CNNAE()\n",
    "cfg_model = cfg.SSCNNAE()\n",
    "\n",
    "\n",
    "cfg_fitting = cfg.Fitting(max_epoch= 50, es_patience=10)\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "data_out_path  = str(Path(os.environ['DATA_ROOT']) / \"ss_data\")\n",
    "\n",
    "data_path = cfg.DataPath(data_out_path, cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "routine = Routine(model, cfg_fitting, cfg_model)\n",
    "# Init a trainer to execute routine\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    sync_batchnorm=True,\n",
    "    logger=logger,\n",
    "    max_epochs=cfg_fitting.max_epoch,\n",
    "    callbacks=CallbackCollection(cfg_fitting, data_path)(),\n",
    "    gradient_clip_val=1.0,\n",
    "    fast_dev_run=cfg_fitting.fast_dev_run,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "trainer.test(dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3786cd07",
   "metadata": {},
   "source": [
    "### Generative DL: conditional variational autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8564d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_synthesis_fitting_set():\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"synthesis_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    FILES = ['train.csv', 'val.csv', 'test.csv']\n",
    "    fitting_set_paths = [str(data_out_path.parent / fname) for fname in FILES]\n",
    "    df = pd.concat([pd.read_csv(file_path) for file_path in fitting_set_paths])\n",
    "    df.reset_index(drop=True, inplace=True )\n",
    "\n",
    "    df = df[df.label == 1.0]\n",
    "    print(df.label.value_counts())\n",
    "    df_fit_data = df[['label', 'wav_path']]\n",
    "    conditonal_generation_cols = ['annotated_age', 'annotated_voice_type']\n",
    "    df = df[conditonal_generation_cols]\n",
    "    print(\"Age distribution\")\n",
    "    df.annotated_age.value_counts() # head()\n",
    "    print(\"Gender distribution\")\n",
    "    df.annotated_voice_type.value_counts() # head()\n",
    "\n",
    "    df_1h = pd.get_dummies(df, columns = conditonal_generation_cols)\n",
    "    df_fitting = pd.concat([df_fit_data , df_1h], axis=1, join='inner')\n",
    "\n",
    "    train_test_set, val_set = train_test_split(df_fitting)\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "    return test_set\n",
    "\n",
    "\n",
    "df= create_synthesis_fitting_set()\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()\n",
    "# cols= ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "# df = pd.read_csv(path) # [cols]# .columns\n",
    "# df = df[categorical_cols]\n",
    "# #import pandas as pd\n",
    "# df = pd.get_dummies(df, columns = categorical_cols)\n",
    "# df.head().to_numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b501d",
   "metadata": {},
   "source": [
    "### Will conditional generate samples based on age and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "one_hot_cateogrical_colum_names = [\n",
    "    \"annotated_age_ADULT\",\n",
    "    \"annotated_age_KID\",\n",
    "    \"annotated_age_UNSURE\",\n",
    "    \"annotated_voice_type_FEMALE\",\n",
    "    \"annotated_voice_type_MALE\",\n",
    "    \"annotated_voice_type_UNKNOWN\"\n",
    "    ]\n",
    "\n",
    "df.loc[1][one_hot_cateogrical_colum_names].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7456f8",
   "metadata": {},
   "source": [
    "### Conditional generative variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ac172",
   "metadata": {},
   "source": [
    "#### architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5234da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVCNNAE(nn.Module):\n",
    "    '''\n",
    "    Conditional variational convoultional neural network auto encoder\n",
    "    '''\n",
    "\n",
    "    def __init__(self,input_size=32000, n_input=1, latent_dim=1024, stride=16, n_channel=32 ,labels_length=6):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.input_size_w_labels =  input_size + labels_length\n",
    "        self.latent_dim = latent_dim\n",
    "        self.latent_dim_w_labels = latent_dim + labels_length\n",
    "\n",
    "\n",
    "\n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "\n",
    "        self.e_fc4_mean = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        self.e_fc4_var = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(self.latent_dim_w_labels, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "    \n",
    "\n",
    "\n",
    "    def encode(self, x, labels):\n",
    "        x_inputs =  torch.cat([x, labels], axis=1) \n",
    "        x = self.e_conv1(x_inputs)\n",
    "\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x_mean = self.e_fc4_mean(x)\n",
    "        x_var = self.e_fc4_var(x)\n",
    "        return idx1, idx2, idx3, x_mean, x_var \n",
    "\n",
    "    def reparameterization_trick(self, mean, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps*std\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, z, labels):\n",
    "        x_inputs =  torch.cat([z, labels], axis=1) \n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x_inputs)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        idx1, idx2, idx3,  z_log_mean, z_log_var = self.encode(x, labels)\n",
    "\n",
    "        z  = self.reparameterization_trick( z_log_mean, z_log_var)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3, z, labels)\n",
    "        return x, decoded_x, z_log_mean, z_log_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ed402",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f70b176",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wwv.config' has no attribute 'CVCNNAE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 155\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[39mreturn\u001b[39;00m  DataLoader(ds_test,\n\u001b[1;32m    143\u001b[0m                           batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg_fitting\u001b[39m.\u001b[39mtest_bs,\n\u001b[1;32m    144\u001b[0m                           shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    145\u001b[0m                           drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    146\u001b[0m                           pin_memory\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpin_memory,\n\u001b[1;32m    147\u001b[0m                           collate_fn\u001b[39m=\u001b[39m DataCollator())\n\u001b[1;32m    154\u001b[0m model \u001b[39m=\u001b[39m CVCNNAE()\n\u001b[0;32m--> 155\u001b[0m cfg_model \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39;49mCVCNNAE()\n\u001b[1;32m    157\u001b[0m cfg_fitting \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mFitting(max_epoch\u001b[39m=\u001b[39m \u001b[39m50\u001b[39m, es_patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m    158\u001b[0m cfg_signal \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mSignal()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wwv.config' has no attribute 'CVCNNAE'"
     ]
    }
   ],
   "source": [
    "from wwv.routine import Routine\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchaudio as ta \n",
    "import torch\n",
    "# import torchaudio \n",
    "# import logging\n",
    "\n",
    "one_hot_cateogrical_colum_names = [\n",
    "    \"annotated_age_ADULT\",\n",
    "    \"annotated_age_KID\",\n",
    "    \"annotated_age_UNSURE\",\n",
    "    \"annotated_voice_type_FEMALE\",\n",
    "    \"annotated_voice_type_MALE\",\n",
    "    \"annotated_voice_type_UNKNOWN\"\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Scaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"int16_max\", torch.tensor([32767]).float())\n",
    "        # self.cfg = cfg \n",
    "\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "\n",
    "        x_scaled = x / self.int16_max\n",
    "        return x_scaled \n",
    "\n",
    "\n",
    "\n",
    "class DataCollator:\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        x = [ x for (x,_) in batch ]\n",
    "        cats = [ cats for (_,cats) in batch ]\n",
    "\n",
    "        x_batched = torch.stack(x).float()\n",
    "        y_batched = torch.stack(cats).float()\n",
    "        return {\n",
    "        \"x\": x_batched,\n",
    "        \"cats\": y_batched\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "class Padder:\n",
    "    def __call__(self, x:torch.tensor) -> torch.tensor:\n",
    "        padding = torch.tensor([0.0]).repeat(1,32000 - x.size()[-1])\n",
    "        x_new = torch.hstack([x, padding])\n",
    "        x_new = x_new.to(device) \n",
    "        return x_new # (1 ,1 , pad_to_len)\n",
    "\n",
    "\n",
    "\n",
    "class Scaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"int16_max\", torch.tensor([32767]).float())\n",
    "        # self.cfg = cfg \n",
    "\n",
    "\n",
    "    def forward(self, x:torch.tensor):\n",
    "\n",
    "        x_scaled = x / self.int16_max\n",
    "        return x_scaled \n",
    "\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                df_path,\n",
    "                cfg_model,\n",
    "                cfg_feature):\n",
    "        self.df = pd.read_csv(df_path)\n",
    "\n",
    "        self.x_pad = Padder()\n",
    "        self.x_scale = Scaler()\n",
    "        self.cfg_model = cfg_model\n",
    "        self.cfg_feature = cfg_feature\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cats_1h = self.df.loc[idx][one_hot_cateogrical_colum_names].to_numpy()\n",
    "        cats_1h = cats_1h.astype(\"int64\")\n",
    "        x_path = self.df.loc[idx]['wav_path']\n",
    "        cats_1h_tensor =  torch.tensor(cats_1h, device=device)\n",
    "        x,_ = ta.load(x_path)\n",
    "        x = self.x_scale(x)\n",
    "        x = self.x_pad(x)\n",
    "        return x,cats_1h_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AudioDataModule():\n",
    "    def __init__(self,df_path, cfg_model, cfg_fitting, cfg_feature):\n",
    "        super().__init__()\n",
    "\n",
    "        # the DataPath data class makes sure the files below are present on init in the root directory. \n",
    "        self.train_df_path = df_path  + \"/train.csv\"\n",
    "        self.val_df_path =  df_path  + \"/val.csv\"\n",
    "        self.test_df_path =  df_path  + \"/test.csv\"\n",
    "\n",
    "        self.cfg_model = cfg_model\n",
    "\n",
    "        self.cfg_fitting = cfg_fitting\n",
    "        self.cfg_feature = cfg_feature\n",
    "        self.pin_memory =  False # True if torch.cuda.is_available() else False \n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ds_train = AudioDataset(df_path=self.train_df_path,cfg_model= self.cfg_model,  cfg_feature=self.cfg_feature) # apply_augmentation)\n",
    "        return DataLoader(ds_train,\n",
    "                          batch_size=self.cfg_fitting.train_bs,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= DataCollator())\n",
    "\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        ds_val = AudioDataset(df_path=self.val_df_path,  cfg_model= self.cfg_model, cfg_feature=self.cfg_feature)\n",
    "        return  DataLoader(ds_val,\n",
    "                          batch_size=self.cfg_fitting.val_bs,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= DataCollator())\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        ds_test = AudioDataset(df_path=self.test_df_path,cfg_model= self.cfg_model, cfg_feature=self.cfg_feature)\n",
    "        return  DataLoader(ds_test,\n",
    "                          batch_size=self.cfg_fitting.test_bs,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= DataCollator())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = CVCNNAE()\n",
    "cfg_model = cfg.CVCNNAE()\n",
    "\n",
    "cfg_fitting = cfg.Fitting(max_epoch= 50, es_patience=10)\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "data_out_path  = str(Path(os.environ['DATA_ROOT']) / \"synthesis_data\")\n",
    "data_path = cfg.DataPath(data_out_path, cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "test_loader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe97c1",
   "metadata": {},
   "source": [
    "#### Fitting routine definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f17943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, cfg_fitting, cfg_model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.mecats = 1e-3\n",
    "\n",
    "\n",
    "    def forward(self, x, cats):\n",
    "        x_recon, logits = self.model(x, labels=cats)\n",
    "        return x_recon, logits\n",
    "\n",
    "\n",
    "\n",
    "    # Reconstruction + KL divergence losses summed over all elements and batch\n",
    "    def loss_function(self, recon_x, x, logmean, logvar):\n",
    "        MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - logmean.pow(2) - logvar.exp())\n",
    "        return MSE + KLD\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        cats = batch['cats']\n",
    "        x_recon, logits = self(x, cats) \n",
    "\n",
    "        y_hat = logits.squeeze()\n",
    "        y_hat = (F.igmoid(y_hat) > 0.5).float()\n",
    "\n",
    "\n",
    "\n",
    "        ########################################################################################################################\n",
    "        loss =  ...\n",
    "        ########################################################################################################################\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"loss\":loss, \"train_ttr\": metrics.ttr, \"train_ftr\": metrics.ftr, \"train_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "\n",
    "        results = {\n",
    "            \"loss\": torch.teepsnsor([x['loss'].mean().item() for x in training_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['train_ttr'] for x in training_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['train_ftr'] for x in training_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['train_acc'] for x in training_step_outputs]).mean()\n",
    "            }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for (k,v) in results.items():\n",
    "        \n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        x_recon, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "        \n",
    "        loss = semi_supervised_loss(x, x_recon, y, y_hat)\n",
    "        print(\"y\", y)\n",
    "        print(\"y_hat\", y_hat)\n",
    "        print(\"x_recon\", x_recon.shape)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"val_loss\": loss, \"val_ttr\": metrics.ttr, \"val_ftr\": metrics.ftr, \"val_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # from pprint import pprint \n",
    "        # pprint(validation_step_outputs)\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].mean().item() for x in validation_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['val_ttr'] for x in validation_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['val_ftr'] for x in validation_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['val_acc'] for x in validation_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        _, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "        # (batch,)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        # (batch_probabilities,)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        # (batch_labels,)\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"test_ttr\": metrics.ttr, \"test_ftr\": metrics.ftr, \"test_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        results = {\n",
    "            \"ttr\": torch.tensor([x['test_ttr'] for x in test_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['test_ftr'] for x in test_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['test_acc'] for x in test_step_outputs]).mean()\n",
    "            }\n",
    "\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"test_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # for normal models CNNs etc. \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8703ded",
   "metadata": {},
   "source": [
    "#### Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "237239c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [48], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Init a trainer to execute routine\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m CVCNNAE()\n\u001b[1;32m      5\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      6\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m----> 9\u001b[0m     logger\u001b[39m=\u001b[39mlogger,\n\u001b[1;32m     10\u001b[0m     max_epochs\u001b[39m=\u001b[39mcfg_fitting\u001b[39m.\u001b[39mmax_epoch,\n\u001b[1;32m     11\u001b[0m     callbacks\u001b[39m=\u001b[39mCallbackCollection(cfg_fitting, data_path)(),\n\u001b[1;32m     12\u001b[0m     gradient_clip_val\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m,\n\u001b[1;32m     13\u001b[0m     fast_dev_run\u001b[39m=\u001b[39mcfg_fitting\u001b[39m.\u001b[39mfast_dev_run,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m trainer\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     17\u001b[0m     routine, train_dataloaders\u001b[39m=\u001b[39mtrain_loader, val_dataloaders\u001b[39m=\u001b[39mval_loader\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m trainer\u001b[39m.\u001b[39mtest(dataloaders\u001b[39m=\u001b[39mtest_loader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "routine = Routine(model, cfg_fitting, cfg_model)\n",
    "# Init a trainer to execute routine\n",
    "model = CVCNNAE()\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    sync_batchnorm=True,\n",
    "    logger=logger,\n",
    "    max_epochs=cfg_fitting.max_epoch,\n",
    "    callbacks=CallbackCollection(cfg_fitting, data_path)(),\n",
    "    gradient_clip_val=1.0,\n",
    "    fast_dev_run=cfg_fitting.fast_dev_run,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "\n",
    "trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7aac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DenseCVAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, labels_length=6,drop_out_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.do_rate = drop_out_rate\n",
    "        # adding 1 hot encoded observables to input dimension \n",
    "        input_size_with_label = input_size + labels_length\n",
    "\n",
    "        # adding 1 hot encoded observables to input dimension of decoder, i.e. latent dimension \n",
    "        hidden_size += labels_length\n",
    "        \n",
    "        dense_layer_1_output = 512\n",
    "        dense_layer_2_output = 256\n",
    "        dense_layer_3_output = 128\n",
    "        # Encoder layers, notice that we parameterise both the variance and the mean vector of the distribution \n",
    "        self.EncoderDenseLayer1  = nn.Linear(input_size_with_label, dense_layer_1_output)\n",
    "        self.EncoderRelu1 =  nn.ReLU(inplace=True)\n",
    "        self.EncoderDropout1 = nn.Dropout(self.do_rate)\n",
    "        self.EncoderDenseLayer2 = nn.Linear(dense_layer_1_output, dense_layer_2_output) \n",
    "        self.EncoderRelu2 = nn.ReLU(inplace=True)\n",
    "        self.EncoderDropout2 =  nn.Dropout(self.do_rate)\n",
    "        \n",
    "        self.EncoderDenseLayer3_var =  nn.Linear(dense_layer_2_output, hidden_size)\n",
    "        self.EncoderDenseLayer3_mean =  nn.Linear(dense_layer_2_output, hidden_size)\n",
    "\n",
    "        # decoder layers \n",
    "        self.DecoderDenseLayer1 = nn.Linear(hidden_size, dense_layer_2_output) \n",
    "        self.DecoderRelu1 = nn.ReLU(inplace=True)\n",
    "        self.DecoderDropout1 =  nn.Dropout(self.do_rate)\n",
    "        self.DecoderDenseLayer2 = nn.Linear(dense_layer_2_output, dense_layer_1_output) \n",
    "        self.DecoderRelu2 = nn.ReLU(inplace=True)\n",
    "        self.DecoderDropout2 =  nn.Dropout(self.do_rate)\n",
    "        self.DecoderDenseLayer3 = nn.Linear(dense_layer_1_output, input_size)\n",
    "        \n",
    "\n",
    "    \n",
    "    def encode(self, x, one_hot_labels):\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs, -1)\n",
    "        x_input = torch.cat([x,one_hot_labels], axis=1)\n",
    "        x = self.EncoderDenseLayer1(x_input)\n",
    "        x = self.EncoderRelu1(x)\n",
    "        x = self.EncoderDropout1(x)\n",
    "        x = self.EncoderDenseLayer2(x)\n",
    "        x = self.EncoderRelu2(x)\n",
    "        x = self.EncoderDropout2(x)\n",
    "        z_var = self.EncoderDenseLayer3_var(x)\n",
    "        z_mean = self.EncoderDenseLayer3_mean(x)\n",
    "        return z_mean, z_var \n",
    "        \n",
    "        \n",
    "        \n",
    "    def decode(self, z, one_hot_labels):\n",
    "\n",
    "        x_decoder_input = torch.cat([z,one_hot_labels], axis=1)\n",
    "        x = self.DecoderDenseLayer1(x_decoder_input)\n",
    "        x = self.DecoderRelu1(x)\n",
    "        x = self.DecoderDropout1(x)\n",
    "        x = self.DecoderDenseLayer2(x)\n",
    "        x = self.DecoderRelu2(x)\n",
    "        x = self.DecoderDropout2(x)\n",
    "        logits = self.DecoderDenseLayer3(x)\n",
    "    \n",
    "        return logits\n",
    "        \n",
    "    def reparameterization_trick(self, mean, logvar):\n",
    "        std = torch.exp(0.5 *logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mean)\n",
    "\n",
    "\n",
    "    def forward(self,x, labels):\n",
    "        mean, logvar = self.encode(x, labels)\n",
    "        z = self.reparameterization_trick(mean, logvar)\n",
    "        x = self.decode(z, labels)\n",
    "        return x, mean, logvar\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6056e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end-to-end-l-ED2XZd",
   "language": "python",
   "name": "end-to-end-l-ed2xzd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
