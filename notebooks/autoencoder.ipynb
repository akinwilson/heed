{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6112835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import librosa\n",
    "import warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # set vis gpus \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import wwv.config  as cfg \n",
    "from wwv.util import CallbackCollection \n",
    "from wwv.data import AudioDataModule\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg_fitting = cfg.Fitting(batch_size=64, train_bs=64, val_bs=64)\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_model = cfg.CNNAE()\n",
    "\n",
    "data_path = cfg.DataPath(os.environ['DATA_ROOT'], cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469a162",
   "metadata": {},
   "source": [
    "## Discriminative Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffdb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, latent_dim=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(latent_dim, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "    \n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "\n",
    "\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "\n",
    "        x = self.d_conv3(x)\n",
    "\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        \n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        \n",
    "\n",
    "        x = self.d_pool2(x, idx2)\n",
    "\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        \n",
    "\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "        return decoded_x\n",
    "\n",
    "x = torch.randn((1,1,32000), device=device)\n",
    "\n",
    "model = CNNAE()\n",
    "model.to(device=device)\n",
    "x_reconstructed = model(x)\n",
    "assert x_reconstructed.shape == x.shape,  f\"The reconstructed input is of different dimensions to the original input. Original: {x_reconstructed.shape}. Reconstructed: {x.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Routine for fitting a autoencoder: encoder decoder structure\n",
    "    \"\"\"\n",
    "    def __init__ (self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-4\n",
    "\n",
    "    def encode(self,x):\n",
    "        idx1, idx2, idx3, x_encoded = self.model.encode(x)\n",
    "        return x_encoded,  idx1, idx2, idx3\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        x_reconstructed = self.model.decode( idx1, idx2, idx3, x)\n",
    "        return x_reconstructed \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_encoded,  idx1, idx2, idx3 = self.encode(x)\n",
    "        x_reconstructed  = self.decode( idx1, idx2, idx3, x_encoded)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "\n",
    "    def training_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"loss\": loss }\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "\n",
    "    def validation_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"val_loss\": loss }\n",
    "\n",
    "    def validation_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['val_loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer } # , \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "routine = Routine(model)\n",
    "trainer = Trainer(accelerator=\"gpu\",sync_batchnorm = True, max_epochs = 5 ,num_sanity_val_steps = 2, gradient_clip_val=1.0)\n",
    "# # PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "# # Trainer executes fitting; training and validating proceducres \n",
    "trainer.fit(routine, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TFVisualiser:\n",
    "    '''\n",
    "    Writes latent code generated by model to tensorboard for adding visualisations\n",
    "\n",
    "    NOTE THE BELOW MODEL EXTRACTION EXPECTS THE ROUTINE BE HAVE BEEN FITTED WITH A SINGLE GPU\n",
    "    ----> you'll have to extract the model slightly differently in a distributed environment \n",
    "    '''\n",
    "\n",
    "    def __init__(self, model=trainer.model.model, test_loader=test_loader, sample_size=2000):\n",
    "\n",
    "        self.model = model \n",
    "        self.sample_size = sample_size\n",
    "        # get test data to embedd \n",
    "        xs,ys = [],[]\n",
    "        for batch in test_loader:\n",
    "            xs.append(batch['x'])\n",
    "            ys.append(batch['y'])\n",
    "        X = torch.vstack(xs)\n",
    "        Y = torch.concat(ys)\n",
    "        X_sampled, Y_sampled = self.sample(X,Y) \n",
    "\n",
    "        self.X = X_sampled\n",
    "        self.Y = Y_sampled\n",
    "        self.latent_code_output_dir = 'runs/cnnae'\n",
    "\n",
    "\n",
    "    def sample(self, X, Y):\n",
    "\n",
    "        smaple_permuation_idxs = torch.randperm(X.size(0))\n",
    "        idxs = smaple_permuation_idxs[:self.sample_size]\n",
    "        X_sampled = X[idxs]\n",
    "        Y_sampled = Y[idxs]\n",
    "        return X_sampled, Y_sampled\n",
    "\n",
    "    def save_latent_code(self):\n",
    "        # encode with model \n",
    "        _,_,_, latent_code = self.model.encode(self.X)\n",
    "        label_list = [{1.:\"Wake word\", 0.:\"Not wake word\"}[y] for y in  self.Y.numpy().tolist()]\n",
    "        # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "        writer = SummaryWriter(self.latent_code_output_dir)\n",
    "        # log for visualisation \n",
    "        writer.add_embedding(latent_code, metadata=label_list, tag=\"AudioEmbedding\")\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        from pathlib import Path \n",
    "\n",
    "        print(f\"Saving subsample of {self.sample_size} of test set's latent encodings to location: {Path().cwd() / self.latent_code_output_dir}\")\n",
    "        self.save_latent_code()\n",
    "\n",
    "TFVisualiser()()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_classifier(nn.Module):\n",
    "    '''\n",
    "    Classifier takes latent code and performs predictions using the latent code\n",
    "\n",
    "    Applications:\n",
    "        Upstream feature extraction for memory-constraint classifier \n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,latent_dim, compression_factor=3):\n",
    "        '''\n",
    "        Compression factor detemines the intermitten dimension reduction factor of the dense network. If latent dim 300, then dense layer 1 out will be 100, and then 33 and finally 1. \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "\n",
    "        dense_layer_1_output = int(latent_dim / compression_factor)\n",
    "        dense_layer_2_output = int(dense_layer_1_output / compression_factor)\n",
    "\n",
    "        module_dict = {\n",
    "            \"DenseLayer1\": nn.Linear(latent_dim, dense_layer_1_output), \n",
    "            \"DenseLayer2\": nn.Linear(dense_layer_1_output, dense_layer_2_output), \n",
    "            \"DenseLayer3\": nn.Linear(dense_layer_2_output, 1), \n",
    "        }\n",
    "        \n",
    "        self.layers =  torch.ModuleDict(module_dict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b494edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.routine import Routine \n",
    "import wwv.config as cfg \n",
    "\n",
    "\n",
    "mdoel_cfg = cfg.AEClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb8e7f",
   "metadata": {},
   "source": [
    "### Autoencoder was fit using training and validation datasets. Will split leftover test set into new training data and review metrics produces by autoencoder-supported classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8564d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/akinwilson/Code/HTS/val.csv\"\n",
    "import pandas as pd \n",
    "cols= ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "df = pd.read_csv(path) # [cols]# .columns\n",
    "categorical_cols = ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "\n",
    "df = df[categorical_cols]\n",
    "#import pandas as pd\n",
    "df = pd.get_dummies(df, columns = categorical_cols)\n",
    "\n",
    "df.head().to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 1\n",
    "n_channel = 32\n",
    "stride= 2\n",
    "e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=160, stride=stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7456f8",
   "metadata": {},
   "source": [
    "## Generative variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, c_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim + c_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim + c_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "    \n",
    "    def encoder(self, x, c):\n",
    "        concat_input = torch.cat([x, c], 1)\n",
    "        h = F.relu(self.fc1(concat_input))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add(mu) # return z sample\n",
    "    \n",
    "    def decoder(self, z, c):\n",
    "        concat_input = torch.cat([z, c], 1)\n",
    "        h = F.relu(self.fc4(concat_input))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h))\n",
    "    \n",
    "    def forward(self, x, c):\n",
    "\n",
    "        \n",
    "        mu, log_var = self.encoder(x.view(-1, 784), c)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z, c), mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.routine import Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7aac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=20):\n",
    "        super(CVAE, self).__init__()\n",
    "        input_size_with_label = input_size + labels_length\n",
    "        hidden_size += labels_length\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size_with_label,512)\n",
    "        self.fc21 = nn.Linear(512, hidden_size)\n",
    "        self.fc22 = nn.Linear(512, hidden_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size, 512)\n",
    "        self.fc4 = nn.Linear(512, input_size)\n",
    "    \n",
    "    def encode(self, x, labels):\n",
    "        x = x.view(-1, 1*28*28)\n",
    "        x = torch.cat((x, labels), 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc21(x), self.fc22(x)\n",
    "        \n",
    "    def decode(self, z, labels):\n",
    "        torch.cat((z, labels), 1)\n",
    "        z = self.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(z))\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 *logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "        \n",
    "    def forward(self,x, labels):\n",
    "        #targets = one_hot(targets,labels_length-1).float().to(DEVICE)\n",
    "        mu, logvar = self.encode(x, labels)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x = self.decode(z, labels)\n",
    "        return x, mu, logvar\n",
    "\n",
    "def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=20):\n",
    "    validation_losses = []\n",
    "    optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "        for i in range(epochs):\n",
    "            for batch, labels in dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "                if flatten:\n",
    "                    batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                x,mu,logvar = net(batch, labels)\n",
    "                loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "    plt.show()\n",
    "    return validation_losses\n",
    "cvae = CVAE(28*28).to(DEVICE)\n",
    "def vae_loss_fn(x, recon_x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def evaluate(losses, autoencoder, dataloader, flatten=True):\n",
    "    model = lambda x, y: autoencoder(x, y)[0]    \n",
    "    loss_sum = []\n",
    "    inp, out = [],[]\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "        if flatten:\n",
    "            inputs = inputs.view(inputs.size(0), 28*28)\n",
    "\n",
    "        outputs = model(inputs, labels)\n",
    "        loss = loss_fn(inputs, outputs)            \n",
    "        loss_sum.append(loss)\n",
    "        inp = inputs\n",
    "        out = outputs\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        plot_gallery([inp[0].detach().cpu(),out[0].detach().cpu()],28,28,1,2)    \n",
    "\n",
    "    losses.append((sum(loss_sum)/len(loss_sum)).item())\n",
    "def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=50):\n",
    "    validation_losses = []\n",
    "    optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "        for i in range(epochs):\n",
    "            for batch, labels in dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "                if flatten:\n",
    "                    batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                x,mu,logvar = net(batch, labels)\n",
    "                loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "    plt.show()\n",
    "    return validation_losses\n",
    "\n",
    "\n",
    "\n",
    "history = train_cvae(cvae, train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end-to-end-l-ED2XZd",
   "language": "python",
   "name": "end-to-end-l-ed2xzd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
