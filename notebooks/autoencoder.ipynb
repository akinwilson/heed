{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6112835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akinwilson/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # set vis gpus \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import pytorch_lightning as pl\n",
    "import wwv.config  as cfg \n",
    "from wwv.util import CallbackCollection \n",
    "from wwv.data import AudioDataModule\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg_fitting = cfg.Fitting(batch_size=64, train_bs=64, val_bs=64)\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_model = cfg.CNNAE()\n",
    "\n",
    "data_path = cfg.DataPath(os.environ['DATA_ROOT'], cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469a162",
   "metadata": {},
   "source": [
    "## Discriminative Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffdb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, latent_dim=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(latent_dim, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "    \n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "        return decoded_x\n",
    "\n",
    "x = torch.randn((1,1,32000), device=device)\n",
    "\n",
    "model = CNNAE()\n",
    "model.to(device=device)\n",
    "x_reconstructed = model(x)\n",
    "assert x_reconstructed.shape == x.shape,  f\"The reconstructed input is of different dimensions to the original input. Original: {x_reconstructed.shape}. Reconstructed: {x.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Routine for fitting a autoencoder: encoder decoder structure\n",
    "    \"\"\"\n",
    "    def __init__ (self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-4\n",
    "\n",
    "    def encode(self,x):\n",
    "        idx1, idx2, idx3, x_encoded = self.model.encode(x)\n",
    "        return x_encoded,  idx1, idx2, idx3\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        x_reconstructed = self.model.decode( idx1, idx2, idx3, x)\n",
    "        return x_reconstructed \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_encoded,  idx1, idx2, idx3 = self.encode(x)\n",
    "        x_reconstructed  = self.decode( idx1, idx2, idx3, x_encoded)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "\n",
    "    def training_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"loss\": loss }\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "\n",
    "    def validation_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"val_loss\": loss }\n",
    "\n",
    "    def validation_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['val_loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer } # , \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "routine = Routine(model)\n",
    "trainer = Trainer(accelerator=\"gpu\",sync_batchnorm = True, max_epochs = 5 ,num_sanity_val_steps = 2, gradient_clip_val=1.0)\n",
    "# # PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "# # Trainer executes fitting; training and validating proceducres \n",
    "trainer.fit(routine, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# used downstream, need to define object here, check the forward method of the AE_classifier \n",
    "encoder = trainer.model\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TFVisualiser:\n",
    "    '''\n",
    "    Writes latent code generated by model to tensorboard for adding visualisations\n",
    "\n",
    "    NOTE THE BELOW MODEL EXTRACTION EXPECTS THE ROUTINE BE HAVE BEEN FITTED WITH A SINGLE GPU\n",
    "    ----> you'll have to extract the model slightly differently in a distributed environment \n",
    "    '''\n",
    "\n",
    "    def __init__(self, model=trainer.model.model, test_loader=test_loader, sample_size=2000):\n",
    "\n",
    "        self.model = model \n",
    "        self.sample_size = sample_size\n",
    "        # get test data to embedd \n",
    "        xs,ys = [],[]\n",
    "        for batch in test_loader:\n",
    "            xs.append(batch['x'])\n",
    "            ys.append(batch['y'])\n",
    "        X = torch.vstack(xs)\n",
    "        Y = torch.concat(ys)\n",
    "        X_sampled, Y_sampled = self.sample(X,Y) \n",
    "\n",
    "        self.X = X_sampled\n",
    "        self.Y = Y_sampled\n",
    "        self.latent_code_output_dir = 'runs/cnnae'\n",
    "\n",
    "\n",
    "    def sample(self, X, Y):\n",
    "\n",
    "        smaple_permuation_idxs = torch.randperm(X.size(0))\n",
    "        idxs = smaple_permuation_idxs[:self.sample_size]\n",
    "        X_sampled = X[idxs]\n",
    "        Y_sampled = Y[idxs]\n",
    "        return X_sampled, Y_sampled\n",
    "\n",
    "    def save_latent_code(self):\n",
    "        # encode with model \n",
    "        _,_,_, latent_code = self.model.encode(self.X)\n",
    "        label_list = [{1.:\"Wake word\", 0.:\"Not wake word\"}[y] for y in  self.Y.numpy().tolist()]\n",
    "        # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "        writer = SummaryWriter(self.latent_code_output_dir)\n",
    "        # log for visualisation \n",
    "        writer.add_embedding(latent_code, metadata=label_list, tag=\"AudioEmbedding\")\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        from pathlib import Path \n",
    "\n",
    "        print(f\"Saving subsample of {self.sample_size} of test set's latent encodings to location: {Path().cwd() / self.latent_code_output_dir}\")\n",
    "        self.save_latent_code()\n",
    "\n",
    "TFVisualiser()()\n",
    "\n",
    "# Extracting encoder for down stream task.\n",
    "encoder = trainer.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class AE_classifier(nn.Module):\n",
    "    '''\n",
    "    Classifier takes latent code and performs predictions using the latent code\n",
    "\n",
    "    Applications:\n",
    "        Upstream feature extraction for memory-constraint classifier \n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,latent_dim, dropout=0.2, compression_factor=3):\n",
    "        '''\n",
    "        Compression factor detemines the intermitten dimension reduction factor of the dense network.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "        self.do_rate = dropout\n",
    "\n",
    "        dense_layer_1_output = int(latent_dim / compression_factor)\n",
    "        dense_layer_2_output = int(dense_layer_1_output / compression_factor)\n",
    "\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            OrderedDict([\n",
    "            (\"DenseLayer1\", nn.Linear(latent_dim, dense_layer_1_output) ) , \n",
    "            (\"relu1\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout1\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer2\", nn.Linear(dense_layer_1_output, dense_layer_2_output) ) , \n",
    "            (\"relu2\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout2\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer3\", nn.Linear(dense_layer_2_output, 1) ),\n",
    "            ])\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder.to(\"cuda\")\n",
    "        x_encoded, _ , _ , _ = encoder.encode(x)\n",
    "        logits = self.layers(x_encoded)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb8e7f",
   "metadata": {},
   "source": [
    "### Autoencoder was fit using training and validation datasets. Will split leftover test set into new training data and review metrics produces by autoencoder-supported classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from pathlib import Path \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def create_temporary_fitting_set():\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"ae_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    fitting_set_path = data_out_path.parent / \"test.csv\"\n",
    "    train_test_set, val_set = train_test_split(pd.read_csv(fitting_set_path))\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "\n",
    "create_temporary_fitting_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.data import AudioDataModule\n",
    "from wwv.util import CallbackCollection\n",
    "import wwv.config as cfg\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import bisect\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from wwv.eval import Metric\n",
    "from pathlib import Path \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "cfg_model = cfg.AEClassifier()\n",
    "cfg_fitting = cfg.Fitting(max_epoch= 50, es_patience=10)\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "data_out_path  = str(Path(os.environ['DATA_ROOT']) / \"ae_data\")\n",
    "\n",
    "data_path = cfg.DataPath(data_out_path, cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "# get input shape for onnx exporting\n",
    "input_shape = data_module.input_shape\n",
    "# init model\n",
    "model = AE_classifier(latent_dim=1024)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, cfg_fitting, cfg_model, localization=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.metric = Metric\n",
    "        self.cfg_fitting = cfg_fitting\n",
    "        self.cfg_model = cfg_model\n",
    "        self.localization = localization\n",
    "        self.lr = 1e-3\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x) \n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"loss\":loss, \"train_ttr\": metrics.ttr, \"train_ftr\": metrics.ftr, \"train_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['loss'].item() for x in training_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['train_ttr'] for x in training_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['train_ftr'] for x in training_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['train_acc'] for x in training_step_outputs]).mean()\n",
    "            }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for (k,v) in results.items():\n",
    "        \n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"val_loss\": loss, \"val_ttr\": metrics.ttr, \"val_ftr\": metrics.ftr, \"val_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # from pprint import pprint \n",
    "        # pprint(validation_step_outputs)\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].item() for x in validation_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['val_ttr'] for x in validation_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['val_ftr'] for x in validation_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['val_acc'] for x in validation_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        y_hat = self(x)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        # (batch_probabilities,)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        # (batch_labels,)\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"test_ttr\": metrics.ttr, \"test_ftr\": metrics.ftr, \"test_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        results = {\n",
    "            \"ttr\": torch.tensor([x['test_ttr'] for x in test_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['test_ftr'] for x in test_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['test_acc'] for x in test_step_outputs]).mean()\n",
    "            }\n",
    "\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"test_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # for normal models CNNs etc. \n",
    "        if self.cfg_model.model_name != \"HSTAT\":        \n",
    "            optimizer = torch.optim.AdamW(\n",
    "                filter(lambda p: p.requires_grad, self.parameters()),\n",
    "                lr = self.lr, \n",
    "                betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "            )\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "            return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "        else:\n",
    "            # special scheduler for transformers\n",
    "            optimizer = optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()),lr = self.cfg_fitting.learning_rate, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, )        \n",
    "            def lr_scheduler_lambda_1(epoch):       \n",
    "                if epoch < 3:\n",
    "                    # warm up lr\n",
    "                    lr_scale = self.cfg_fitting.lr_rate[epoch]\n",
    "                else:\n",
    "                    # warmup schedule\n",
    "                    lr_pos = int(-1 - bisect.bisect_left(self.cfg_fitting.lr_scheduler_epoch, epoch))\n",
    "                    if lr_pos < -3:\n",
    "                        lr_scale = max(self.cfg_fitting.lr_rate[0] * (0.98 ** epoch), 0.03 )\n",
    "                    else:\n",
    "                        lr_scale = self.cfg_fitting.lr_rate[lr_pos]\n",
    "                return lr_scale\n",
    "            scheduler_1 = optim.lr_scheduler.LambdaLR(optimizer,lr_lambda=lr_scheduler_lambda_1)\n",
    "\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_1, \"monitor\": \"val_loss\"}\n",
    "\n",
    "\n",
    "routine = Routine(model, cfg_fitting, cfg_model)\n",
    "# Init a trainer to execute routine\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    sync_batchnorm=True,\n",
    "    logger=logger,\n",
    "    max_epochs=cfg_fitting.max_epoch,\n",
    "    callbacks=CallbackCollection(cfg_fitting, data_path)(),\n",
    "    gradient_clip_val=1.0,\n",
    "    fast_dev_run=cfg_fitting.fast_dev_run,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "trainer.test(dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157354d",
   "metadata": {},
   "source": [
    "### Semi-supervised loss : combining the reconstruction and binary cross entropy loss\n",
    "\n",
    "- Combine all available data - such that we have the form:\n",
    "\n",
    "    D = (x, y)\n",
    "    \n",
    "    Where the y $\\in$ $\\{0,1,2\\}$:\n",
    "- Let the loss function be a **piece-wise function** on the domain of the target \n",
    "\n",
    "    \n",
    "$$ Loss(x, x_{recon} y,\\hat{y})=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      MSE(x, x_{recon}), \\text{    }  y \\in \\{2 \\} \\\\\n",
    "      MSE(x, x_{recon}) + BinaryCrossEntropy(y,\\hat{y}), \\text{    }  y \\in \\{0,1\\} \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "We consider then the target label of 2 to be the unknown target label. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c4e7794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupting 85368 samples out of 94854\n",
      "New target distribution\n",
      "2.0    85368\n",
      "0.0     6126\n",
      "1.0     3360\n",
      "Name: label, dtype: int64\n",
      "Training set contains: 53355\n",
      "Validation set contains: 23714\n",
      "Testing set contains: 17785\n",
      "Saving to directory: /media/akinwilson/Samsung_T5/data/audio/keyword-spotting/ss_data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path \n",
    "from collections import OrderedDict\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from wwv.data import AudioDataModule\n",
    "from wwv.util import CallbackCollection\n",
    "import wwv.config as cfg\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import bisect\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from wwv.eval import Metric\n",
    "from pathlib import Path \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def semi_supervised_conversion(df, unknown_label_fraction=0.9,  unknown_target_label=2.):\n",
    "\n",
    "    sample_size = df.shape[0]\n",
    "    unknown_label_sample_size = int( sample_size * unknown_label_fraction)  \n",
    "\n",
    "    print(f\"Corrupting {unknown_label_sample_size} samples out of {sample_size}\")\n",
    "    indices = np.random.choice(np.arange(0, sample_size,1), size=unknown_label_sample_size, replace=False)\n",
    "    df.loc[indices,'label'] = 2.\n",
    "    return df \n",
    "\n",
    "def create_temporary_semi_superivsed_fitting_set(unknown_label_fraction=0.9,  unknown_target_label=2.):\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"ss_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    FILES = ['train.csv', 'val.csv', 'test.csv']\n",
    "    fitting_set_paths = [str(data_out_path.parent / fname) for fname in FILES]\n",
    "    df = pd.concat([pd.read_csv(file_path) for file_path in fitting_set_paths])\n",
    "    df.reset_index(drop=True, inplace=True )\n",
    "    df = semi_supervised_conversion(df, unknown_label_fraction,  unknown_target_label)\n",
    "    print(\"New target distribution\")\n",
    "    print(df.label.value_counts())\n",
    "    # return df \n",
    "    train_test_set, val_set = train_test_split(df)\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "\n",
    "create_temporary_semi_superivsed_fitting_set()\n",
    "\n",
    "data_out_path  = Path(os.environ['DATA_ROOT']) / \"ss_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "968eee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | SS_CNNAE | 4.1 M \n",
      "-----------------------------------\n",
      "4.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 M     Total params\n",
      "16.441    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1147 [00:00<?, ?it/s]                          "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 301\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39m# Init a trainer to execute routine\u001b[39;00m\n\u001b[1;32m    290\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m    291\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    292\u001b[0m     devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m     fast_dev_run\u001b[39m=\u001b[39mcfg_fitting\u001b[39m.\u001b[39mfast_dev_run,\n\u001b[1;32m    299\u001b[0m )\n\u001b[0;32m--> 301\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    302\u001b[0m     routine, train_dataloaders\u001b[39m=\u001b[39;49mtrain_loader, val_dataloaders\u001b[39m=\u001b[39;49mval_loader\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m trainer\u001b[39m.\u001b[39mtest(dataloaders\u001b[39m=\u001b[39mtest_loader)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:582\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 582\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    583\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    584\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:624\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    617\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    619\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    620\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    622\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    623\u001b[0m )\n\u001b[0;32m--> 624\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    626\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1061\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1059\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1061\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1063\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1140\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1163\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1162\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1163\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(dataloader, batch_to_device\u001b[39m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:214\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    213\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 214\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[1;32m    216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    218\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     85\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\n\u001b[1;32m     86\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbatch_idx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(optimizers, kwargs)\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:200\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, optimizers: List[Tuple[\u001b[39mint\u001b[39m, Optimizer]], kwargs: OrderedDict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_kwargs(kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hiddens)\n\u001b[0;32m--> 200\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position])\n\u001b[1;32m    201\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:247\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[1;32m    239\u001b[0m         closure()\n\u001b[1;32m    241\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[39m# the `batch_idx` is optional with inter-batch parallelism\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[1;32m    249\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:357\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[1;32m    356\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[1;32m    358\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[1;32m    360\u001b[0m     batch_idx,\n\u001b[1;32m    361\u001b[0m     optimizer,\n\u001b[1;32m    362\u001b[0m     opt_idx,\n\u001b[1;32m    363\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    364\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[1;32m    365\u001b[0m     using_native_amp\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39m==\u001b[39;49m AMPType\u001b[39m.\u001b[39;49mNATIVE),\n\u001b[1;32m    366\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[1;32m    367\u001b[0m )\n\u001b[1;32m    369\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1305\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m   1304\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1305\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1307\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1661\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[1;32m   1580\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1581\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1588\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1589\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m \u001b[39m    each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \n\u001b[1;32m   1660\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1661\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:169\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:234\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m# TODO(lite): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(\n\u001b[1;32m    235\u001b[0m     optimizer, model\u001b[39m=\u001b[39;49mmodel, optimizer_idx\u001b[39m=\u001b[39;49mopt_idx, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    236\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:121\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/torch/optim/adamw.py:120\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 120\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    123\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:107\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     96\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    100\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    101\u001b[0m     \u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    108\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:147\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    148\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:142\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_fn()\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backward_fn(step_output\u001b[39m.\u001b[39;49mclosure_loss)\n\u001b[1;32m    144\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:303\u001b[0m, in \u001b[0;36mOptimizerLoop._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_fn\u001b[39m(loss: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mbackward\u001b[39;49m\u001b[39m\"\u001b[39;49m, loss, optimizer, opt_idx)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1443\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1443\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1445\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:207\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, optimizer, optimizer_idx, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    205\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpre_backward(closure_loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module)\n\u001b[0;32m--> 207\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49mbackward(closure_loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module, optimizer, optimizer_idx, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    209\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpost_backward(closure_loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module)\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_backward(closure_loss)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:69\u001b[0m, in \u001b[0;36mPrecisionPlugin.backward\u001b[0;34m(self, tensor, model, optimizer, optimizer_idx, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     51\u001b[0m     tensor: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     57\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m        \\**kwargs: Keyword arguments for the same purpose as ``*args``.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     model\u001b[39m.\u001b[39;49mbackward(tensor, optimizer, optimizer_idx, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1406\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, optimizer, optimizer_idx, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\n\u001b[1;32m   1390\u001b[0m     \u001b[39mself\u001b[39m, loss: Tensor, optimizer: Optional[Steppable], optimizer_idx: Optional[\u001b[39mint\u001b[39m], \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m   1391\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1392\u001b[0m     \u001b[39m\"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your\u001b[39;00m\n\u001b[1;32m   1393\u001b[0m \u001b[39m    own implementation if you need to.\u001b[39;00m\n\u001b[1;32m   1394\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[39m            loss.backward()\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1406\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/torch/autograd/__init__.py:190\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    187\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    189\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 190\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/torch/autograd/__init__.py:85\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "class DenseClassifier(nn.Module):\n",
    "    def __init__(self,latent_dim, dropout=0.2, compression_factor=3):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "        self.do_rate = dropout\n",
    "\n",
    "        dense_layer_1_output = int(latent_dim / compression_factor)\n",
    "        dense_layer_2_output = int(dense_layer_1_output / compression_factor)\n",
    "\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            OrderedDict([\n",
    "            (\"DenseLayer1\", nn.Linear(latent_dim, dense_layer_1_output) ) , \n",
    "            (\"relu1\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout1\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer2\", nn.Linear(dense_layer_1_output, dense_layer_2_output) ) , \n",
    "            (\"relu2\", nn.ReLU(inplace=True)),\n",
    "            (\"dropout2\", nn.Dropout(self.do_rate)),\n",
    "            (\"DenseLayer3\", nn.Linear(dense_layer_2_output, 1) ),\n",
    "            ])\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class SS_CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, latent_dim=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(latent_dim, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "\n",
    "        self.classifier = DenseClassifier(latent_dim)\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "    def classify(self, x):\n",
    "        _, _, _, x_encoded = self.encode(x)\n",
    "        logits = self.classifier(x_encoded)\n",
    "        return logits \n",
    "\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Autoencoding forward method'''\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        logits = self.classify(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "\n",
    "        return decoded_x, logits # mse and binary cross entropy inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def semi_supervised_loss(x=None, x_recon=None,y=None, y_hat=None):\n",
    "    bs = x.shape[0]\n",
    "    losses = []\n",
    "    for idx in range(bs):\n",
    "        if y[idx] == 2.:\n",
    "            loss = F.mse_loss(x[idx],x_recon[idx])\n",
    "        else: \n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat[idx], y[idx]) + F.mse_loss(x[idx],x_recon[idx])\n",
    "        losses.append(loss)\n",
    "    return torch.stack(losses)\n",
    "\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, cfg_fitting, cfg_model, localization=False):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.metric = Metric\n",
    "        self.cfg_fitting = cfg_fitting\n",
    "        self.cfg_model = cfg_model\n",
    "        self.localization = localization\n",
    "        self.lr = 1e-3\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_recon, logits = self.model(x)\n",
    "        return x_recon, logits\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        x_recon, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "\n",
    "        loss = semi_supervised_loss(x, x_recon, y, y_hat)\n",
    "\n",
    "        y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"loss\":loss, \"train_ttr\": metrics.ttr, \"train_ftr\": metrics.ftr, \"train_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['loss'].mean().item() for x in training_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['train_ttr'] for x in training_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['train_ftr'] for x in training_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['train_acc'] for x in training_step_outputs]).mean()\n",
    "            }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for (k,v) in results.items():\n",
    "        \n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        x_recon, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "        \n",
    "        loss = semi_supervised_loss(x, x_recon, y, y_hat)\n",
    "\n",
    "\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"val_loss\": loss, \"val_ttr\": metrics.ttr, \"val_ftr\": metrics.ftr, \"val_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # from pprint import pprint \n",
    "        # pprint(validation_step_outputs)\n",
    "        results = {\n",
    "            \"loss\": torch.tensor([x['val_loss'].mean().item() for x in validation_step_outputs]).mean(),\n",
    "            \"ttr\": torch.tensor([x['val_ttr'] for x in validation_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['val_ftr'] for x in validation_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['val_acc'] for x in validation_step_outputs]).mean()\n",
    "            }\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        _, logits = self(x, y) \n",
    "        y_hat = logits.squeeze()\n",
    "        # (batch,)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "        # (batch_probabilities,)\n",
    "        y_hat = (pred > 0.5).float()\n",
    "        # (batch_labels,)\n",
    "        metrics = self.metric(y_hat, y)()\n",
    "        return {\"test_ttr\": metrics.ttr, \"test_ftr\": metrics.ftr, \"test_acc\": metrics.acc}\n",
    "\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        results = {\n",
    "            \"ttr\": torch.tensor([x['test_ttr'] for x in test_step_outputs]).mean(),\n",
    "            \"ftr\": torch.tensor([x['test_ftr'] for x in test_step_outputs]).mean(),\n",
    "            \"acc\": torch.tensor([x['test_acc'] for x in test_step_outputs]).mean()\n",
    "            }\n",
    "\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"test_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # for normal models CNNs etc. \n",
    "        if self.cfg_model.model_name != \"HSTAT\":        \n",
    "            optimizer = torch.optim.AdamW(\n",
    "                filter(lambda p: p.requires_grad, self.parameters()),\n",
    "                lr = self.lr, \n",
    "                betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "            )\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "            return  {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "        else:\n",
    "            # special scheduler for transformers\n",
    "            optimizer = optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()),lr = self.cfg_fitting.learning_rate, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, )        \n",
    "            def lr_scheduler_lambda_1(epoch):       \n",
    "                if epoch < 3:\n",
    "                    # warm up lr\n",
    "                    lr_scale = self.cfg_fitting.lr_rate[epoch]\n",
    "                else:\n",
    "                    # warmup schedule\n",
    "                    lr_pos = int(-1 - bisect.bisect_left(self.cfg_fitting.lr_scheduler_epoch, epoch))\n",
    "                    if lr_pos < -3:\n",
    "                        lr_scale = max(self.cfg_fitting.lr_rate[0] * (0.98 ** epoch), 0.03 )\n",
    "                    else:\n",
    "                        lr_scale = self.cfg_fitting.lr_rate[lr_pos]\n",
    "                return lr_scale\n",
    "            scheduler_1 = optim.lr_scheduler.LambdaLR(optimizer,lr_lambda=lr_scheduler_lambda_1)\n",
    "\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_1, \"monitor\": \"val_loss\"}\n",
    "\n",
    "\n",
    "\n",
    "model = SS_CNNAE()\n",
    "cfg_model = cfg.SSCNNAE()\n",
    "\n",
    "\n",
    "cfg_fitting = cfg.Fitting(max_epoch= 50, es_patience=10)\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "data_out_path  = str(Path(os.environ['DATA_ROOT']) / \"ss_data\")\n",
    "\n",
    "data_path = cfg.DataPath(data_out_path, cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "routine = Routine(model, cfg_fitting, cfg_model)\n",
    "# Init a trainer to execute routine\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    sync_batchnorm=True,\n",
    "    logger=logger,\n",
    "    max_epochs=cfg_fitting.max_epoch,\n",
    "    callbacks=CallbackCollection(cfg_fitting, data_path)(),\n",
    "    gradient_clip_val=1.0,\n",
    "    fast_dev_run=cfg_fitting.fast_dev_run,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "trainer.test(dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786cd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9ac34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8564d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/akinwilson/Code/HTS/val.csv\"\n",
    "import pandas as pd \n",
    "cols= ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "df = pd.read_csv(path) # [cols]# .columns\n",
    "categorical_cols = ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "\n",
    "df = df[categorical_cols]\n",
    "#import pandas as pd\n",
    "df = pd.get_dummies(df, columns = categorical_cols)\n",
    "\n",
    "df.head().to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 1\n",
    "n_channel = 32\n",
    "stride= 2\n",
    "e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=160, stride=stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7456f8",
   "metadata": {},
   "source": [
    "## Generative variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.routine import Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7aac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=20):\n",
    "        super(CVAE, self).__init__()\n",
    "        input_size_with_label = input_size + labels_length\n",
    "        hidden_size += labels_length\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size_with_label,512)\n",
    "        self.fc21 = nn.Linear(512, hidden_size)\n",
    "        self.fc22 = nn.Linear(512, hidden_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size, 512)\n",
    "        self.fc4 = nn.Linear(512, input_size)\n",
    "    \n",
    "    def encode(self, x, labels):\n",
    "        x = x.view(-1, 1*28*28)\n",
    "        x = torch.cat((x, labels), 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc21(x), self.fc22(x)\n",
    "        \n",
    "    def decode(self, z, labels):\n",
    "        torch.cat((z, labels), 1)\n",
    "        z = self.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(z))\n",
    "        \n",
    "    def reparameterization_trick(self, mu, logvar):\n",
    "        std = torch.exp(0.5 *logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "        \n",
    "    def forward(self,x, labels):\n",
    "        #targets = one_hot(targets,labels_length-1).float().to(DEVICE)\n",
    "        mu, logvar = self.encode(x, labels)\n",
    "        z = self.reparameterization_trick(mu, logvar)\n",
    "        x = self.decode(z, labels)\n",
    "        return x, mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=20):\n",
    "#     validation_losses = []\n",
    "#     optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "#     log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "#     with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "#         for i in range(epochs):\n",
    "#             for batch, labels in dataloader:\n",
    "#                 batch = batch.to(DEVICE)\n",
    "#                 labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "#                 if flatten:\n",
    "#                     batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "#                 optim.zero_grad()\n",
    "#                 x,mu,logvar = net(batch, labels)\n",
    "#                 loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "#                 loss.backward()\n",
    "#                 optim.step()\n",
    "#             evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "#             pbar_outer.update(1)\n",
    "#             tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "#     plt.show()\n",
    "#     return validation_losses\n",
    "# cvae = CVAE(28*28).to(DEVICE)\n",
    "# def vae_loss_fn(x, recon_x, mu, logvar):\n",
    "#     BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "#     KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "#     return BCE + KLD\n",
    "\n",
    "# def evaluate(losses, autoencoder, dataloader, flatten=True):\n",
    "#     model = lambda x, y: autoencoder(x, y)[0]    \n",
    "#     loss_sum = []\n",
    "#     inp, out = [],[]\n",
    "#     loss_fn = nn.MSELoss()\n",
    "#     for inputs, labels in dataloader:\n",
    "#         inputs = inputs.to(DEVICE)\n",
    "#         labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "#         if flatten:\n",
    "#             inputs = inputs.view(inputs.size(0), 28*28)\n",
    "\n",
    "#         outputs = model(inputs, labels)\n",
    "#         loss = loss_fn(inputs, outputs)            \n",
    "#         loss_sum.append(loss)\n",
    "#         inp = inputs\n",
    "#         out = outputs\n",
    "\n",
    "#     with torch.set_grad_enabled(False):\n",
    "#         plot_gallery([inp[0].detach().cpu(),out[0].detach().cpu()],28,28,1,2)    \n",
    "\n",
    "#     losses.append((sum(loss_sum)/len(loss_sum)).item())\n",
    "# def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=50):\n",
    "#     validation_losses = []\n",
    "#     optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "#     log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "#     with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "#         for i in range(epochs):\n",
    "#             for batch, labels in dataloader:\n",
    "#                 batch = batch.to(DEVICE)\n",
    "#                 labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "#                 if flatten:\n",
    "#                     batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "#                 optim.zero_grad()\n",
    "#                 x,mu,logvar = net(batch, labels)\n",
    "#                 loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "#                 loss.backward()\n",
    "#                 optim.step()\n",
    "#             evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "#             pbar_outer.update(1)\n",
    "#             tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "#     plt.show()\n",
    "#     return validation_losses\n",
    "\n",
    "\n",
    "\n",
    "# history = train_cvae(cvae, train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end-to-end-l-ED2XZd",
   "language": "python",
   "name": "end-to-end-l-ed2xzd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
