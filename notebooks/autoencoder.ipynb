{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6112835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # set vis gpus \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import pytorch_lightning as pl\n",
    "import wwv.config  as cfg \n",
    "from wwv.util import CallbackCollection \n",
    "from wwv.data import AudioDataModule\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg_fitting = cfg.Fitting(batch_size=64, train_bs=64, val_bs=64)\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_model = cfg.CNNAE()\n",
    "\n",
    "data_path = cfg.DataPath(os.environ['DATA_ROOT'], cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469a162",
   "metadata": {},
   "source": [
    "## Discriminative Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ffdb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, latent_dim=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, latent_dim)\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(latent_dim, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "    \n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "        x = self.d_conv3(x)\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool2(x, idx2)\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "        return decoded_x\n",
    "\n",
    "x = torch.randn((1,1,32000), device=device)\n",
    "\n",
    "model = CNNAE()\n",
    "model.to(device=device)\n",
    "x_reconstructed = model(x)\n",
    "assert x_reconstructed.shape == x.shape,  f\"The reconstructed input is of different dimensions to the original input. Original: {x_reconstructed.shape}. Reconstructed: {x.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b88c55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | CNNAE | 3.7 M \n",
      "--------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.888    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 541/541 [00:18<00:00, 29.75it/s, loss=5.44e-05, v_num=3, val_loss=0.0329, train_loss=6.75e-5]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 541/541 [00:18<00:00, 29.59it/s, loss=5.44e-05, v_num=3, val_loss=0.0329, train_loss=6.75e-5]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Routine for fitting a autoencoder: encoder decoder structure\n",
    "    \"\"\"\n",
    "    def __init__ (self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-4\n",
    "\n",
    "    def encode(self,x):\n",
    "        idx1, idx2, idx3, x_encoded = self.model.encode(x)\n",
    "        return x_encoded,  idx1, idx2, idx3\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        x_reconstructed = self.model.decode( idx1, idx2, idx3, x)\n",
    "        return x_reconstructed \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_encoded,  idx1, idx2, idx3 = self.encode(x)\n",
    "        x_reconstructed  = self.decode( idx1, idx2, idx3, x_encoded)\n",
    "        return x_reconstructed\n",
    "\n",
    "\n",
    "\n",
    "    def training_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"loss\": loss }\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "\n",
    "    def validation_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self.forward(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"val_loss\": loss }\n",
    "\n",
    "    def validation_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['val_loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer } # , \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "routine = Routine(model)\n",
    "trainer = Trainer(accelerator=\"gpu\",sync_batchnorm = True, max_epochs = 5 ,num_sanity_val_steps = 2, gradient_clip_val=1.0)\n",
    "# # PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "# # Trainer executes fitting; training and validating proceducres \n",
    "trainer.fit(routine, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# used downstream, need to define object here, check the forward method of the AE_classifier \n",
    "encoder = trainer.model\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TFVisualiser:\n",
    "    '''\n",
    "    Writes latent code generated by model to tensorboard for adding visualisations\n",
    "\n",
    "    NOTE THE BELOW MODEL EXTRACTION EXPECTS THE ROUTINE BE HAVE BEEN FITTED WITH A SINGLE GPU\n",
    "    ----> you'll have to extract the model slightly differently in a distributed environment \n",
    "    '''\n",
    "\n",
    "    def __init__(self, model=trainer.model.model, test_loader=test_loader, sample_size=2000):\n",
    "\n",
    "        self.model = model \n",
    "        self.sample_size = sample_size\n",
    "        # get test data to embedd \n",
    "        xs,ys = [],[]\n",
    "        for batch in test_loader:\n",
    "            xs.append(batch['x'])\n",
    "            ys.append(batch['y'])\n",
    "        X = torch.vstack(xs)\n",
    "        Y = torch.concat(ys)\n",
    "        X_sampled, Y_sampled = self.sample(X,Y) \n",
    "\n",
    "        self.X = X_sampled\n",
    "        self.Y = Y_sampled\n",
    "        self.latent_code_output_dir = 'runs/cnnae'\n",
    "\n",
    "\n",
    "    def sample(self, X, Y):\n",
    "\n",
    "        smaple_permuation_idxs = torch.randperm(X.size(0))\n",
    "        idxs = smaple_permuation_idxs[:self.sample_size]\n",
    "        X_sampled = X[idxs]\n",
    "        Y_sampled = Y[idxs]\n",
    "        return X_sampled, Y_sampled\n",
    "\n",
    "    def save_latent_code(self):\n",
    "        # encode with model \n",
    "        _,_,_, latent_code = self.model.encode(self.X)\n",
    "        label_list = [{1.:\"Wake word\", 0.:\"Not wake word\"}[y] for y in  self.Y.numpy().tolist()]\n",
    "        # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "        writer = SummaryWriter(self.latent_code_output_dir)\n",
    "        # log for visualisation \n",
    "        writer.add_embedding(latent_code, metadata=label_list, tag=\"AudioEmbedding\")\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        from pathlib import Path \n",
    "\n",
    "        print(f\"Saving subsample of {self.sample_size} of test set's latent encodings to location: {Path().cwd() / self.latent_code_output_dir}\")\n",
    "        self.save_latent_code()\n",
    "\n",
    "TFVisualiser()()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea390dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b34ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "class AE_classifier(nn.Module):\n",
    "    '''\n",
    "    Classifier takes latent code and performs predictions using the latent code\n",
    "\n",
    "    Applications:\n",
    "        Upstream feature extraction for memory-constraint classifier \n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self,latent_dim, compression_factor=3):\n",
    "        '''\n",
    "        Compression factor detemines the intermitten dimension reduction factor of the dense network. If latent dim 300, then dense layer 1 out will be 100, and then 33 and finally 1. \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "\n",
    "        dense_layer_1_output = int(latent_dim / compression_factor)\n",
    "        dense_layer_2_output = int(dense_layer_1_output / compression_factor)\n",
    "        self.layers = torch.nn.Sequential(\n",
    "                OrderedDict([\n",
    "            (\"DenseLayer1\", nn.Linear(latent_dim, dense_layer_1_output) ) , \n",
    "            (\"DenseLayer2\", nn.Linear(dense_layer_1_output, dense_layer_2_output) ) , \n",
    "            (\"DenseLayer3\", nn.Linear(dense_layer_2_output, 1) ) , \n",
    "                ])\n",
    "\n",
    "\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder.to(\"cuda\")\n",
    "        x_encoded, _ , _ , _ = encoder.encode(x)\n",
    "        logits = self.layers(x_encoded)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb8e7f",
   "metadata": {},
   "source": [
    "### Autoencoder was fit using training and validation datasets. Will split leftover test set into new training data and review metrics produces by autoencoder-supported classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f912f0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set contains: 12015\n",
      "Validation set contains: 5340\n",
      "Testing set contains: 4005\n",
      "Saving to directory: /media/akinwilson/Samsung_T5/data/audio/keyword-spotting/ae_data\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from pathlib import Path \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def create_temporary_fitting_set():\n",
    "    data_out_path  = Path(os.environ['DATA_ROOT']) / \"ae_data\"\n",
    "    data_out_path.mkdir(exist_ok=True, parents=True)\n",
    "    fitting_set_path = data_out_path.parent / \"test.csv\"\n",
    "    train_test_set, val_set = train_test_split(pd.read_csv(fitting_set_path))\n",
    "    train_set, test_set  = train_test_split(train_test_set)\n",
    "    print(f\"Training set contains: {train_set.shape[0]}\" )\n",
    "    print(f\"Validation set contains: {val_set.shape[0]}\" )\n",
    "    print(f\"Testing set contains: {test_set.shape[0]}\" )\n",
    "    print(f\"Saving to directory: {data_out_path}\")\n",
    "    for (fname, df) in [(\"train.csv\", train_set), (\"val.csv\", val_set), (\"test.csv\", test_set)]:\n",
    "        df.to_csv(data_out_path / fname, index=False)\n",
    "\n",
    "create_temporary_fitting_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e50f682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.routine import Routine \n",
    "import wwv.config as cfg \n",
    "from wwv.routine import Routine\n",
    "\n",
    "cfg_model = cfg.AEClassifier()\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "data_out_path  = str(Path(os.environ['DATA_ROOT']) / \"ae_data\")\n",
    "\n",
    "data_path = cfg.DataPath(data_out_path, cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcd3b7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | AE_classifier | 388 K \n",
      "----------------------------------------\n",
      "388 K     Trainable params\n",
      "0         Non-trainable params\n",
      "388 K     Total params\n",
      "1.553     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 52.27it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [43], line 46\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# Init a trainer to execute routine\u001b[39;00m\n\u001b[1;32m     34\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     35\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m     devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     fast_dev_run\u001b[39m=\u001b[39mcfg_fitting\u001b[39m.\u001b[39mfast_dev_run,\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     47\u001b[0m     routine, train_dataloaders\u001b[39m=\u001b[39;49mtrain_loader, val_dataloaders\u001b[39m=\u001b[39;49mval_loader\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m trainer\u001b[39m.\u001b[39mtest(dataloaders\u001b[39m=\u001b[39mtest_loader)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:582\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 582\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    583\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    584\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:624\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    617\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    619\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    620\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    622\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    623\u001b[0m )\n\u001b[0;32m--> 624\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    626\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1061\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1059\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1061\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1063\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1140\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1153\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1152\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1155\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1225\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1225\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1229\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:206\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_run_end()\n\u001b[1;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:180\u001b[0m, in \u001b[0;36mEvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mepoch_end_reached()\n\u001b[1;32m    179\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_epoch_end(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outputs)\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m []  \u001b[39m# free memory\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# hook\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:288\u001b[0m, in \u001b[0;36mEvaluationLoop._evaluation_epoch_end\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39m# call the model epoch end\u001b[39;00m\n\u001b[1;32m    287\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_epoch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_epoch_end\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 288\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(hook_name, output_or_outputs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/end-to-end-l-ED2XZd/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1305\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m   1304\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1305\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1307\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Code/end-to-end/app/src/wwv/routine.py:78\u001b[0m, in \u001b[0;36mRoutine.validation_epoch_end\u001b[0;34m(self, validation_step_outputs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, validation_step_outputs):\n\u001b[1;32m     76\u001b[0m     results \u001b[39m=\u001b[39m {\n\u001b[1;32m     77\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mtensor([x[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m validation_step_outputs])\u001b[39m.\u001b[39mmean(),\n\u001b[0;32m---> 78\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mttr\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mtensor([x[\u001b[39m'\u001b[39m\u001b[39mval_ttr\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m validation_step_outputs])\u001b[39m.\u001b[39mmean(),\n\u001b[1;32m     79\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mftr\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mtensor([x[\u001b[39m'\u001b[39m\u001b[39mval_ftr\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m validation_step_outputs])\u001b[39m.\u001b[39mmean(),\n\u001b[1;32m     80\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39macc\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mtensor([x[\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m validation_step_outputs])\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     81\u001b[0m         }\n\u001b[1;32m     82\u001b[0m     \u001b[39mfor\u001b[39;00m (k,v) \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     83\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, v, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, logger\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Code/end-to-end/app/src/wwv/routine.py:78\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, validation_step_outputs):\n\u001b[1;32m     76\u001b[0m     results \u001b[39m=\u001b[39m {\n\u001b[1;32m     77\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mtensor([x[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m validation_step_outputs])\u001b[39m.\u001b[39mmean(),\n\u001b[0;32m---> 78\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mttr\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mtensor([x[\u001b[39m'\u001b[39;49m\u001b[39mval_ttr\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m validation_step_outputs])\u001b[39m.\u001b[39mmean(),\n\u001b[1;32m     79\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mftr\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mtensor([x[\u001b[39m'\u001b[39m\u001b[39mval_ftr\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m validation_step_outputs])\u001b[39m.\u001b[39mmean(),\n\u001b[1;32m     80\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39macc\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mtensor([x[\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m validation_step_outputs])\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     81\u001b[0m         }\n\u001b[1;32m     82\u001b[0m     \u001b[39mfor\u001b[39;00m (k,v) \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     83\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, v, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, logger\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "routine = Routine\n",
    "\n",
    "\n",
    "\n",
    "from wwv.data import AudioDataModule\n",
    "from wwv.util import OnnxExporter, CallbackCollection\n",
    "from wwv.routine import Routine\n",
    "import wwv.config as cfg\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "# get input shape for onnx exporting\n",
    "input_shape = data_module.input_shape\n",
    "# init model\n",
    "model = AE_classifier(latent_dim=1024)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "# setup training, validating and testing routines for the model\n",
    "routine = Routine(model, cfg_fitting, cfg_model)\n",
    "# Init a trainer to execute routine\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    sync_batchnorm=True,\n",
    "    logger=logger,\n",
    "    max_epochs=cfg_fitting.max_epoch,\n",
    "    callbacks=CallbackCollection(cfg_fitting, data_path)(),\n",
    "    num_sanity_val_steps=2,\n",
    "    gradient_clip_val=1.0,\n",
    "    fast_dev_run=cfg_fitting.fast_dev_run,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "trainer.test(dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157354d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e7794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968eee64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786cd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9ac34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8564d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/akinwilson/Code/HTS/val.csv\"\n",
    "import pandas as pd \n",
    "cols= ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "df = pd.read_csv(path) # [cols]# .columns\n",
    "categorical_cols = ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "\n",
    "df = df[categorical_cols]\n",
    "#import pandas as pd\n",
    "df = pd.get_dummies(df, columns = categorical_cols)\n",
    "\n",
    "df.head().to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 1\n",
    "n_channel = 32\n",
    "stride= 2\n",
    "e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=160, stride=stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7456f8",
   "metadata": {},
   "source": [
    "## Generative variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, c_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim + c_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim + c_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "    \n",
    "    def encoder(self, x, c):\n",
    "        concat_input = torch.cat([x, c], 1)\n",
    "        h = F.relu(self.fc1(concat_input))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add(mu) # return z sample\n",
    "    \n",
    "    def decoder(self, z, c):\n",
    "        concat_input = torch.cat([z, c], 1)\n",
    "        h = F.relu(self.fc4(concat_input))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h))\n",
    "    \n",
    "    def forward(self, x, c):\n",
    "\n",
    "        \n",
    "        mu, log_var = self.encoder(x.view(-1, 784), c)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z, c), mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.routine import Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7aac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=20):\n",
    "        super(CVAE, self).__init__()\n",
    "        input_size_with_label = input_size + labels_length\n",
    "        hidden_size += labels_length\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size_with_label,512)\n",
    "        self.fc21 = nn.Linear(512, hidden_size)\n",
    "        self.fc22 = nn.Linear(512, hidden_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size, 512)\n",
    "        self.fc4 = nn.Linear(512, input_size)\n",
    "    \n",
    "    def encode(self, x, labels):\n",
    "        x = x.view(-1, 1*28*28)\n",
    "        x = torch.cat((x, labels), 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc21(x), self.fc22(x)\n",
    "        \n",
    "    def decode(self, z, labels):\n",
    "        torch.cat((z, labels), 1)\n",
    "        z = self.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(z))\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 *logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "        \n",
    "    def forward(self,x, labels):\n",
    "        #targets = one_hot(targets,labels_length-1).float().to(DEVICE)\n",
    "        mu, logvar = self.encode(x, labels)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x = self.decode(z, labels)\n",
    "        return x, mu, logvar\n",
    "\n",
    "def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=20):\n",
    "    validation_losses = []\n",
    "    optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "        for i in range(epochs):\n",
    "            for batch, labels in dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "                if flatten:\n",
    "                    batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                x,mu,logvar = net(batch, labels)\n",
    "                loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "    plt.show()\n",
    "    return validation_losses\n",
    "cvae = CVAE(28*28).to(DEVICE)\n",
    "def vae_loss_fn(x, recon_x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def evaluate(losses, autoencoder, dataloader, flatten=True):\n",
    "    model = lambda x, y: autoencoder(x, y)[0]    \n",
    "    loss_sum = []\n",
    "    inp, out = [],[]\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "        if flatten:\n",
    "            inputs = inputs.view(inputs.size(0), 28*28)\n",
    "\n",
    "        outputs = model(inputs, labels)\n",
    "        loss = loss_fn(inputs, outputs)            \n",
    "        loss_sum.append(loss)\n",
    "        inp = inputs\n",
    "        out = outputs\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        plot_gallery([inp[0].detach().cpu(),out[0].detach().cpu()],28,28,1,2)    \n",
    "\n",
    "    losses.append((sum(loss_sum)/len(loss_sum)).item())\n",
    "def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=50):\n",
    "    validation_losses = []\n",
    "    optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "        for i in range(epochs):\n",
    "            for batch, labels in dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "                if flatten:\n",
    "                    batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                x,mu,logvar = net(batch, labels)\n",
    "                loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "    plt.show()\n",
    "    return validation_losses\n",
    "\n",
    "\n",
    "\n",
    "history = train_cvae(cvae, train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end-to-end-l-ED2XZd",
   "language": "python",
   "name": "end-to-end-l-ed2xzd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
