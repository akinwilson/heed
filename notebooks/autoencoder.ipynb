{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6112835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import librosa\n",
    "import warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\" # set vis gpus \n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import wwv.config  as cfg \n",
    "from wwv.util import CallbackCollection \n",
    "from wwv.data import AudioDataModule\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg_fitting = cfg.Fitting(batch_size=64, train_bs=64, val_bs=64)\n",
    "cfg_feature = cfg.Feature()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_model = cfg.CNNAE()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12fa6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = cfg.DataPath(\"/home/akinwilson/Code/HTS\", cfg_model.model_name, cfg_model.model_dir)\n",
    "data_module = AudioDataModule(data_path.root_data_dir, cfg_model=cfg_model, cfg_feature=cfg_feature, cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "# x = next(iter(train_loader))['x']\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469a162",
   "metadata": {},
   "source": [
    "## Discriminative Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ffdb0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class CNNAE(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=1024, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.n_channel = n_channel\n",
    "        # encoder layers \n",
    "        self.e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.e_bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool1 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.e_bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.e_pool2 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        # \n",
    "        self.e_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool3 = nn.MaxPool1d(4, return_indices=True)\n",
    "        self.e_conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "#######################################################################################\n",
    "        self.e_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.e_pool4 = nn.MaxPool1d(2, return_indices=True)\n",
    "        self.e_fc4 = nn.Linear(2 * n_channel * 28, n_output)\n",
    "#######################################################################################\n",
    "        # decoder layers \n",
    "        self.d_fc4 = nn.Linear(n_output, 2 * n_channel * 28)\n",
    "        self.d_pool4 = nn.MaxUnpool1d(2)\n",
    "        self.d_bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "#######################################################################################\n",
    "        self.d_conv4 = nn.ConvTranspose1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.d_pool3 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "#######################################################################################\n",
    "        self.d_conv3 = nn.ConvTranspose1d(2 * n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool2 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn2 = nn.BatchNorm1d(n_channel)\n",
    "\n",
    "        self.d_conv2 = nn.ConvTranspose1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.d_pool1 = nn.MaxUnpool1d(4)\n",
    "        self.d_bn1 = nn.BatchNorm1d(n_channel)\n",
    "\n",
    "        self.d_conv1 = nn.ConvTranspose1d(n_channel, n_input, kernel_size=80, stride=stride)\n",
    "    \n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.e_conv1(x)\n",
    "        x = F.relu(self.e_bn1(x))\n",
    "        x, idx1 = self.e_pool1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = F.relu(self.e_bn2(x))\n",
    "        x, idx2 = self.e_pool2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = F.relu(self.e_bn3(x))\n",
    "        x, idx3  = self.e_pool3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = F.relu(self.e_bn4(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.e_fc4(x)\n",
    "        return idx1, idx2, idx3, x\n",
    "\n",
    "\n",
    "    def decode(self, idx1, idx2, idx3, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.d_fc4(x)\n",
    "        x = x.view(bs, 2 * self.n_channel,  28)\n",
    "        x = F.relu(self.d_bn4(x))\n",
    "        x = self.d_conv4(x)\n",
    "\n",
    "\n",
    "        x = self.d_pool3(x, idx3)\n",
    "        x = F.relu(self.d_bn3(x))\n",
    "\n",
    "        x = self.d_conv3(x)\n",
    "\n",
    "        padding = idx2.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding),device=self.device)\n",
    "        \n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "        \n",
    "\n",
    "        x = self.d_pool2(x, idx2)\n",
    "\n",
    "        x = F.relu(self.d_bn2(x))\n",
    "        x = self.d_conv2(x)\n",
    "        \n",
    "\n",
    "        padding = idx1.shape[2] - x.shape[2] \n",
    "        pad = torch.zeros((bs,32, padding), device=self.device)\n",
    "        x = torch.cat([x,pad],dim=2)\n",
    "\n",
    "        x = self.d_pool1(x, idx1)\n",
    "        x = F.relu(self.d_bn1(x))\n",
    "        x = self.d_conv1(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        idx1, idx2, idx3, encoded_x = self.encode(x)\n",
    "        decoded_x = self.decode(idx1, idx2, idx3,encoded_x)\n",
    "        return decoded_x\n",
    "\n",
    "x = torch.randn((1,1,32000), device=device)\n",
    "\n",
    "\n",
    "model = CNNAE()\n",
    "model.to(device=device)\n",
    "x_reconstructed = model(x)\n",
    "assert x_reconstructed.shape == x.shape,  f\"The reconstructed input is of different dimensions to the original input. Original: {x_reconstructed.shape}. Reconstructed: {x.shape}\"\n",
    "x_reconstructed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b88c55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | CNNAE | 3.7 M \n",
      "--------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.888    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|‚ñè         | 29/1333 [00:11<08:42,  2.50it/s, loss=0.291, v_num=4]"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "\n",
    "    def __init__ (self, model): # , cfg_model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-4\n",
    "        # self.cfg_model = cfg_model \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_reconstructed = self.model(x)\n",
    "        return x_reconstructed \n",
    "\n",
    "\n",
    "    def training_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"loss\": loss }\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"train_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "\n",
    "    def validation_step( self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_reconstructed = self(x)\n",
    "        loss = F.mse_loss(x,x_reconstructed)\n",
    "        return {\"val_loss\": loss }\n",
    "\n",
    "    def validation_epoch_end(self, training_step_outputs):\n",
    "        results = {\"loss\": torch.tensor( [ x['val_loss'].float().mean().item() for x in training_step_outputs]).mean()}\n",
    "        for (k,v) in results.items():\n",
    "            self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr = self.lr, \n",
    "            betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0.05, \n",
    "        )\n",
    "        # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        return  {\"optimizer\": optimizer } # , \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"} \n",
    "\n",
    "\n",
    "routine = Routine(model)\n",
    "trainer = Trainer(accelerator=\"gpu\",devices=3,strategy='dp',sync_batchnorm = True,max_epochs = 20,num_sanity_val_steps = 2, gradient_clip_val=1.0)\n",
    "# # PATH  = \"/home/akinwilson/Code/pytorch/output/model/ResNet/epoch=18-val_loss=0.15-val_acc=0.95-val_ttr=0.92-val_ftr=0.03.ckpt\"                  \n",
    "# # Trainer executes fitting; training and validating proceducres \n",
    "trainer.fit(routine, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "m = trainer.model.module.module.model\n",
    "encoded_wav = m.encode_forward(torch.randn((1,1,32000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b34ba0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# m = trainer.model.module.module.model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m m\u001b[39m.\u001b[39mencode_forward(torch\u001b[39m.\u001b[39mrandn((\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m32000\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "# condition to correctly extract model in data parallel and single device training \n",
    "m = trainer.model.module.module.model if isinstance(trainer, torch.nn.DataParallel) else  trainer.model.module.model\n",
    "\n",
    "encoded_wav = m.encode_forward(torch.randn((1,1,32000)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8564d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/akinwilson/Code/HTS/val.csv\"\n",
    "import pandas as pd \n",
    "cols= ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "df = pd.read_csv(path) # [cols]# .columns\n",
    "categorical_cols = ['annotated_quality', 'annotated_age', 'annotated_voice_type']\n",
    "\n",
    "df = df[categorical_cols]\n",
    "#import pandas as pd\n",
    "df = pd.get_dummies(df, columns = categorical_cols)\n",
    "\n",
    "df.head().to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 1\n",
    "n_channel = 32\n",
    "stride= 2\n",
    "e_conv1 = nn.Conv1d(n_input, n_channel, kernel_size=160, stride=stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7456f8",
   "metadata": {},
   "source": [
    "## Generative variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, c_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim + c_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim + c_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "    \n",
    "    def encoder(self, x, c):\n",
    "        concat_input = torch.cat([x, c], 1)\n",
    "        h = F.relu(self.fc1(concat_input))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add(mu) # return z sample\n",
    "    \n",
    "    def decoder(self, z, c):\n",
    "        concat_input = torch.cat([z, c], 1)\n",
    "        h = F.relu(self.fc4(concat_input))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h))\n",
    "    \n",
    "    def forward(self, x, c):\n",
    "\n",
    "        \n",
    "        mu, log_var = self.encoder(x.view(-1, 784), c)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z, c), mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wwv.routine import Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7aac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=20):\n",
    "        super(CVAE, self).__init__()\n",
    "        input_size_with_label = input_size + labels_length\n",
    "        hidden_size += labels_length\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size_with_label,512)\n",
    "        self.fc21 = nn.Linear(512, hidden_size)\n",
    "        self.fc22 = nn.Linear(512, hidden_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size, 512)\n",
    "        self.fc4 = nn.Linear(512, input_size)\n",
    "    \n",
    "    def encode(self, x, labels):\n",
    "        x = x.view(-1, 1*28*28)\n",
    "        x = torch.cat((x, labels), 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc21(x), self.fc22(x)\n",
    "        \n",
    "    def decode(self, z, labels):\n",
    "        torch.cat((z, labels), 1)\n",
    "        z = self.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(z))\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 *logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "        \n",
    "    def forward(self,x, labels):\n",
    "        #targets = one_hot(targets,labels_length-1).float().to(DEVICE)\n",
    "        mu, logvar = self.encode(x, labels)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x = self.decode(z, labels)\n",
    "        return x, mu, logvar\n",
    "\n",
    "def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=20):\n",
    "    validation_losses = []\n",
    "    optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "        for i in range(epochs):\n",
    "            for batch, labels in dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "                if flatten:\n",
    "                    batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                x,mu,logvar = net(batch, labels)\n",
    "                loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "    plt.show()\n",
    "    return validation_losses\n",
    "cvae = CVAE(28*28).to(DEVICE)\n",
    "def vae_loss_fn(x, recon_x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def evaluate(losses, autoencoder, dataloader, flatten=True):\n",
    "    model = lambda x, y: autoencoder(x, y)[0]    \n",
    "    loss_sum = []\n",
    "    inp, out = [],[]\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "        if flatten:\n",
    "            inputs = inputs.view(inputs.size(0), 28*28)\n",
    "\n",
    "        outputs = model(inputs, labels)\n",
    "        loss = loss_fn(inputs, outputs)            \n",
    "        loss_sum.append(loss)\n",
    "        inp = inputs\n",
    "        out = outputs\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        plot_gallery([inp[0].detach().cpu(),out[0].detach().cpu()],28,28,1,2)    \n",
    "\n",
    "    losses.append((sum(loss_sum)/len(loss_sum)).item())\n",
    "def train_cvae(net, dataloader, test_dataloader, flatten=True, epochs=50):\n",
    "    validation_losses = []\n",
    "    optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    log_template = \"\\nEpoch {ep:03d} val_loss {v_loss:0.4f}\"\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:  \n",
    "        for i in range(epochs):\n",
    "            for batch, labels in dataloader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                labels = one_hot(labels,9).to(DEVICE)\n",
    "\n",
    "                if flatten:\n",
    "                    batch = batch.view(batch.size(0), 28*28)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                x,mu,logvar = net(batch, labels)\n",
    "                loss = vae_loss_fn(batch, x[:, :784], mu, logvar)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            evaluate(validation_losses, net, test_dataloader, flatten=True)\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=i+1, v_loss=validation_losses[i]))\n",
    "    plt.show()\n",
    "    return validation_losses\n",
    "\n",
    "\n",
    "\n",
    "history = train_cvae(cvae, train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-T3BHxh3q",
   "language": "python",
   "name": "pytorch-t3bhxh3q"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
