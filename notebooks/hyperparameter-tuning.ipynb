{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " changing file location of dataset: train.csv\n",
      " changing file location of dataset: train.csv\n",
      " changing file location of dataset: train.csv\n",
      " changing file location of dataset: train.csv\n",
      " changing file location of dataset: val.csv\n",
      " changing file location of dataset: val.csv\n",
      " changing file location of dataset: val.csv\n",
      " changing file location of dataset: val.csv\n",
      " changing file location of dataset: test.csv\n",
      " changing file location of dataset: test.csv\n",
      " changing file location of dataset: test.csv\n",
      " changing file location of dataset: test.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading env vars from file: ./env_vars/resnet/.dev.env\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import os \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "from wwv.Architecture.ResNet.model import ResNet\n",
    "from wwv.routine import Routine \n",
    "from wwv.eval import Metric\n",
    "from wwv.util import OnnxExporter\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from wwv.eval import Metric\n",
    "import statistics\n",
    "from wwv.data import AudioDataModule\n",
    "import wwv.config as cfg\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "import bisect \n",
    "import torch \n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl \n",
    "import torch.nn.functional as F \n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor, ModelPruning\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from wwv.util import get_username\n",
    "from wwv.Architecture.ResNet.model import ResNet\n",
    "from wwv.Architecture.HTSwin.model import HTSwinTransformer\n",
    "from wwv.Architecture.DeepSpeech.model import DeepSpeech\n",
    "from wwv.Architecture.LeeNet.model import LeeNet\n",
    "from wwv.Architecture.MobileNet.model import MobileNet\n",
    "\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from wwv.util import change_username_of_dataset_fileloactions, get_username\n",
    "# Init a trainer to execute routineSTR_TO_MODELS\n",
    "from wwv.util import OnnxExporter, CallbackCollection\n",
    "\n",
    "\n",
    "\n",
    "change_username_of_dataset_fileloactions(\"useraye\", \"otis\")\n",
    "\n",
    "import torch.optim as optim \n",
    "\n",
    "model_name = \"ResNet\"\n",
    "\n",
    "STR_TO_MODEL_CFGS = {\n",
    "    \"HSTAT\": cfg.HTSwin(),\n",
    "    \"ResNet\": cfg.ResNet(),\n",
    "    \"DeepSpeech\": cfg.DeepSpeech(),\n",
    "    \"LeeNet\": cfg.LeeNet(),\n",
    "    \"MobileNet\": cfg.MobileNet(),\n",
    "}\n",
    "STR_TO_MODELS = {\n",
    "    \"HSTAT\": HTSwinTransformer,\n",
    "    \"ResNet\": ResNet,\n",
    "    \"DeepSpeech\": DeepSpeech,\n",
    "    \"LeeNet\": LeeNet,\n",
    "    \"MobileNet\": MobileNet,\n",
    "}\n",
    "\n",
    "\n",
    "cfg_model = STR_TO_MODEL_CFGS[model_name]\n",
    "# select comp graph/model arch\n",
    "model = STR_TO_MODELS[model_name]\n",
    "\n",
    "\n",
    "\n",
    "env_filepath = os.getenv(\n",
    "    \"ENV_FILE_PATH\", f\"../env_vars/{model_name.lower()}/.dev.env\"\n",
    ")\n",
    "\n",
    "print(f\"Loading env vars from file: {env_filepath}\")\n",
    "load_dotenv(env_filepath)\n",
    "\n",
    "\n",
    "\n",
    "# init the fitter <---- associated  data loaders and fitting routine to model\n",
    "\n",
    "model = model\n",
    "cfg_model = cfg_model\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_path = cfg.DataPath(\n",
    "    f\"/media/{get_username()}/Samsung_T5/data/audio/keyword-spotting\", cfg_model.model_name, cfg_model.model_dir\n",
    ")\n",
    "\n",
    "def setup():\n",
    "    '''\n",
    "    Set up data module and loaders\n",
    "    '''\n",
    "    data_module = AudioDataModule(\n",
    "        data_path.root_data_dir,\n",
    "        cfg_model=cfg_model,\n",
    "        cfg_feature=cfg_feature,\n",
    "        cfg_fitting=cfg_fitting,\n",
    "    )\n",
    "\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    val_loader = data_module.val_dataloader()\n",
    "    test_loader = data_module.test_dataloader()\n",
    "\n",
    "    return data_module, train_loader, val_loader, test_loader\n",
    "\n",
    "# get loaders and datamodule to access input shape\n",
    "data_module, train_loader, val_loader, test_loader = setup()\n",
    "\n",
    "# get input shape for onnx exporting\n",
    "input_shape = data_module.input_shape\n",
    "# init model\n",
    "\n",
    "\n",
    "\n",
    "# callback_dict = callbacks()\n",
    "# callback_list = [v for (_, v) in callback_dict.items()]\n",
    "number_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", \"1,\").split(\",\")\n",
    "try:\n",
    "    number_devices.remove(\"\")\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_callbacks():\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    early_stopping = EarlyStopping(mode=\"min\", monitor='val_loss', patience=cfg_fitting.es_patience)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                            dirpath=data_path.model_dir,\n",
    "                                            save_top_k=1,\n",
    "                                            mode=\"min\",\n",
    "                                            filename='{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_ttr:.2f}-{val_ftr:.2f}')\n",
    "    callbacks = [checkpoint_callback, lr_monitor, early_stopping]\n",
    "    return callbacks \n",
    "\n",
    "# def callbacks():\n",
    "#     cfg_fitting =cfg_fitting\n",
    "#     data_path = data_path\n",
    "#     callback_collection = CallbackCollection(cfg_fitting, data_path)\n",
    "#     return callback_collection()\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "    \n",
    "\n",
    "    kwargs = {\n",
    "        \"num_blocks\": cfg_model.num_blocks,\n",
    "        \"dropout\": cfg_model.dropout,\n",
    "    }\n",
    "\n",
    "    Model = STR_TO_MODELS[model_name]\n",
    "    kwargs['dropout'] = dropout\n",
    "\n",
    "    model = Model(**kwargs)\n",
    "    # setup training, validating and testing routines for the model\n",
    "    routine = Routine(model, cfg_fitting, cfg_model)\n",
    "    callbacks = get_callbacks() + [PyTorchLightningPruningCallback(trial, monitor=\"val_acc\")]\n",
    "\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=len(number_devices),\n",
    "        strategy= \"dpp_notebook\",# os.getenv(\"STRATEGY\", \"dp\"),\n",
    "        sync_batchnorm=True,\n",
    "        max_epochs=cfg_fitting.max_epoch,\n",
    "        callbacks=callbacks,\n",
    "        num_sanity_val_steps=2,\n",
    "        # resume_from_checkpoint=self.cfg_fitting.resume_from_checkpoint,\n",
    "        gradient_clip_val=1.0,\n",
    "        fast_dev_run=cfg_fitting.fast_dev_run,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    hyperparameters = dict(dropout=dropout)\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    trainer.fit(\n",
    "        routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    "    )  # ,ckpt_path=PATH)\n",
    "    val_acc = trainer.callback_metrics[\"val_acc\"].item()\n",
    "    return val_acc\n",
    "\n",
    "# trainer.test(dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-06 22:35:33,968] A new study created in memory with name: no-name-daa90dcb-b0a2-46d7-b063-2bfeeb5c9f58\n",
      "[W 2023-07-06 22:35:34,186] Trial 0 failed with parameters: {'dropout': 0.25845403824870894} because of the following error: MisconfigurationException(\"`Trainer(strategy='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: `Fabric(strategy='dp'|'ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/useraye/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_3527479/4128642925.py\", line 177, in objective\n",
      "    trainer = Trainer(\n",
      "  File \"/home/useraye/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py\", line 69, in insert_env_defaults\n",
      "    return fn(self, **kwargs)\n",
      "  File \"/home/useraye/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 398, in __init__\n",
      "    self._accelerator_connector = _AcceleratorConnector(\n",
      "  File \"/home/useraye/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 173, in __init__\n",
      "    self._lazy_init_strategy()\n",
      "  File \"/home/useraye/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\", line 577, in _lazy_init_strategy\n",
      "    raise MisconfigurationException(\n",
      "lightning_fabric.utilities.exceptions.MisconfigurationException: `Trainer(strategy='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: `Fabric(strategy='dp'|'ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.\n",
      "[W 2023-07-06 22:35:34,188] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: `Fabric(strategy='dp'|'ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, timeout\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/optuna/study/study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     _optimize(\n\u001b[1;32m    444\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    446\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    447\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    448\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    449\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    450\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    451\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    452\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    453\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[7], line 177\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    174\u001b[0m routine \u001b[39m=\u001b[39m Routine(model, cfg_fitting, cfg_model)\n\u001b[1;32m    175\u001b[0m callbacks \u001b[39m=\u001b[39m get_callbacks() \u001b[39m+\u001b[39m [PyTorchLightningPruningCallback(trial, monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m--> 177\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m    178\u001b[0m     accelerator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    179\u001b[0m     devices\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(number_devices),\n\u001b[1;32m    180\u001b[0m     strategy\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mgetenv(\u001b[39m\"\u001b[39;49m\u001b[39mSTRATEGY\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mddp\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    181\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    182\u001b[0m     max_epochs\u001b[39m=\u001b[39;49mcfg_fitting\u001b[39m.\u001b[39;49mmax_epoch,\n\u001b[1;32m    183\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    184\u001b[0m     num_sanity_val_steps\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m    185\u001b[0m     \u001b[39m# resume_from_checkpoint=self.cfg_fitting.resume_from_checkpoint,\u001b[39;49;00m\n\u001b[1;32m    186\u001b[0m     gradient_clip_val\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m    187\u001b[0m     fast_dev_run\u001b[39m=\u001b[39;49mcfg_fitting\u001b[39m.\u001b[39;49mfast_dev_run,\n\u001b[1;32m    188\u001b[0m )\n\u001b[1;32m    192\u001b[0m hyperparameters \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dropout\u001b[39m=\u001b[39mdropout)\n\u001b[1;32m    193\u001b[0m trainer\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog_hyperparams(hyperparameters)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py:69\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m     68\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:398\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39m# init connectors\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector \u001b[39m=\u001b[39m _DataConnector(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 398\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector \u001b[39m=\u001b[39m _AcceleratorConnector(\n\u001b[1;32m    399\u001b[0m     devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[1;32m    400\u001b[0m     accelerator\u001b[39m=\u001b[39;49maccelerator,\n\u001b[1;32m    401\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m    402\u001b[0m     num_nodes\u001b[39m=\u001b[39;49mnum_nodes,\n\u001b[1;32m    403\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49msync_batchnorm,\n\u001b[1;32m    404\u001b[0m     benchmark\u001b[39m=\u001b[39;49mbenchmark,\n\u001b[1;32m    405\u001b[0m     use_distributed_sampler\u001b[39m=\u001b[39;49muse_distributed_sampler,\n\u001b[1;32m    406\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m    407\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[1;32m    408\u001b[0m     plugins\u001b[39m=\u001b[39;49mplugins,\n\u001b[1;32m    409\u001b[0m )\n\u001b[1;32m    410\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector \u001b[39m=\u001b[39m _LoggerConnector(\u001b[39mself\u001b[39m)\n\u001b[1;32m    411\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector \u001b[39m=\u001b[39m _CallbackConnector(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:173\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_and_init_precision()\n\u001b[1;32m    172\u001b[0m \u001b[39m# 6. Instantiate Strategy - Part 2\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy_init_strategy()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-Q_7byvF4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:577\u001b[0m, in \u001b[0;36m_AcceleratorConnector._lazy_init_strategy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_configure_launcher()\n\u001b[1;32m    576\u001b[0m \u001b[39mif\u001b[39;00m _IS_INTERACTIVE \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mis_interactive_compatible:\n\u001b[0;32m--> 577\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    578\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer(strategy=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy_flag\u001b[39m!r}\u001b[39;00m\u001b[39m)` is not compatible with an interactive\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m environment. Run your code as a script, or choose one of the compatible strategies:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `Fabric(strategy=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39mddp_notebook\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m In case you are spawning processes yourself, make sure to include the Trainer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m creation inside the worker function.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n\u001b[1;32m    585\u001b[0m \u001b[39m# TODO: should be moved to _check_strategy_and_fallback().\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[39m# Current test check precision first, so keep this check here to meet error order\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, TPUAccelerator) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    588\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy, (SingleTPUStrategy, XLAStrategy)\n\u001b[1;32m    589\u001b[0m ):\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: `Fabric(strategy='dp'|'ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "import optuna \n",
    "from optuna import Trial, TrialPruned\n",
    "\n",
    "\n",
    "# optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "# lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "# optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "parser = ArgumentParser(description=\"PyTorch Lightning example.\")\n",
    "parser.add_argument(\n",
    "    \"--pruning\",\n",
    "    \"-p\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Activate the pruning feature. `MedianPruner` stops unpromising \"\n",
    "    \"trials at the early stages of training.\",\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "pruner = optuna.pruners.BasePruner = optuna.pruners.MedianPruner()\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-torch-audio-sllXh0tU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
