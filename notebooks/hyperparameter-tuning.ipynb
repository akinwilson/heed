{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading env vars from file: ./env_vars/resnet/.dev.env\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "objective() missing 1 required positional argument: 'trial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/otis/Code/DL-torch-audio/notebooks/hyperparameter-tuning.ipynb Cell 2\u001b[0m in \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/otis/Code/DL-torch-audio/notebooks/hyperparameter-tuning.ipynb#W1sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mcallback_metrics[\u001b[39m\"\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/otis/Code/DL-torch-audio/notebooks/hyperparameter-tuning.ipynb#W1sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m \u001b[39m# trainer.test(dataloaders=test_loader)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/otis/Code/DL-torch-audio/notebooks/hyperparameter-tuning.ipynb#W1sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m objective(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/otis/Code/DL-torch-audio/notebooks/hyperparameter-tuning.ipynb#W1sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: objective() missing 1 required positional argument: 'trial'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import os \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "from wwv.Architecture.ResNet.model import ResNet\n",
    "from wwv.routine import Routine \n",
    "from wwv.eval import Metric\n",
    "from wwv.util import OnnxExporter\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from wwv.eval import Metric\n",
    "import statistics\n",
    "from wwv.data import AudioDataModule\n",
    "import wwv.config as cfg\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "import bisect \n",
    "import torch \n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl \n",
    "import torch.nn.functional as F \n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor, ModelPruning\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from wwv.util import get_username\n",
    "from wwv.Architecture.ResNet.model import ResNet\n",
    "from wwv.Architecture.HTSwin.model import HTSwinTransformer\n",
    "from wwv.Architecture.DeepSpeech.model import DeepSpeech\n",
    "from wwv.Architecture.LeeNet.model import LeeNet\n",
    "from wwv.Architecture.MobileNet.model import MobileNet\n",
    "\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "model_name = \"ResNet\"\n",
    "\n",
    "STR_TO_MODEL_CFGS = {\n",
    "    \"HSTAT\": cfg.HTSwin(),\n",
    "    \"ResNet\": cfg.ResNet(),\n",
    "    \"DeepSpeech\": cfg.DeepSpeech(),\n",
    "    \"LeeNet\": cfg.LeeNet(),\n",
    "    \"MobileNet\": cfg.MobileNet(),\n",
    "}\n",
    "STR_TO_MODELS = {\n",
    "    \"HSTAT\": HTSwinTransformer,\n",
    "    \"ResNet\": ResNet,\n",
    "    \"DeepSpeech\": DeepSpeech,\n",
    "    \"LeeNet\": LeeNet,\n",
    "    \"MobileNet\": MobileNet,\n",
    "}\n",
    "\n",
    "\n",
    "cfg_model = STR_TO_MODEL_CFGS[model_name]\n",
    "# select comp graph/model arch\n",
    "model = STR_TO_MODELS[model_name]\n",
    "\n",
    "\n",
    "\n",
    "env_filepath = os.getenv(\n",
    "    \"ENV_FILE_PATH\", f\"../env_vars/{model_name.lower()}/.dev.env\"\n",
    ")\n",
    "\n",
    "print(f\"Loading env vars from file: {env_filepath}\")\n",
    "load_dotenv(env_filepath)\n",
    "\n",
    "\n",
    "\n",
    "# init the fitter <---- associated  data loaders and fitting routine to model\n",
    "\n",
    "model = model\n",
    "cfg_model = cfg_model\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_signal = cfg.Signal()\n",
    "cfg_feature = cfg.Feature()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_path = cfg.DataPath(\n",
    "    f\"/media/{get_username()}/Samsung_T5/data/audio/keyword-spotting\", cfg_model.model_name, cfg_model.model_dir\n",
    ")\n",
    "\n",
    "def setup():\n",
    "    '''\n",
    "    Set up data module and loaders\n",
    "    '''\n",
    "    data_module = AudioDataModule(\n",
    "        data_path.root_data_dir,\n",
    "        cfg_model=cfg_model,\n",
    "        cfg_feature=cfg_feature,\n",
    "        cfg_fitting=cfg_fitting,\n",
    "    )\n",
    "\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    val_loader = data_module.val_dataloader()\n",
    "    test_loader = data_module.test_dataloader()\n",
    "\n",
    "    return data_module, train_loader, val_loader, test_loader\n",
    "\n",
    "# get loaders and datamodule to access input shape\n",
    "data_module, train_loader, val_loader, test_loader = setup()\n",
    "\n",
    "# get input shape for onnx exporting\n",
    "input_shape = data_module.input_shape\n",
    "# init model\n",
    "\n",
    "# Init a trainer to execute routineSTR_TO_MODELS\n",
    "from wwv.util import OnnxExporter, CallbackCollection\n",
    "\n",
    "\n",
    "\n",
    "# callback_dict = callbacks()\n",
    "# callback_list = [v for (_, v) in callback_dict.items()]\n",
    "number_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", \"1,\").split(\",\")\n",
    "try:\n",
    "    number_devices.remove(\"\")\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_callbacks():\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    early_stopping = EarlyStopping(mode=\"min\", monitor='val_loss', patience=cfg_fitting.es_patience)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                            dirpath=data_path.model_dir,\n",
    "                                            save_top_k=1,\n",
    "                                            mode=\"min\",\n",
    "                                            filename='{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_ttr:.2f}-{val_ftr:.2f}')\n",
    "    callbacks = [checkpoint_callback, lr_monitor, early_stopping]\n",
    "    return callbacks \n",
    "\n",
    "# def callbacks():\n",
    "#     cfg_fitting =cfg_fitting\n",
    "#     data_path = data_path\n",
    "#     callback_collection = CallbackCollection(cfg_fitting, data_path)\n",
    "#     return callback_collection()\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "\n",
    "    Model = STR_TO_MODELS[model_name]\n",
    "\n",
    "    kwargs['dropout'] = dropout\n",
    "\n",
    "    model = Model(**kwargs)\n",
    "    # setup training, validating and testing routines for the model\n",
    "    routine = Routine(model, cfg_fitting, cfg_model)\n",
    "    callbacks = get_callbacks() + [PyTorchLightningPruningCallback(trial, monitor=\"val_acc\")]\n",
    "\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=len(number_devices),\n",
    "        strategy=os.getenv(\"STRATEGY\", \"ddp\"),\n",
    "        sync_batchnorm=True,\n",
    "        max_epochs=cfg_fitting.max_epoch,\n",
    "        callbacks=callbacks,\n",
    "        num_sanity_val_steps=2,\n",
    "        # resume_from_checkpoint=self.cfg_fitting.resume_from_checkpoint,\n",
    "        gradient_clip_val=1.0,\n",
    "        fast_dev_run=cfg_fitting.fast_dev_run,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    hyperparameters = dict(dropout=dropout)\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    trainer.fit(\n",
    "        routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    "    )  # ,ckpt_path=PATH)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_acc\"].item()\n",
    "\n",
    "\n",
    "# trainer.test(dataloaders=test_loader)\n",
    "\n",
    "objective(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "parser = ArgumentParser(description=\"PyTorch Lightning example.\")\n",
    "parser.add_argument(\n",
    "    \"--pruning\",\n",
    "    \"-p\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Activate the pruning feature. `MedianPruner` stops unpromising \"\n",
    "    \"trials at the early stages of training.\",\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "pruner = optuna.pruners.BasePruner = optuna.pruners.MedianPruner()\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-torch-audio-sllXh0tU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
