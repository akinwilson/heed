{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 93\u001b[0m\n\u001b[1;32m     89\u001b[0m     trainer\u001b[39m.\u001b[39mfit(Routine(model, cfg), train_dataloaders\u001b[39m=\u001b[39mtrain_loader, val_dataloaders\u001b[39m=\u001b[39mval_loader)\n\u001b[1;32m     90\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mcallback_metrics[\u001b[39m\"\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> 93\u001b[0m trainer\u001b[39m.\u001b[39mtest(dataloaders\u001b[39m=\u001b[39mtest_loader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import os \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "from wwv.Architecture.ResNet.model import ResNet\n",
    "from wwv.routine import Routine \n",
    "from wwv.eval import Metric\n",
    "from wwv.util import OnnxExporter\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from wwv.eval import Metric\n",
    "import statistics\n",
    "from wwv.data import AudioDataModule\n",
    "import wwv.config as cfg\n",
    "from wwv.meta import params as params \n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "import bisect \n",
    "import torch \n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl \n",
    "import torch.nn.functional as F \n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor, ModelPruning\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "torch.cuda.is_available()\n",
    "cfg_fitting = cfg.Fitting()\n",
    "cfg_resnet = cfg.ResNet()\n",
    "\n",
    "\n",
    "params['model_name'] = \"ResNet\"\n",
    "Cfg = cfg.Config(params)\n",
    "\n",
    "params\n",
    "Cfg = cfg.Config(params)\n",
    "data_path = cfg.DataPath(\"/home/akinwilson/Code/HTS-Audio-Transformer\", Cfg.model_name, Cfg.path['model_dir'])\n",
    "\n",
    "data_module = AudioDataModule(data_path.root_data_dir + \"/train.csv\",\n",
    "                              data_path.root_data_dir + \"/val.csv\",\n",
    "                              data_path.root_data_dir + \"/test.csv\",\n",
    "                              cfg=Cfg,\n",
    "                              cfg_fitting=cfg_fitting)\n",
    "                              \n",
    "train_loader =  data_module.train_dataloader()\n",
    "val_loader =  data_module.val_dataloader()\n",
    "test_loader =  data_module.test_dataloader()\n",
    "\n",
    "cfg = Cfg \n",
    "\n",
    "\n",
    "\n",
    "def get_callbacks():\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    early_stopping = EarlyStopping(mode=\"min\", monitor='val_loss', patience=cfg_fitting.es_patience)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                                            dirpath=data_path.model_dir,\n",
    "                                            save_top_k=1,\n",
    "                                            mode=\"min\",\n",
    "                                            filename='{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_ttr:.2f}-{val_ftr:.2f}')\n",
    "    callbacks = [checkpoint_callback, lr_monitor, early_stopping]\n",
    "    return callbacks \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "    model = ResNet(num_blocks=cfg_resnet.num_blocks, cfg=cfg, dropout=dropout)\n",
    "    callbacks = get_callbacks() + [PyTorchLightningPruningCallback(trial, monitor=\"val_acc\")]\n",
    "    logger = TensorBoardLogger(save_dir=data_path.model_dir, version=1, name=\"lightning_logs\")\n",
    "\n",
    "    trainer = Trainer(accelerator=\"gpu\",\n",
    "                    devices=1,\n",
    "                    strategy='dp',\n",
    "                    logger = logger, \n",
    "                    default_root_dir=data_path.model_dir,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "\n",
    "    hyperparameters = dict(dropout=dropout)\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "\n",
    "    trainer.fit(Routine(model, cfg), train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    return trainer.callback_metrics[\"val_acc\"].item()\n",
    "\n",
    "\n",
    "# trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "parser = ArgumentParser(description=\"PyTorch Lightning example.\")\n",
    "parser.add_argument(\n",
    "    \"--pruning\",\n",
    "    \"-p\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Activate the pruning feature. `MedianPruner` stops unpromising \"\n",
    "    \"trials at the early stages of training.\",\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "pruner = optuna.pruners.BasePruner = optuna.pruners.MedianPruner()\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-T3BHxh3q",
   "language": "python",
   "name": "pytorch-t3bhxh3q"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
