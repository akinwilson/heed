{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff77f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/asteroid-team/torch-audiomentations.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2ea912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akinwilson/.local/share/virtualenvs/pytorch-example-hBW4oP_j/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List \n",
    "from argparse import ArgumentParser\n",
    "parser = ArgumentParser()\n",
    "from pathlib import Path \n",
    "import datetime \n",
    "\n",
    "from torch_audiomentations import (\n",
    "    Gain,\n",
    "    PolarityInversion,\n",
    "    TimeInversion,\n",
    "    AddBackgroundNoise,\n",
    "    BandPassFilter,\n",
    "    BandStopFilter, \n",
    "    AddColoredNoise,\n",
    "    HighPassFilter,\n",
    "    ApplyImpulseResponse,\n",
    "    LowPassFilter,\n",
    "    PitchShift,\n",
    "    RandomCrop,\n",
    "    SpliceOut,\n",
    "    Compose\n",
    ")\n",
    "\n",
    "\n",
    "PROBABILITY_OF_APPLICATION = 0.011\n",
    "\n",
    "parser.add_argument(\"--prob-aug-gain\", type=float,default=PROBABILITY_OF_APPLICATION)\n",
    "parser.add_argument(\"--prob-aug-band-pass-filter\", type=float,default=PROBABILITY_OF_APPLICATION)\n",
    "parser.add_argument(\"--prob-aug-band-stop-filter\", type=float,default=PROBABILITY_OF_APPLICATION)\n",
    "parser.add_argument(\"--prob-aug-polarity-inversion\", type=float,default=PROBABILITY_OF_APPLICATION)\n",
    "parser.add_argument(\"--prob-aug-time-inversion\", type=float,default=PROBABILITY_OF_APPLICATION)\n",
    "\n",
    "parser.add_argument(\"--prob-aug-background-noise\", type=float,default=PROBABILITY_OF_APPLICATION)\n",
    "parser.add_argument(\"--prob-aug-coloured-noise\", type=float,default=PROBABILITY_OF_APPLICATION)\n",
    "parser.add_argument(\"--prob-aug-high-pass-filter\", type=float,default=PROBABILITY_OF_APPLICATION)\n",
    "parser.add_argument(\"--prob-aug-low-pass-filter\", type=float,default=PROBABILITY_OF_APPLICATION)\n",
    "parser.add_argument(\"--prob-aug-impulse-response\", type=float,default=PROBABILITY_OF_APPLICATION)\n",
    "parser.add_argument(\"--prob-aug-pitch-shift\", type=float,default=PROBABILITY_OF_APPLICATION)\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "    \n",
    "apply_augmentation = Compose(\n",
    "    transforms=[\n",
    "        \n",
    "        Gain(p=args.prob_aug_gain,\n",
    "            min_gain_in_db=-15.0,\n",
    "            max_gain_in_db=5.0),\n",
    "        BandPassFilter(p=args.prob_aug_band_pass_filter,\n",
    "                        min_center_frequency=200,\n",
    "                        max_center_frequency=4000,\n",
    "                        min_bandwidth_fraction=0.5,\n",
    "                        max_bandwidth_fraction=1.99),\n",
    "        BandStopFilter(p=args.prob_aug_band_stop_filter,\n",
    "                       min_center_frequency=200,\n",
    "                       max_center_frequency=4000,\n",
    "                       min_bandwidth_fraction=0.5,\n",
    "                       max_bandwidth_fraction=1.99),\n",
    "        \n",
    "        PolarityInversion(p=args.prob_aug_polarity_inversion),\n",
    "        TimeInversion(p=args.prob_aug_time_inversion),\n",
    "    \n",
    "        AddColoredNoise(p=args.prob_aug_coloured_noise, \n",
    "                        min_snr_in_db = 3.0,\n",
    "                        max_snr_in_db = 30.0,\n",
    "                        min_f_decay = -2.0,\n",
    "                        max_f_decay = 2.0),\n",
    "        \n",
    "        HighPassFilter(p=args.prob_aug_high_pass_filter,\n",
    "                       min_cutoff_freq=20,\n",
    "                       max_cutoff_freq=2400), \n",
    "        \n",
    "        LowPassFilter(p=args.prob_aug_low_pass_filter,\n",
    "                      min_cutoff_freq=150,\n",
    "                      max_cutoff_freq=7500),\n",
    "    \n",
    "        PitchShift(p=args.prob_aug_pitch_shift,\n",
    "                   min_transpose_semitones= -4.0,\n",
    "                   max_transpose_semitones= 4.0,\n",
    "                  sample_rate=8000)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe109ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactive = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64524111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pprint import pprint\n",
    "from pathlib import Path \n",
    "import json\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchaudio as ta \n",
    "import torch\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn \n",
    "from torch_audiomentations import Compose, Gain, PolarityInversion\n",
    "import pytorch_lightning as pl\n",
    "from typing import List\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d35c4e",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc39c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_to_tensor_batch(batch):\n",
    "    x = [ x for (x,_) in batch ]\n",
    "    y = [ y for (_,y) in batch ]\n",
    "\n",
    "    x_batched = torch.stack(x).float() \n",
    "    y_batched = torch.stack(y).long()\n",
    "    # return dictionary for unpacking easily as args \n",
    "    return {\"x\": x_batched, \"y\": y_batched}\n",
    "\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 x_path_trg_list,\n",
    "                 x_transforms=None,\n",
    "                 x_aug_transforms= None, \n",
    "                 y_transforms=None):\n",
    "    \n",
    "        self.path_list = x_path_trg_list \n",
    "        self.x_transforms = x_transforms\n",
    "        self.y_transforms = y_transforms \n",
    "        self.x_aug_transforms = x_aug_transforms\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_path, y =  self.path_list[idx]\n",
    "        y =  torch.tensor(int(y))\n",
    "        x,sample_rate = ta.load(x_path)\n",
    "\n",
    "        if self.x_aug_transforms  is not None:\n",
    "            x_aug = None\n",
    "            while x_aug is None:\n",
    "                try:\n",
    "                    x_aug = self.x_aug_transforms(torch.unsqueeze(x,1), sample_rate=sample_rate)            \n",
    "                except ValueError:\n",
    "                    pass \n",
    "        x = pad_sequence(x)\n",
    "        return x,y\n",
    "\n",
    "\n",
    "class AudioDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,train_df, val_df, test_df, train_batch_size=128, val_batch_size=128, test_batch_size=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.val_df =  val_df\n",
    "        self.test_df =  test_df\n",
    "\n",
    "        \n",
    "        self.train_paths = list(zip(self.train_df.wav_path.tolist(), self.train_df.label.tolist())) \n",
    "        self.val_paths = list(zip(self.val_df.wav_path.tolist(), self.val_df.label.tolist())) \n",
    "        self.test_paths = list(zip(self.test_df.wav_path.tolist(), self.test_df.label.tolist())) \n",
    "\n",
    "        self.pin_memory =  False # True if torch.cuda.is_available() else False \n",
    "        \n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size= val_batch_size\n",
    "        self.test_batch_size =  test_batch_size\n",
    "        \n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ds_train = AudioDataset(path_list = self.train_paths,\n",
    "                                x_aug_transforms = apply_augmentation)\n",
    "        return DataLoader(ds_train,\n",
    "                          batch_size=self.train_batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= collate_to_tensor_batch)\n",
    "\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        ds_val = AudioDataset(path_list = self.val_paths,\n",
    "                              x_aug_transforms = None)\n",
    "        return  DataLoader(ds_val,\n",
    "                          batch_size=self.val_batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= collate_to_tensor_batch)\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        ds_test = AudioDataset(path_list = self.test_paths,\n",
    "                              x_aug_transforms = None)\n",
    "        \n",
    "        return  DataLoader(ds_test,\n",
    "                          batch_size=self.test_batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= collate_to_tensor_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e7de16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 20000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SAMPLE_LEN = 20000\n",
    "\n",
    "def pad_sequence(x:torch.Tensor, pad_to_len:int=MAX_SAMPLE_LEN):\n",
    "    '''\n",
    "    Pads audio file upto length pad_to_len\n",
    "    returns padded audiofile \n",
    "    '''\n",
    "    if x.size()[-1] > pad_to_len: # case longer than pad_to_len\n",
    "        x_new = x[:,:pad_to_len] \n",
    "    else: # case shorted than pad_to_len\n",
    "        padding = torch.tensor([0.0]).repeat([1,pad_to_len - x.size()[-1]])\n",
    "        x_new = torch.hstack([x, padding])\n",
    "        \n",
    "    assert x_new.size()[-1] == pad_to_len, f\"Incorrect padded length, Should be {pad_to_len}, got {x_new.size()[-1]}\"\n",
    "    return x_new # (1 ,1 , pad_to_len)\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 path_list:List[str],\n",
    "                 x_transforms=None,\n",
    "                 x_aug_transforms= None, \n",
    "                 y_transforms=None):\n",
    "    \n",
    "        self.path_list = path_list \n",
    "        self.x_transforms = x_transforms\n",
    "        self.y_transforms = y_transforms \n",
    "        self.x_aug_transforms = x_aug_transforms\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_path =  self.path_list[idx]\n",
    "        y =  torch.tensor(int(x_path.split(\"/\")[-1].split(\"_\")[0]))\n",
    "        x,sample_rate = ta.load(x_path)\n",
    "\n",
    "        if self.x_aug_transforms  is not None:\n",
    "            x_aug = None\n",
    "            while x_aug is None:\n",
    "                try:\n",
    "                    x_aug = self.x_aug_transforms(torch.unsqueeze(x,1), sample_rate=sample_rate)            \n",
    "                except ValueError:\n",
    "                    pass \n",
    "        x = pad_sequence(x)\n",
    "        return x,y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collate_to_tensor_batch(batch):\n",
    "    x = [ x for (x,_) in batch ]\n",
    "    y = [ y for (_,y) in batch ]\n",
    "\n",
    "    x_batched = torch.stack(x).float() \n",
    "    y_batched = torch.stack(y).long()\n",
    "    # return dictionary for unpacking easily as args \n",
    "    return {\"x\": x_batched, \"y\": y_batched}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AudioDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_batch_size=128, val_batch_size=128, test_batch_size=128):\n",
    "        super().__init__()\n",
    "        self.all_ds_paths = self.get_all_sample_paths()\n",
    "        \n",
    "        train_paths, val_paths, test_paths = self.split_train_val_test(self.all_ds_paths)\n",
    "        self.train_paths = train_paths\n",
    "        self.val_paths = val_paths\n",
    "        self.test_paths = test_paths \n",
    "        self.pin_memory =  False # True if torch.cuda.is_available() else False \n",
    "        \n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size= val_batch_size\n",
    "        self.test_batch_size =  test_batch_size\n",
    "        \n",
    "    def split_train_val_test(self, list_of_paths):\n",
    "        '''\n",
    "        splits list of all paths into train, val and testing sets\n",
    "        '''\n",
    "        xy_train, xy_val_and_test =  train_test_split(list_of_paths, test_size=0.33, random_state=42)\n",
    "        xy_val, xy_test =  train_test_split(xy_val_and_test, test_size=0.33, random_state=42)\n",
    "        return xy_train, xy_val, xy_test \n",
    "\n",
    "\n",
    "    def get_all_sample_paths(self):\n",
    "        '''\n",
    "        function lists all samples available for training.\n",
    "        '''\n",
    "        data_path = Path().cwd() / \"datasets/free-spoken-digit-dataset-master/recordings\"\n",
    "        all_file_paths = [str(x) for x in [ data_path /  x for x in os.listdir(data_path)]]\n",
    "        return all_file_paths \n",
    "\n",
    "\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        ds_train = AudioDataset(path_list = self.train_paths,\n",
    "                                x_aug_transforms = apply_augmentation)\n",
    "        return DataLoader(ds_train,\n",
    "                          batch_size=self.train_batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= collate_to_tensor_batch)\n",
    "\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        ds_val = AudioDataset(path_list = self.val_paths,\n",
    "                              x_aug_transforms = None)\n",
    "        return  DataLoader(ds_val,\n",
    "                          batch_size=self.val_batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= collate_to_tensor_batch)\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        ds_test = AudioDataset(path_list = self.test_paths,\n",
    "                              x_aug_transforms = None)\n",
    "        \n",
    "        return  DataLoader(ds_test,\n",
    "                          batch_size=self.test_batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True,\n",
    "                          pin_memory= self.pin_memory,\n",
    "                          collate_fn= collate_to_tensor_batch)\n",
    "\n",
    "\n",
    "x = AudioDataModule().train_dataloader()\n",
    "b = next(iter(x))\n",
    "b['x'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa8d1c",
   "metadata": {},
   "source": [
    "## Network M5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3260e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      " M5(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(80,), stride=(16,))\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable layers\n",
      " 25290\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Architecture definition\n",
    "\n",
    "We will define the model architecture in this file\n",
    "\n",
    "'''\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "import torch.nn as nn \n",
    "\n",
    "class M5(nn.Module):\n",
    "    '''\n",
    "    Model origins: \n",
    "        https://arxiv.org/pdf/1610.00087.pdf\n",
    "    '''\n",
    "    def __init__(self, n_input=1, n_output=10, stride=16, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # y is a dummy variable, it is just such that the dictionary \n",
    "        # can easily be unpacked via M5(**batch)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = M5()\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "n = count_trainable_params(model)\n",
    "\n",
    "\n",
    "print(\"Model\\n\",model)\n",
    "print(\"Number of trainable layers\\n\", n)\n",
    "\n",
    "\n",
    "model = M5()\n",
    "model_class_name = type(model).__name__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loading trained model\n",
    "# filename = [file for file in dir_content if model_class_name in file][0]\n",
    "path = \"/home/akinwilson/Code/pytorch-example/model/2022_07_27-09:49:59_PM-model-M5-val-acc-0.98.pt\"\n",
    "\n",
    "state_dict = torch.load(path)\n",
    "model.load_state_dict(state_dict,  strict=False)\n",
    "\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ae871",
   "metadata": {},
   "source": [
    "### Training, validation and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f7f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.\n",
      "======>Avg loss: 1.3341615334633858\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/akinwilson/Code/pytorch-example/testing.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 158>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=165'>166</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=168'>169</a>\u001b[0m data_loader \u001b[39m=\u001b[39m data_module\u001b[39m.\u001b[39mval_dataloader()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=170'>171</a>\u001b[0m total_val_epoch_losss, val_acc, val_recall, val_precision \u001b[39m=\u001b[39m validate_one_epoch(model, data_loader)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=172'>173</a>\u001b[0m avg_per_sample_loss \u001b[39m=\u001b[39m total_val_epoch_losss\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(data_loader)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=173'>174</a>\u001b[0m val_losses\u001b[39m.\u001b[39mappend((epoch, avg_per_sample_loss))\n",
      "\u001b[1;32m/home/akinwilson/Code/pytorch-example/testing.ipynb Cell 10\u001b[0m in \u001b[0;36mvalidate_one_epoch\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=43'>44</a>\u001b[0m     loss \u001b[39m=\u001b[39m validate_one_step(model, data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=44'>45</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=47'>48</a>\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=49'>50</a>\u001b[0m pred \u001b[39m=\u001b[39m get_likely_index(output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=50'>51</a>\u001b[0m all_predictions\u001b[39m.\u001b[39mappend(pred\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-example-hBW4oP_j/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/akinwilson/Code/pytorch-example/testing.ipynb Cell 10\u001b[0m in \u001b[0;36mM5.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=33'>34</a>\u001b[0m     \u001b[39m# y is a dummy variable, it is just such that the dictionary \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=34'>35</a>\u001b[0m     \u001b[39m# can easily be unpacked via M5(**batch)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=35'>36</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=36'>37</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000013?line=37'>38</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(x)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-example-hBW4oP_j/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-example-hBW4oP_j/lib/python3.10/site-packages/torch/nn/modules/conv.py:307\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-example-hBW4oP_j/lib/python3.10/site-packages/torch/nn/modules/conv.py:303\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    301\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    302\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 303\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    304\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_one_step(model, data, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    data = {k:v.to(device) for (k,v) in data.items()}\n",
    "    y_hat = model(**data)\n",
    "    y_hat = y_hat.squeeze()\n",
    "    y = data['y'].squeeze()\n",
    "    loss = F.nll_loss(y_hat , y, reduction='mean')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss \n",
    "\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer):\n",
    "    # model put into training mode \n",
    "    model.train()\n",
    "    total_loss = 0 \n",
    "    for batch_idx, data in enumerate(data_loader):\n",
    "        loss = train_one_step(model, data, optimizer)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "def validate_one_step(model, data):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    data = {k:v.to(device) for (k,v) in data.items()}\n",
    "    y_hat = model(**data)\n",
    "\n",
    "    y_hat = y_hat.squeeze()\n",
    "    y = data['y'].squeeze()\n",
    "    \n",
    "    loss = F.nll_loss(y_hat, y , reduction='mean')\n",
    "\n",
    "    return loss \n",
    "\n",
    "def validate_one_epoch(model,  data_loader):\n",
    "    all_predictions = [ ]\n",
    "    all_targets = [ ]\n",
    "    model.eval()\n",
    "    total_loss = 0 \n",
    "    for batch_idx, data in enumerate(data_loader):\n",
    "        loss = validate_one_step(model, data)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    output = model(**data)\n",
    "\n",
    "    pred = get_likely_index(output)\n",
    "    all_predictions.append(pred.cpu().numpy().tolist())\n",
    "    all_targets.append(data['y'].cpu().numpy().tolist())\n",
    "\n",
    "    flatten =  lambda l: sum(l , [])\n",
    "    \n",
    "    tot_preds = flatten(all_predictions)\n",
    "    tot_trgs = flatten(all_targets)\n",
    "\n",
    "\n",
    "    acc = accuracy_score(tot_trgs, tot_preds)\n",
    "    recall = recall_score(tot_trgs, tot_preds, average='macro')\n",
    "    precision = precision_score(tot_trgs, tot_preds, average='macro')\n",
    "\n",
    "\n",
    "    return total_loss, acc, recall, precision\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    test_loader = data_module.test_dataloader()\n",
    "    num_samples = test_loader.batch_size * len(test_loader)\n",
    "    \n",
    "    all_predictions = [] \n",
    "    all_targets = []\n",
    "\n",
    "    for data in test_loader:\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        data = {k:v.to(device) for (k,v) in data.items()}\n",
    "\n",
    "        output = model(**data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        all_predictions.append(pred.cpu().numpy().tolist())\n",
    "        all_targets.append(data['y'].cpu().numpy().tolist())\n",
    "\n",
    "        correct += number_of_correct(pred, data['y'])\n",
    "\n",
    "\n",
    "    flatten =  lambda l: sum(l , [])\n",
    "    \n",
    "    tot_preds = flatten(all_predictions)\n",
    "    tot_trgs = flatten(all_targets)\n",
    "\n",
    "    acc = correct / num_samples\n",
    "    recall = recall_score(tot_trgs, tot_preds, average='macro')\n",
    "    precision = precision_score(tot_trgs, tot_preds, average='macro')\n",
    "\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(f\"========>Test accuracy: {acc:.2f}\")\n",
    "    print(f\"========>Test recall: {recall}\")\n",
    "    print(f\"========>Test precision: {precision}\")    \n",
    "\n",
    "    return acc, recall, precision\n",
    "\n",
    "\n",
    "LR = 0.01\n",
    "WEIGHT_DECAY = 0.0001\n",
    "MAX_EPOCH = 1000\n",
    "GAMMA = 0.1\n",
    "\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VAL_BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "model_class_name = type(model).__name__\n",
    "\n",
    "# model = torch.nn.DataParallel(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "opt_class_name = type(optimizer).__name__ \n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=GAMMA) \n",
    "\n",
    "data_module = AudioDataModule(train_batch_size=TRAIN_BATCH_SIZE,val_batch_size=VAL_BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "early_stopping_condition_met = False \n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 20\n",
    "metric_history = []\n",
    "epoch = 0 \n",
    "\n",
    "\n",
    "def check_es(hist, min_or_max='max', es_patience=5):\n",
    "    extrema = lambda x : max(x) if min_or_max=='max' else min(x)\n",
    "    return len(hist) - hist.index(extrema(hist)) >= es_patience , extrema(hist)\n",
    "\n",
    "\n",
    "train_losses, val_losses = [ ], [ ]\n",
    "for epoch in range(1, MAX_EPOCH + 1):\n",
    "\n",
    "    data_loader = data_module.train_dataloader()\n",
    "\n",
    "    total_train_epoch_losss = train_one_epoch(model, data_loader, optimizer)\n",
    "    avg_per_sample_loss = total_train_epoch_losss/len(data_loader)\n",
    "    train_losses.append((epoch, avg_per_sample_loss))\n",
    "    print(f\"Epoch: {epoch}.\\n======>Avg loss: {avg_per_sample_loss}\")\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    data_loader = data_module.val_dataloader()\n",
    "\n",
    "    total_val_epoch_losss, val_acc, val_recall, val_precision = validate_one_epoch(model, data_loader)\n",
    "\n",
    "    avg_per_sample_loss = total_val_epoch_losss/len(data_loader)\n",
    "    val_losses.append((epoch, avg_per_sample_loss))\n",
    "    print(f\"Epoch: {epoch}.\\n======>Avg loss: {avg_per_sample_loss}\")\n",
    "\n",
    "    print(f\"Current learning rate: {scheduler.get_lr()[0]}\")\n",
    "    \n",
    "    test_acc, test_recall, test_precision = test(model, epoch)  \n",
    "\n",
    "    metric_history.append(val_acc)\n",
    "    early_stopping_condition_met, extrema_val = check_es(metric_history, es_patience=EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "    if early_stopping_condition_met:\n",
    "        print(\"Early stopping condition met\")\n",
    "        break \n",
    "\n",
    "\n",
    "print(f\"Final epoch: {epoch}\")\n",
    "# mat\n",
    "\n",
    "\n",
    "def save_torch_object(obj, path):\n",
    "    torch.save(obj.state_dict(), path)\n",
    "\n",
    "model_dir = Path().cwd() / \"model\" \n",
    "(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%I:%M:%S_%p\")\n",
    "\n",
    "\n",
    "model_file_name = f\"{date}-model-{model_class_name}-val-acc-{extrema_val:.2f}.pt\"\n",
    "optimizer_file_name = f\"{date}-opt-{opt_class_name}-epoch-{epoch}.pt\"\n",
    "\n",
    "\n",
    "save_torch_object( model , model_dir / model_file_name)\n",
    "save_torch_object( optimizer , model_dir / optimizer_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a44d7a26",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for M5:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"conv4.weight\", \"conv4.bias\", \"bn4.weight\", \"bn4.bias\", \"bn4.running_mean\", \"bn4.running_var\", \"fc1.weight\", \"fc1.bias\". \n\tUnexpected key(s) in state_dict: \"module.conv1.weight\", \"module.conv1.bias\", \"module.bn1.weight\", \"module.bn1.bias\", \"module.bn1.running_mean\", \"module.bn1.running_var\", \"module.bn1.num_batches_tracked\", \"module.conv2.weight\", \"module.conv2.bias\", \"module.bn2.weight\", \"module.bn2.bias\", \"module.bn2.running_mean\", \"module.bn2.running_var\", \"module.bn2.num_batches_tracked\", \"module.conv3.weight\", \"module.conv3.bias\", \"module.bn3.weight\", \"module.bn3.bias\", \"module.bn3.running_mean\", \"module.bn3.running_var\", \"module.bn3.num_batches_tracked\", \"module.conv4.weight\", \"module.conv4.bias\", \"module.bn4.weight\", \"module.bn4.bias\", \"module.bn4.running_mean\", \"module.bn4.running_var\", \"module.bn4.num_batches_tracked\", \"module.fc1.weight\", \"module.fc1.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/akinwilson/Code/pytorch-example/testing.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000033?line=5'>6</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/akinwilson/Code/pytorch-example/model/2022_07_27-09:49:59_PM-model-M5-val-acc-0.98.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000033?line=7'>8</a>\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(path)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akinwilson/Code/pytorch-example/testing.ipynb#ch0000033?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(state_dict)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pytorch-example-hBW4oP_j/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for M5:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"conv2.weight\", \"conv2.bias\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"conv3.weight\", \"conv3.bias\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"conv4.weight\", \"conv4.bias\", \"bn4.weight\", \"bn4.bias\", \"bn4.running_mean\", \"bn4.running_var\", \"fc1.weight\", \"fc1.bias\". \n\tUnexpected key(s) in state_dict: \"module.conv1.weight\", \"module.conv1.bias\", \"module.bn1.weight\", \"module.bn1.bias\", \"module.bn1.running_mean\", \"module.bn1.running_var\", \"module.bn1.num_batches_tracked\", \"module.conv2.weight\", \"module.conv2.bias\", \"module.bn2.weight\", \"module.bn2.bias\", \"module.bn2.running_mean\", \"module.bn2.running_var\", \"module.bn2.num_batches_tracked\", \"module.conv3.weight\", \"module.conv3.bias\", \"module.bn3.weight\", \"module.bn3.bias\", \"module.bn3.running_mean\", \"module.bn3.running_var\", \"module.bn3.num_batches_tracked\", \"module.conv4.weight\", \"module.conv4.bias\", \"module.bn4.weight\", \"module.bn4.bias\", \"module.bn4.running_mean\", \"module.bn4.running_var\", \"module.bn4.num_batches_tracked\", \"module.fc1.weight\", \"module.fc1.bias\". "
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b31a0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0000000000000003e-05]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_train_val_test(list_of_paths):\n",
    "    # train_test_split(list_path_paths test_size=0.33, random_state=42)\n",
    "    xy_train, xy_val_and_test =  train_test_split(list_of_paths, test_size=0.33, random_state=42)\n",
    "    xy_val, xy_test =  train_test_split(xy_val_and_test, test_size=0.33, random_state=42)\n",
    "    return xy_train, xy_val, xy_test \n",
    "\n",
    "\n",
    "def get_all_sample_paths():\n",
    "    '''\n",
    "    function lists all samples available for training.\n",
    "    '''\n",
    "    data_path = Path().cwd() / \"datasets/free-spoken-digit-dataset-master/recordings\"\n",
    "    all_file_paths = [str(x) for x in [ data_path /  x for x in os.listdir(data_path)]]\n",
    "    return all_file_paths \n",
    "\n",
    "\n",
    "\n",
    "# waveform, sample_rate = ta.load(all_file_paths[0])\n",
    "\n",
    "def plot_waveform(waveform, sample_rate):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "    figure.suptitle(\"waveform\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "# plot_waveform(waveform, sample_rate)\n",
    "\n",
    "# waveform.var(axis=1)\n",
    "\n",
    "\n",
    "def get_avg_mean_and_var(full_path_file_list):\n",
    "    means, variances, sample_rates = [], [], []\n",
    "    for f in full_path_file_list: \n",
    "        waveform, x  = ta.load(f)\n",
    "        means.append(waveform.mean(axis=1))\n",
    "        variances.append(waveform.var(axis=1))\n",
    "        sample_rates.append(x)\n",
    "        \n",
    "    \n",
    "    return torch.hstack(means).mean(axis=0), torch.hstack(variances).mean(axis=0), set(sample_rates)\n",
    "        \n",
    "# get_avg_mean_and_var(get_all_sample_paths())\n",
    "# train_paths, val_paths, test_paths  = split_train_val_test(get_all_sample_paths())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-example-hBW4oP_j",
   "language": "python",
   "name": "pytorch-example-hbw4op_j"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
