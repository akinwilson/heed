# Model Deployment

Using onnxruntime to serve predictions, wrapped around a web-service framework; FastAPI. 

To do: 
3rd October 2022
- [] Make the execution provider an environment variable ['TensorrtExecutionProvider','CPUExecutionProvider','CUDAExecutionProvider']
- [] Test the endpoint and create report using different providers and record latency and max QPS

