# Model serving

Using onnxruntime to serve predictions wrapped within a web-framework ( [fastAPI](https://fastapi.tiangolo.com/) ) to create an API.

## Containerised serving

### Building serving container

### Running serving container

To do:
3rd October 2022

- [ ] Make the execution provider an environment variable ['TensorrtExecutionProvider','CPUExecutionProvider','CUDAExecutionProvider']
- [ ] Test the endpoint and create report using different providers and record latency and max QPS
