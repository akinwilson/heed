FROM tensorflow/serving

RUN apt-get -y update && apt-get install -y --no-install-recommends \
    build-essential \
    wget \
    python3-dev \
    python3-pip \
    python3-setuptools \
    nginx \
    ca-certificates \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

RUN ln -s /usr/bin/python3 /usr/bin/python
RUN ln -s /usr/bin/pip3 /usr/bin/pip

RUN pip install --no-cache-dir \
    gevent \
    gunicorn \
    flask \
    requests \
    && rm -rf /root/.cache

# If using this image with SageMaker, you don't need to copy the model into the container as 
# part of the build process, since SageMaker gets the model artifact from S3 and copies it in 
# for you. This allows you to re-use the same serving image for multiple SageMaker models
# COPY data/06_models/saved_model/ /opt/ml/model/1/
ENV MODEL_BASE_PATH="/opt/ml"

COPY dockers/inference/code/ /opt/program/

# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard
# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE
# keeps Python from writing the .pyc files which are unnecessary in this case. We also update
# PATH so that the train and serve programs are found when the container is invoked

ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV PATH="/opt/program:${PATH}"

WORKDIR /opt/program/

ENTRYPOINT ["/bin/bash"]
